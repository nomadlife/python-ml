{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "http://bit.ly/PyTorchZeroAll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 0.0\n",
      "\t 1.0 2.0 0.0 4.0\n",
      "\t 2.0 4.0 0.0 16.0\n",
      "\t 3.0 6.0 0.0 36.0\n",
      "MSE= 18.6666666667\n",
      "w= 0.1\n",
      "\t 1.0 2.0 0.1 3.61\n",
      "\t 2.0 4.0 0.2 14.44\n",
      "\t 3.0 6.0 0.3 32.49\n",
      "MSE= 16.8466666667\n",
      "w= 0.2\n",
      "\t 1.0 2.0 0.2 3.24\n",
      "\t 2.0 4.0 0.4 12.96\n",
      "\t 3.0 6.0 0.6 29.16\n",
      "MSE= 15.12\n",
      "w= 0.3\n",
      "\t 1.0 2.0 0.3 2.89\n",
      "\t 2.0 4.0 0.6 11.56\n",
      "\t 3.0 6.0 0.9 26.01\n",
      "MSE= 13.4866666667\n",
      "w= 0.4\n",
      "\t 1.0 2.0 0.4 2.56\n",
      "\t 2.0 4.0 0.8 10.24\n",
      "\t 3.0 6.0 1.2 23.04\n",
      "MSE= 11.9466666667\n",
      "w= 0.5\n",
      "\t 1.0 2.0 0.5 2.25\n",
      "\t 2.0 4.0 1.0 9.0\n",
      "\t 3.0 6.0 1.5 20.25\n",
      "MSE= 10.5\n",
      "w= 0.6\n",
      "\t 1.0 2.0 0.6 1.96\n",
      "\t 2.0 4.0 1.2 7.84\n",
      "\t 3.0 6.0 1.8 17.64\n",
      "MSE= 9.14666666667\n",
      "w= 0.7\n",
      "\t 1.0 2.0 0.7 1.69\n",
      "\t 2.0 4.0 1.4 6.76\n",
      "\t 3.0 6.0 2.1 15.21\n",
      "MSE= 7.88666666667\n",
      "w= 0.8\n",
      "\t 1.0 2.0 0.8 1.44\n",
      "\t 2.0 4.0 1.6 5.76\n",
      "\t 3.0 6.0 2.4 12.96\n",
      "MSE= 6.72\n",
      "w= 0.9\n",
      "\t 1.0 2.0 0.9 1.21\n",
      "\t 2.0 4.0 1.8 4.84\n",
      "\t 3.0 6.0 2.7 10.89\n",
      "MSE= 5.64666666667\n",
      "w= 1.0\n",
      "\t 1.0 2.0 1.0 1.0\n",
      "\t 2.0 4.0 2.0 4.0\n",
      "\t 3.0 6.0 3.0 9.0\n",
      "MSE= 4.66666666667\n",
      "w= 1.1\n",
      "\t 1.0 2.0 1.1 0.81\n",
      "\t 2.0 4.0 2.2 3.24\n",
      "\t 3.0 6.0 3.3 7.29\n",
      "MSE= 3.78\n",
      "w= 1.2\n",
      "\t 1.0 2.0 1.2 0.64\n",
      "\t 2.0 4.0 2.4 2.56\n",
      "\t 3.0 6.0 3.6 5.76\n",
      "MSE= 2.98666666667\n",
      "w= 1.3\n",
      "\t 1.0 2.0 1.3 0.49\n",
      "\t 2.0 4.0 2.6 1.96\n",
      "\t 3.0 6.0 3.9 4.41\n",
      "MSE= 2.28666666667\n",
      "w= 1.4\n",
      "\t 1.0 2.0 1.4 0.36\n",
      "\t 2.0 4.0 2.8 1.44\n",
      "\t 3.0 6.0 4.2 3.24\n",
      "MSE= 1.68\n",
      "w= 1.5\n",
      "\t 1.0 2.0 1.5 0.25\n",
      "\t 2.0 4.0 3.0 1.0\n",
      "\t 3.0 6.0 4.5 2.25\n",
      "MSE= 1.16666666667\n",
      "w= 1.6\n",
      "\t 1.0 2.0 1.6 0.16\n",
      "\t 2.0 4.0 3.2 0.64\n",
      "\t 3.0 6.0 4.8 1.44\n",
      "MSE= 0.746666666667\n",
      "w= 1.7\n",
      "\t 1.0 2.0 1.7 0.09\n",
      "\t 2.0 4.0 3.4 0.36\n",
      "\t 3.0 6.0 5.1 0.81\n",
      "MSE= 0.42\n",
      "w= 1.8\n",
      "\t 1.0 2.0 1.8 0.04\n",
      "\t 2.0 4.0 3.6 0.16\n",
      "\t 3.0 6.0 5.4 0.36\n",
      "MSE= 0.186666666667\n",
      "w= 1.9\n",
      "\t 1.0 2.0 1.9 0.01\n",
      "\t 2.0 4.0 3.8 0.04\n",
      "\t 3.0 6.0 5.7 0.09\n",
      "MSE= 0.0466666666667\n",
      "w= 2.0\n",
      "\t 1.0 2.0 2.0 0.0\n",
      "\t 2.0 4.0 4.0 0.0\n",
      "\t 3.0 6.0 6.0 0.0\n",
      "MSE= 0.0\n",
      "w= 2.1\n",
      "\t 1.0 2.0 2.1 0.01\n",
      "\t 2.0 4.0 4.2 0.04\n",
      "\t 3.0 6.0 6.3 0.09\n",
      "MSE= 0.0466666666667\n",
      "w= 2.2\n",
      "\t 1.0 2.0 2.2 0.04\n",
      "\t 2.0 4.0 4.4 0.16\n",
      "\t 3.0 6.0 6.6 0.36\n",
      "MSE= 0.186666666667\n",
      "w= 2.3\n",
      "\t 1.0 2.0 2.3 0.09\n",
      "\t 2.0 4.0 4.6 0.36\n",
      "\t 3.0 6.0 6.9 0.81\n",
      "MSE= 0.42\n",
      "w= 2.4\n",
      "\t 1.0 2.0 2.4 0.16\n",
      "\t 2.0 4.0 4.8 0.64\n",
      "\t 3.0 6.0 7.2 1.44\n",
      "MSE= 0.746666666667\n",
      "w= 2.5\n",
      "\t 1.0 2.0 2.5 0.25\n",
      "\t 2.0 4.0 5.0 1.0\n",
      "\t 3.0 6.0 7.5 2.25\n",
      "MSE= 1.16666666667\n",
      "w= 2.6\n",
      "\t 1.0 2.0 2.6 0.36\n",
      "\t 2.0 4.0 5.2 1.44\n",
      "\t 3.0 6.0 7.8 3.24\n",
      "MSE= 1.68\n",
      "w= 2.7\n",
      "\t 1.0 2.0 2.7 0.49\n",
      "\t 2.0 4.0 5.4 1.96\n",
      "\t 3.0 6.0 8.1 4.41\n",
      "MSE= 2.28666666667\n",
      "w= 2.8\n",
      "\t 1.0 2.0 2.8 0.64\n",
      "\t 2.0 4.0 5.6 2.56\n",
      "\t 3.0 6.0 8.4 5.76\n",
      "MSE= 2.98666666667\n",
      "w= 2.9\n",
      "\t 1.0 2.0 2.9 0.81\n",
      "\t 2.0 4.0 5.8 3.24\n",
      "\t 3.0 6.0 8.7 7.29\n",
      "MSE= 3.78\n",
      "w= 3.0\n",
      "\t 1.0 2.0 3.0 1.0\n",
      "\t 2.0 4.0 6.0 4.0\n",
      "\t 3.0 6.0 9.0 9.0\n",
      "MSE= 4.66666666667\n",
      "w= 3.1\n",
      "\t 1.0 2.0 3.1 1.21\n",
      "\t 2.0 4.0 6.2 4.84\n",
      "\t 3.0 6.0 9.3 10.89\n",
      "MSE= 5.64666666667\n",
      "w= 3.2\n",
      "\t 1.0 2.0 3.2 1.44\n",
      "\t 2.0 4.0 6.4 5.76\n",
      "\t 3.0 6.0 9.6 12.96\n",
      "MSE= 6.72\n",
      "w= 3.3\n",
      "\t 1.0 2.0 3.3 1.69\n",
      "\t 2.0 4.0 6.6 6.76\n",
      "\t 3.0 6.0 9.9 15.21\n",
      "MSE= 7.88666666667\n",
      "w= 3.4\n",
      "\t 1.0 2.0 3.4 1.96\n",
      "\t 2.0 4.0 6.8 7.84\n",
      "\t 3.0 6.0 10.2 17.64\n",
      "MSE= 9.14666666667\n",
      "w= 3.5\n",
      "\t 1.0 2.0 3.5 2.25\n",
      "\t 2.0 4.0 7.0 9.0\n",
      "\t 3.0 6.0 10.5 20.25\n",
      "MSE= 10.5\n",
      "w= 3.6\n",
      "\t 1.0 2.0 3.6 2.56\n",
      "\t 2.0 4.0 7.2 10.24\n",
      "\t 3.0 6.0 10.8 23.04\n",
      "MSE= 11.9466666667\n",
      "w= 3.7\n",
      "\t 1.0 2.0 3.7 2.89\n",
      "\t 2.0 4.0 7.4 11.56\n",
      "\t 3.0 6.0 11.1 26.01\n",
      "MSE= 13.4866666667\n",
      "w= 3.8\n",
      "\t 1.0 2.0 3.8 3.24\n",
      "\t 2.0 4.0 7.6 12.96\n",
      "\t 3.0 6.0 11.4 29.16\n",
      "MSE= 15.12\n",
      "w= 3.9\n",
      "\t 1.0 2.0 3.9 3.61\n",
      "\t 2.0 4.0 7.8 14.44\n",
      "\t 3.0 6.0 11.7 32.49\n",
      "MSE= 16.8466666667\n",
      "w= 4.0\n",
      "\t 1.0 2.0 4.0 4.0\n",
      "\t 2.0 4.0 8.0 16.0\n",
      "\t 3.0 6.0 12.0 36.0\n",
      "MSE= 18.6666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf6434ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. basics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "\n",
    "# our model for the forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "w_list = []\n",
    "mse_list = []\n",
    "\n",
    "for w in np.arange(0.0, 4.1, 0.1):\n",
    "    print(\"w=\", w)\n",
    "    l_sum = 0\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred_val = forward(x_val)\n",
    "        l = loss(x_val, y_val)\n",
    "        l_sum += l\n",
    "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
    "    print(\"MSE=\", l_sum / 3)\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum / 3)\n",
    "\n",
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.23\n",
      "progress: 0 w= 1.26 loss= 4.92\n",
      "\tgrad:  1.0 2.0 -1.48\n",
      "\tgrad:  2.0 4.0 -5.8\n",
      "\tgrad:  3.0 6.0 -12.0\n",
      "progress: 1 w= 1.45 loss= 2.69\n",
      "\tgrad:  1.0 2.0 -1.09\n",
      "\tgrad:  2.0 4.0 -4.29\n",
      "\tgrad:  3.0 6.0 -8.87\n",
      "progress: 2 w= 1.6 loss= 1.47\n",
      "\tgrad:  1.0 2.0 -0.81\n",
      "\tgrad:  2.0 4.0 -3.17\n",
      "\tgrad:  3.0 6.0 -6.56\n",
      "progress: 3 w= 1.7 loss= 0.8\n",
      "\tgrad:  1.0 2.0 -0.6\n",
      "\tgrad:  2.0 4.0 -2.34\n",
      "\tgrad:  3.0 6.0 -4.85\n",
      "progress: 4 w= 1.78 loss= 0.44\n",
      "\tgrad:  1.0 2.0 -0.44\n",
      "\tgrad:  2.0 4.0 -1.73\n",
      "\tgrad:  3.0 6.0 -3.58\n",
      "progress: 5 w= 1.84 loss= 0.24\n",
      "\tgrad:  1.0 2.0 -0.33\n",
      "\tgrad:  2.0 4.0 -1.28\n",
      "\tgrad:  3.0 6.0 -2.65\n",
      "progress: 6 w= 1.88 loss= 0.13\n",
      "\tgrad:  1.0 2.0 -0.24\n",
      "\tgrad:  2.0 4.0 -0.95\n",
      "\tgrad:  3.0 6.0 -1.96\n",
      "progress: 7 w= 1.91 loss= 0.07\n",
      "\tgrad:  1.0 2.0 -0.18\n",
      "\tgrad:  2.0 4.0 -0.7\n",
      "\tgrad:  3.0 6.0 -1.45\n",
      "progress: 8 w= 1.93 loss= 0.04\n",
      "\tgrad:  1.0 2.0 -0.13\n",
      "\tgrad:  2.0 4.0 -0.52\n",
      "\tgrad:  3.0 6.0 -1.07\n",
      "progress: 9 w= 1.95 loss= 0.02\n",
      "predict (after training) 4 hours 7.804863933862125\n"
     ]
    }
   ],
   "source": [
    "# 2. manual gradient\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0  # a random guess: random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "# compute gradient\n",
    "def gradient(x, y):  # d_loss/d_w\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad\n",
    "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
    "        l = loss(x_val, y_val)\n",
    "\n",
    "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", \"4 hours\", forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.840000152587891\n",
      "\tgrad:  3.0 6.0 -16.228801727294922\n",
      "progress: 0 7.315943717956543\n",
      "\tgrad:  1.0 2.0 -1.478623867034912\n",
      "\tgrad:  2.0 4.0 -5.796205520629883\n",
      "\tgrad:  3.0 6.0 -11.998146057128906\n",
      "progress: 1 3.9987640380859375\n",
      "\tgrad:  1.0 2.0 -1.0931644439697266\n",
      "\tgrad:  2.0 4.0 -4.285204887390137\n",
      "\tgrad:  3.0 6.0 -8.870372772216797\n",
      "progress: 2 2.1856532096862793\n",
      "\tgrad:  1.0 2.0 -0.8081896305084229\n",
      "\tgrad:  2.0 4.0 -3.1681032180786133\n",
      "\tgrad:  3.0 6.0 -6.557973861694336\n",
      "progress: 3 1.1946394443511963\n",
      "\tgrad:  1.0 2.0 -0.5975041389465332\n",
      "\tgrad:  2.0 4.0 -2.3422164916992188\n",
      "\tgrad:  3.0 6.0 -4.848389625549316\n",
      "progress: 4 0.6529689431190491\n",
      "\tgrad:  1.0 2.0 -0.4417421817779541\n",
      "\tgrad:  2.0 4.0 -1.7316293716430664\n",
      "\tgrad:  3.0 6.0 -3.58447265625\n",
      "progress: 5 0.35690122842788696\n",
      "\tgrad:  1.0 2.0 -0.3265852928161621\n",
      "\tgrad:  2.0 4.0 -1.2802143096923828\n",
      "\tgrad:  3.0 6.0 -2.650045394897461\n",
      "progress: 6 0.195076122879982\n",
      "\tgrad:  1.0 2.0 -0.24144840240478516\n",
      "\tgrad:  2.0 4.0 -0.9464778900146484\n",
      "\tgrad:  3.0 6.0 -1.9592113494873047\n",
      "progress: 7 0.10662525147199631\n",
      "\tgrad:  1.0 2.0 -0.17850565910339355\n",
      "\tgrad:  2.0 4.0 -0.699742317199707\n",
      "\tgrad:  3.0 6.0 -1.4484672546386719\n",
      "progress: 8 0.0582793727517128\n",
      "\tgrad:  1.0 2.0 -0.1319713592529297\n",
      "\tgrad:  2.0 4.0 -0.5173273086547852\n",
      "\tgrad:  3.0 6.0 -1.070866584777832\n",
      "progress: 9 0.03185431286692619\n",
      "predict (after training) 4 7.804864406585693\n"
     ]
    }
   ],
   "source": [
    "# 3.auto-gradient\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Any random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", 4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 49.68061065673828\n",
      "1 22.38979721069336\n",
      "2 10.236771583557129\n",
      "3 4.82271671295166\n",
      "4 2.408717632293701\n",
      "5 1.3303112983703613\n",
      "6 0.8465259671211243\n",
      "7 0.6275047659873962\n",
      "8 0.5263998508453369\n",
      "9 0.47784096002578735\n",
      "10 0.4527240991592407\n",
      "11 0.4380934238433838\n",
      "12 0.42818063497543335\n",
      "13 0.42041727900505066\n",
      "14 0.4136583209037781\n",
      "15 0.407394140958786\n",
      "16 0.40139758586883545\n",
      "17 0.3955652117729187\n",
      "18 0.3898524343967438\n",
      "19 0.38423705101013184\n",
      "20 0.3787093162536621\n",
      "21 0.3732643127441406\n",
      "22 0.36789894104003906\n",
      "23 0.3626110851764679\n",
      "24 0.3573996424674988\n",
      "25 0.35226285457611084\n",
      "26 0.34720057249069214\n",
      "27 0.34221068024635315\n",
      "28 0.33729246258735657\n",
      "29 0.3324451446533203\n",
      "30 0.3276674151420593\n",
      "31 0.3229581415653229\n",
      "32 0.31831681728363037\n",
      "33 0.3137420117855072\n",
      "34 0.30923300981521606\n",
      "35 0.3047887086868286\n",
      "36 0.3004087209701538\n",
      "37 0.29609137773513794\n",
      "38 0.29183587431907654\n",
      "39 0.28764158487319946\n",
      "40 0.28350791335105896\n",
      "41 0.27943357825279236\n",
      "42 0.27541762590408325\n",
      "43 0.27145928144454956\n",
      "44 0.2675580084323883\n",
      "45 0.26371294260025024\n",
      "46 0.2599228024482727\n",
      "47 0.2561872899532318\n",
      "48 0.2525055408477783\n",
      "49 0.2488764524459839\n",
      "50 0.2452998012304306\n",
      "51 0.241774320602417\n",
      "52 0.23829974234104156\n",
      "53 0.23487503826618195\n",
      "54 0.2314995527267456\n",
      "55 0.22817248106002808\n",
      "56 0.22489331662654877\n",
      "57 0.22166135907173157\n",
      "58 0.21847550570964813\n",
      "59 0.2153356671333313\n",
      "60 0.21224087476730347\n",
      "61 0.20919086039066315\n",
      "62 0.2061845064163208\n",
      "63 0.2032211422920227\n",
      "64 0.20030052959918976\n",
      "65 0.1974218785762787\n",
      "66 0.19458454847335815\n",
      "67 0.19178828597068787\n",
      "68 0.18903197348117828\n",
      "69 0.18631523847579956\n",
      "70 0.1836373656988144\n",
      "71 0.1809982806444168\n",
      "72 0.17839708924293518\n",
      "73 0.17583319544792175\n",
      "74 0.1733061671257019\n",
      "75 0.17081542313098907\n",
      "76 0.16836047172546387\n",
      "77 0.1659410297870636\n",
      "78 0.1635562926530838\n",
      "79 0.16120564937591553\n",
      "80 0.1588887870311737\n",
      "81 0.15660548210144043\n",
      "82 0.15435457229614258\n",
      "83 0.15213629603385925\n",
      "84 0.1499497890472412\n",
      "85 0.1477949470281601\n",
      "86 0.14567101001739502\n",
      "87 0.143577441573143\n",
      "88 0.14151394367218018\n",
      "89 0.13948014378547668\n",
      "90 0.13747571408748627\n",
      "91 0.13550004363059998\n",
      "92 0.1335524618625641\n",
      "93 0.13163314759731293\n",
      "94 0.12974154949188232\n",
      "95 0.12787675857543945\n",
      "96 0.12603899836540222\n",
      "97 0.1242276281118393\n",
      "98 0.12244215607643127\n",
      "99 0.12068241834640503\n",
      "100 0.1189480721950531\n",
      "101 0.11723857372999191\n",
      "102 0.11555371433496475\n",
      "103 0.11389299482107162\n",
      "104 0.11225626617670059\n",
      "105 0.11064288020133972\n",
      "106 0.10905289649963379\n",
      "107 0.10748565196990967\n",
      "108 0.10594084113836288\n",
      "109 0.10441839694976807\n",
      "110 0.10291774570941925\n",
      "111 0.10143858939409256\n",
      "112 0.09998071193695068\n",
      "113 0.09854389727115631\n",
      "114 0.09712764620780945\n",
      "115 0.09573188424110413\n",
      "116 0.09435602277517319\n",
      "117 0.09299997985363007\n",
      "118 0.09166335314512253\n",
      "119 0.09034591913223267\n",
      "120 0.08904750645160675\n",
      "121 0.08776774257421494\n",
      "122 0.08650657534599304\n",
      "123 0.08526315540075302\n",
      "124 0.08403795957565308\n",
      "125 0.08283012360334396\n",
      "126 0.08163981139659882\n",
      "127 0.08046650141477585\n",
      "128 0.07930994033813477\n",
      "129 0.07817019522190094\n",
      "130 0.07704676687717438\n",
      "131 0.07593941688537598\n",
      "132 0.07484806329011917\n",
      "133 0.07377244532108307\n",
      "134 0.07271228730678558\n",
      "135 0.07166723161935806\n",
      "136 0.07063724100589752\n",
      "137 0.06962216645479202\n",
      "138 0.06862156093120575\n",
      "139 0.06763528287410736\n",
      "140 0.06666325777769089\n",
      "141 0.06570517271757126\n",
      "142 0.06476092338562012\n",
      "143 0.06383025646209717\n",
      "144 0.06291291117668152\n",
      "145 0.062008682638406754\n",
      "146 0.06111748516559601\n",
      "147 0.060239166021347046\n",
      "148 0.05937352776527405\n",
      "149 0.05852022394537926\n",
      "150 0.05767911672592163\n",
      "151 0.0568501353263855\n",
      "152 0.05603310465812683\n",
      "153 0.05522787570953369\n",
      "154 0.05443419888615608\n",
      "155 0.05365191027522087\n",
      "156 0.05288086459040642\n",
      "157 0.05212075263261795\n",
      "158 0.051371749490499496\n",
      "159 0.05063345283269882\n",
      "160 0.049905773252248764\n",
      "161 0.04918859153985977\n",
      "162 0.048481639474630356\n",
      "163 0.04778491333127022\n",
      "164 0.04709811881184578\n",
      "165 0.04642127454280853\n",
      "166 0.04575406387448311\n",
      "167 0.045096561312675476\n",
      "168 0.044448383152484894\n",
      "169 0.04380965232849121\n",
      "170 0.043179966509342194\n",
      "171 0.04255952313542366\n",
      "172 0.041947804391384125\n",
      "173 0.041344933211803436\n",
      "174 0.04075082391500473\n",
      "175 0.040165118873119354\n",
      "176 0.03958785533905029\n",
      "177 0.03901900723576546\n",
      "178 0.03845816105604172\n",
      "179 0.037905462086200714\n",
      "180 0.03736069053411484\n",
      "181 0.03682372719049454\n",
      "182 0.0362946093082428\n",
      "183 0.03577292710542679\n",
      "184 0.03525886312127113\n",
      "185 0.034752096980810165\n",
      "186 0.034252673387527466\n",
      "187 0.03376037999987602\n",
      "188 0.03327514976263046\n",
      "189 0.03279698267579079\n",
      "190 0.0323256179690361\n",
      "191 0.03186113387346268\n",
      "192 0.031403180211782455\n",
      "193 0.03095184452831745\n",
      "194 0.030507046729326248\n",
      "195 0.030068615451455116\n",
      "196 0.0296364463865757\n",
      "197 0.02921050786972046\n",
      "198 0.02879074215888977\n",
      "199 0.028376949951052666\n",
      "200 0.0279691219329834\n",
      "201 0.027567196637392044\n",
      "202 0.027171015739440918\n",
      "203 0.026780512183904648\n",
      "204 0.026395604014396667\n",
      "205 0.02601625770330429\n",
      "206 0.0256423931568861\n",
      "207 0.025273840874433517\n",
      "208 0.024910664185881615\n",
      "209 0.024552594870328903\n",
      "210 0.024199804291129112\n",
      "211 0.023851998150348663\n",
      "212 0.023509260267019272\n",
      "213 0.023171383887529373\n",
      "214 0.022838380187749863\n",
      "215 0.022510085254907608\n",
      "216 0.022186610847711563\n",
      "217 0.021867785602808\n",
      "218 0.02155352383852005\n",
      "219 0.021243728697299957\n",
      "220 0.020938387140631676\n",
      "221 0.020637501031160355\n",
      "222 0.020340900868177414\n",
      "223 0.020048536360263824\n",
      "224 0.019760414958000183\n",
      "225 0.019476400688290596\n",
      "226 0.019196534529328346\n",
      "227 0.018920671194791794\n",
      "228 0.01864870823919773\n",
      "229 0.018380779772996902\n",
      "230 0.018116528168320656\n",
      "231 0.017856169492006302\n",
      "232 0.017599549144506454\n",
      "233 0.017346661537885666\n",
      "234 0.017097312957048416\n",
      "235 0.016851620748639107\n",
      "236 0.016609443351626396\n",
      "237 0.016370752826333046\n",
      "238 0.01613551191985607\n",
      "239 0.015903549268841743\n",
      "240 0.015675019472837448\n",
      "241 0.015449749305844307\n",
      "242 0.015227709896862507\n",
      "243 0.015008828602731228\n",
      "244 0.01479313150048256\n",
      "245 0.0145805599167943\n",
      "246 0.014371027238667011\n",
      "247 0.014164526015520096\n",
      "248 0.013960892334580421\n",
      "249 0.013760330155491829\n",
      "250 0.013562524691224098\n",
      "251 0.013367651030421257\n",
      "252 0.01317552849650383\n",
      "253 0.012986162677407265\n",
      "254 0.012799521908164024\n",
      "255 0.012615597806870937\n",
      "256 0.012434222735464573\n",
      "257 0.012255558744072914\n",
      "258 0.012079420499503613\n",
      "259 0.011905800551176071\n",
      "260 0.011734690517187119\n",
      "261 0.01156605128198862\n",
      "262 0.0113998306915164\n",
      "263 0.011236058548092842\n",
      "264 0.011074516922235489\n",
      "265 0.010915391147136688\n",
      "266 0.010758520103991032\n",
      "267 0.01060391589999199\n",
      "268 0.010451482608914375\n",
      "269 0.010301316156983376\n",
      "270 0.010153237730264664\n",
      "271 0.010007365606725216\n",
      "272 0.009863495826721191\n",
      "273 0.009721780195832253\n",
      "274 0.009582011960446835\n",
      "275 0.009444329887628555\n",
      "276 0.009308608248829842\n",
      "277 0.009174849838018417\n",
      "278 0.009042943827807903\n",
      "279 0.008913019672036171\n",
      "280 0.008784879930317402\n",
      "281 0.008658679202198982\n",
      "282 0.008534250780940056\n",
      "283 0.008411572314798832\n",
      "284 0.008290670812129974\n",
      "285 0.008171548135578632\n",
      "286 0.008054071106016636\n",
      "287 0.007938324473798275\n",
      "288 0.007824264466762543\n",
      "289 0.007711815647780895\n",
      "290 0.007600957062095404\n",
      "291 0.0074917711317539215\n",
      "292 0.007384074851870537\n",
      "293 0.007277937605977058\n",
      "294 0.007173370569944382\n",
      "295 0.00707026943564415\n",
      "296 0.006968633271753788\n",
      "297 0.006868479307740927\n",
      "298 0.006769821047782898\n",
      "299 0.0066725024953484535\n",
      "300 0.00657663494348526\n",
      "301 0.006482082884758711\n",
      "302 0.006388912443071604\n",
      "303 0.006297104526311159\n",
      "304 0.00620661024004221\n",
      "305 0.006117422599345446\n",
      "306 0.0060295164585113525\n",
      "307 0.005942820571362972\n",
      "308 0.005857410375028849\n",
      "309 0.005773250944912434\n",
      "310 0.005690252408385277\n",
      "311 0.005608477629721165\n",
      "312 0.005527883302420378\n",
      "313 0.005448442883789539\n",
      "314 0.005370177328586578\n",
      "315 0.005292957182973623\n",
      "316 0.005216887686401606\n",
      "317 0.005141936242580414\n",
      "318 0.005068027414381504\n",
      "319 0.004995221272110939\n",
      "320 0.0049234479665756226\n",
      "321 0.004852659069001675\n",
      "322 0.004782934207469225\n",
      "323 0.004714164882898331\n",
      "324 0.004646455869078636\n",
      "325 0.004579642787575722\n",
      "326 0.004513836465775967\n",
      "327 0.004448971711099148\n",
      "328 0.004385008011013269\n",
      "329 0.004322031047195196\n",
      "330 0.004259900655597448\n",
      "331 0.004198694135993719\n",
      "332 0.004138320218771696\n",
      "333 0.00407883245497942\n",
      "334 0.004020217806100845\n",
      "335 0.00396245950832963\n",
      "336 0.0039055077359080315\n",
      "337 0.003849368542432785\n",
      "338 0.003794061951339245\n",
      "339 0.0037395255640149117\n",
      "340 0.0036857863888144493\n",
      "341 0.0036328004207462072\n",
      "342 0.0035806121304631233\n",
      "343 0.003529136534780264\n",
      "344 0.0034784418530762196\n",
      "345 0.003428450785577297\n",
      "346 0.0033791628666222095\n",
      "347 0.0033305943943560123\n",
      "348 0.003282747231423855\n",
      "349 0.0032355671282857656\n",
      "350 0.003189053386449814\n",
      "351 0.003143231850117445\n",
      "352 0.003098065499216318\n",
      "353 0.0030535124242305756\n",
      "354 0.0030096545815467834\n",
      "355 0.002966380212455988\n",
      "356 0.0029237556736916304\n",
      "357 0.0028817360289394855\n",
      "358 0.002840318251401186\n",
      "359 0.0027994983829557896\n",
      "360 0.0027592582628130913\n",
      "361 0.002719615586102009\n",
      "362 0.0026805305387824774\n",
      "363 0.002642006380483508\n",
      "364 0.0026040372904390097\n",
      "365 0.0025666167493909597\n",
      "366 0.0025297419633716345\n",
      "367 0.0024933575186878443\n",
      "368 0.0024575393181294203\n",
      "369 0.00242221774533391\n",
      "370 0.002387424698099494\n",
      "371 0.0023531089536845684\n",
      "372 0.0023192795924842358\n",
      "373 0.0022859591990709305\n",
      "374 0.002253092359751463\n",
      "375 0.0022207212168723345\n",
      "376 0.0021887964103370905\n",
      "377 0.002157337963581085\n",
      "378 0.002126337494701147\n",
      "379 0.002095784991979599\n",
      "380 0.002065638778731227\n",
      "381 0.0020359731279313564\n",
      "382 0.002006704453378916\n",
      "383 0.0019778672140091658\n",
      "384 0.00194944953545928\n",
      "385 0.0019214412895962596\n",
      "386 0.0018938153516501188\n",
      "387 0.0018666130490601063\n",
      "388 0.0018397695384919643\n",
      "389 0.0018133434932678938\n",
      "390 0.001787275541573763\n",
      "391 0.0017615989781916142\n",
      "392 0.0017362706130370498\n",
      "393 0.001711317920126021\n",
      "394 0.001686708303168416\n",
      "395 0.0016624972922727466\n",
      "396 0.0016385954804718494\n",
      "397 0.0016150374431163073\n",
      "398 0.001591828535310924\n",
      "399 0.0015689465217292309\n",
      "400 0.0015463947784155607\n",
      "401 0.0015241948422044516\n",
      "402 0.0015022903680801392\n",
      "403 0.001480674371123314\n",
      "404 0.0014594215899705887\n",
      "405 0.0014384326059371233\n",
      "406 0.0014177599223330617\n",
      "407 0.0013973802560940385\n",
      "408 0.0013772991951555014\n",
      "409 0.0013575199991464615\n",
      "410 0.0013379964511841536\n",
      "411 0.00131877267267555\n",
      "412 0.0012998264282941818\n",
      "413 0.0012811420019716024\n",
      "414 0.0012627190444618464\n",
      "415 0.001244562678039074\n",
      "416 0.001226682448759675\n",
      "417 0.0012090629898011684\n",
      "418 0.0011916874209418893\n",
      "419 0.0011745592346414924\n",
      "420 0.001157679595053196\n",
      "421 0.0011410302249714732\n",
      "422 0.001124649541452527\n",
      "423 0.0011084802681580186\n",
      "424 0.0010925395181402564\n",
      "425 0.0010768534848466516\n",
      "426 0.0010613680351525545\n",
      "427 0.001046107616275549\n",
      "428 0.0010310809593647718\n",
      "429 0.0010162629187107086\n",
      "430 0.0010016626911237836\n",
      "431 0.000987261300906539\n",
      "432 0.0009730728925205767\n",
      "433 0.0009590791305527091\n",
      "434 0.0009453136008232832\n",
      "435 0.0009317203075625002\n",
      "436 0.0009183224174194038\n",
      "437 0.0009051369852386415\n",
      "438 0.0008921354310587049\n",
      "439 0.0008792986627668142\n",
      "440 0.0008666798821650445\n",
      "441 0.0008542134892195463\n",
      "442 0.0008419386576861143\n",
      "443 0.0008298314060084522\n",
      "444 0.0008179081487469375\n",
      "445 0.0008061587577685714\n",
      "446 0.0007945779361762106\n",
      "447 0.0007831558468751609\n",
      "448 0.0007718906854279339\n",
      "449 0.0007607969455420971\n",
      "450 0.0007498599006794393\n",
      "451 0.000739082635845989\n",
      "452 0.0007284681778401136\n",
      "453 0.0007179974345490336\n",
      "454 0.0007076860638335347\n",
      "455 0.0006975153228268027\n",
      "456 0.0006874776445329189\n",
      "457 0.0006776005029678345\n",
      "458 0.0006678638746961951\n",
      "459 0.0006582738715223968\n",
      "460 0.0006487991777248681\n",
      "461 0.0006394842639565468\n",
      "462 0.0006302877445705235\n",
      "463 0.000621243380010128\n",
      "464 0.0006123089697211981\n",
      "465 0.0006035121623426676\n",
      "466 0.0005948328762315214\n",
      "467 0.0005862938705831766\n",
      "468 0.0005778515478596091\n",
      "469 0.000569549563806504\n",
      "470 0.0005613778484985232\n",
      "471 0.0005533035146072507\n",
      "472 0.0005453609628602862\n",
      "473 0.0005375136388465762\n",
      "474 0.0005297861061990261\n",
      "475 0.0005221755709499121\n",
      "476 0.0005146741750650108\n",
      "477 0.0005072781932540238\n",
      "478 0.0004999759839847684\n",
      "479 0.0004928028793074191\n",
      "480 0.0004857213352806866\n",
      "481 0.00047872410505078733\n",
      "482 0.00047184969298541546\n",
      "483 0.0004650763003155589\n",
      "484 0.0004583843401633203\n",
      "485 0.00045180137385614216\n",
      "486 0.0004453138681128621\n",
      "487 0.00043890741653740406\n",
      "488 0.00043259820085950196\n",
      "489 0.00042638383456505835\n",
      "490 0.00042026094160974026\n",
      "491 0.0004142154648434371\n",
      "492 0.0004082607920281589\n",
      "493 0.00040239933878183365\n",
      "494 0.00039661736809648573\n",
      "495 0.000390910601709038\n",
      "496 0.00038529420271515846\n",
      "497 0.00037975242594256997\n",
      "498 0.0003743025881703943\n",
      "499 0.00036891555646434426\n",
      "predict (after training) 4 7.9779205322265625\n"
     ]
    }
   ],
   "source": [
    "# 5. linear regression\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (after training)\", 4, model(hour_var).data[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5842203497886658\n",
      "1 0.5822995901107788\n",
      "2 0.5804102420806885\n",
      "3 0.5785517692565918\n",
      "4 0.5767239928245544\n",
      "5 0.574926495552063\n",
      "6 0.5731590986251831\n",
      "7 0.5714214444160461\n",
      "8 0.5697131156921387\n",
      "9 0.5680338740348816\n",
      "10 0.5663833618164062\n",
      "11 0.5647613406181335\n",
      "12 0.5631673336029053\n",
      "13 0.5616010427474976\n",
      "14 0.5600622296333313\n",
      "15 0.5585504174232483\n",
      "16 0.5570654273033142\n",
      "17 0.5556067824363708\n",
      "18 0.5541740655899048\n",
      "19 0.5527670979499817\n",
      "20 0.5513855218887329\n",
      "21 0.5500289797782898\n",
      "22 0.5486970543861389\n",
      "23 0.5473893284797668\n",
      "24 0.5461056232452393\n",
      "25 0.544845461845398\n",
      "26 0.5436086058616638\n",
      "27 0.5423946380615234\n",
      "28 0.5412032008171082\n",
      "29 0.5400339961051941\n",
      "30 0.5388866066932678\n",
      "31 0.5377606749534607\n",
      "32 0.5366560220718384\n",
      "33 0.5355721116065979\n",
      "34 0.5345088243484497\n",
      "35 0.5334655046463013\n",
      "36 0.532442033290863\n",
      "37 0.531437873840332\n",
      "38 0.5304530262947083\n",
      "39 0.5294869542121887\n",
      "40 0.5285392999649048\n",
      "41 0.5276097059249878\n",
      "42 0.5266979336738586\n",
      "43 0.5258036851882935\n",
      "44 0.5249266624450684\n",
      "45 0.5240663290023804\n",
      "46 0.5232226252555847\n",
      "47 0.5223950743675232\n",
      "48 0.5215834379196167\n",
      "49 0.5207874178886414\n",
      "50 0.5200066566467285\n",
      "51 0.5192409753799438\n",
      "52 0.5184898972511292\n",
      "53 0.5177532434463501\n",
      "54 0.517030656337738\n",
      "55 0.5163219571113586\n",
      "56 0.5156267285346985\n",
      "57 0.514944851398468\n",
      "58 0.5142759084701538\n",
      "59 0.5136197209358215\n",
      "60 0.5129759907722473\n",
      "61 0.5123444199562073\n",
      "62 0.5117247700691223\n",
      "63 0.5111167430877686\n",
      "64 0.5105202198028564\n",
      "65 0.5099348425865173\n",
      "66 0.5093603730201721\n",
      "67 0.5087965726852417\n",
      "68 0.5082433223724365\n",
      "69 0.5077001452445984\n",
      "70 0.5071669816970825\n",
      "71 0.5066435933113098\n",
      "72 0.5061296820640564\n",
      "73 0.5056252479553223\n",
      "74 0.5051297545433044\n",
      "75 0.5046432614326477\n",
      "76 0.5041653513908386\n",
      "77 0.5036960244178772\n",
      "78 0.5032349228858948\n",
      "79 0.5027819275856018\n",
      "80 0.5023368000984192\n",
      "81 0.5018994212150574\n",
      "82 0.5014695525169373\n",
      "83 0.5010470151901245\n",
      "84 0.5006316304206848\n",
      "85 0.5002233386039734\n",
      "86 0.4998217225074768\n",
      "87 0.49942687153816223\n",
      "88 0.4990385174751282\n",
      "89 0.49865636229515076\n",
      "90 0.49828049540519714\n",
      "91 0.49791064858436584\n",
      "92 0.49754661321640015\n",
      "93 0.4971883296966553\n",
      "94 0.49683570861816406\n",
      "95 0.49648839235305786\n",
      "96 0.49614647030830383\n",
      "97 0.4958096444606781\n",
      "98 0.49547797441482544\n",
      "99 0.4951511323451996\n",
      "100 0.4948291480541229\n",
      "101 0.49451175332069397\n",
      "102 0.4941989481449127\n",
      "103 0.4938904643058777\n",
      "104 0.49358639121055603\n",
      "105 0.49328646063804626\n",
      "106 0.4929906725883484\n",
      "107 0.49269887804985046\n",
      "108 0.4924108684062958\n",
      "109 0.4921266734600067\n",
      "110 0.4918462038040161\n",
      "111 0.49156931042671204\n",
      "112 0.49129584431648254\n",
      "113 0.49102583527565\n",
      "114 0.49075907468795776\n",
      "115 0.4904955327510834\n",
      "116 0.49023517966270447\n",
      "117 0.4899778664112091\n",
      "118 0.4897235035896301\n",
      "119 0.48947203159332275\n",
      "120 0.48922333121299744\n",
      "121 0.4889774024486542\n",
      "122 0.4887341856956482\n",
      "123 0.4884934425354004\n",
      "124 0.4882553219795227\n",
      "125 0.4880196750164032\n",
      "126 0.48778629302978516\n",
      "127 0.4875553846359253\n",
      "128 0.48732665181159973\n",
      "129 0.48710009455680847\n",
      "130 0.4868757128715515\n",
      "131 0.4866534471511841\n",
      "132 0.4864331781864166\n",
      "133 0.48621487617492676\n",
      "134 0.4859984815120697\n",
      "135 0.48578396439552307\n",
      "136 0.4855712652206421\n",
      "137 0.485360324382782\n",
      "138 0.48515117168426514\n",
      "139 0.48494359850883484\n",
      "140 0.48473772406578064\n",
      "141 0.4845333695411682\n",
      "142 0.48433059453964233\n",
      "143 0.48412925004959106\n",
      "144 0.48392942547798157\n",
      "145 0.48373109102249146\n",
      "146 0.4835340082645416\n",
      "147 0.4833383560180664\n",
      "148 0.48314398527145386\n",
      "149 0.4829508364200592\n",
      "150 0.4827589988708496\n",
      "151 0.48256832361221313\n",
      "152 0.4823788106441498\n",
      "153 0.48219045996665955\n",
      "154 0.48200318217277527\n",
      "155 0.4818170666694641\n",
      "156 0.4816319942474365\n",
      "157 0.48144787549972534\n",
      "158 0.48126479983329773\n",
      "159 0.4810826778411865\n",
      "160 0.4809015095233917\n",
      "161 0.48072126507759094\n",
      "162 0.4805418848991394\n",
      "163 0.4803633391857147\n",
      "164 0.48018574714660645\n",
      "165 0.48000890016555786\n",
      "166 0.47983288764953613\n",
      "167 0.4796576499938965\n",
      "168 0.4794831871986389\n",
      "169 0.47930946946144104\n",
      "170 0.47913646697998047\n",
      "171 0.4789642095565796\n",
      "172 0.4787925183773041\n",
      "173 0.47862157225608826\n",
      "174 0.47845128178596497\n",
      "175 0.4782816767692566\n",
      "176 0.4781126081943512\n",
      "177 0.47794416546821594\n",
      "178 0.47777631878852844\n",
      "179 0.4776090085506439\n",
      "180 0.47744232416152954\n",
      "181 0.47727611660957336\n",
      "182 0.47711047530174255\n",
      "183 0.4769454002380371\n",
      "184 0.47678080201148987\n",
      "185 0.47661665081977844\n",
      "186 0.4764529764652252\n",
      "187 0.476289838552475\n",
      "188 0.47612714767456055\n",
      "189 0.47596487402915955\n",
      "190 0.475803017616272\n",
      "191 0.4756416380405426\n",
      "192 0.4754806160926819\n",
      "193 0.47532010078430176\n",
      "194 0.4751598536968231\n",
      "195 0.4750001132488251\n",
      "196 0.4748407006263733\n",
      "197 0.4746816158294678\n",
      "198 0.4745229482650757\n",
      "199 0.47436460852622986\n",
      "200 0.47420668601989746\n",
      "201 0.47404903173446655\n",
      "202 0.47389164566993713\n",
      "203 0.47373470664024353\n",
      "204 0.4735780656337738\n",
      "205 0.47342172265052795\n",
      "206 0.4732656180858612\n",
      "207 0.4731098711490631\n",
      "208 0.4729544222354889\n",
      "209 0.47279927134513855\n",
      "210 0.4726443588733673\n",
      "211 0.47248974442481995\n",
      "212 0.4723353385925293\n",
      "213 0.4721812307834625\n",
      "214 0.47202742099761963\n",
      "215 0.47187384963035583\n",
      "216 0.47172054648399353\n",
      "217 0.47156742215156555\n",
      "218 0.47141456604003906\n",
      "219 0.4712619185447693\n",
      "220 0.471109539270401\n",
      "221 0.47095736861228943\n",
      "222 0.47080543637275696\n",
      "223 0.4706537425518036\n",
      "224 0.47050222754478455\n",
      "225 0.4703509509563446\n",
      "226 0.4701998233795166\n",
      "227 0.4700489342212677\n",
      "228 0.4698982834815979\n",
      "229 0.4697478115558624\n",
      "230 0.4695974886417389\n",
      "231 0.4694473445415497\n",
      "232 0.46929749846458435\n",
      "233 0.4691477417945862\n",
      "234 0.46899816393852234\n",
      "235 0.4688488245010376\n",
      "236 0.4686996340751648\n",
      "237 0.46855059266090393\n",
      "238 0.4684017598628998\n",
      "239 0.46825307607650757\n",
      "240 0.46810463070869446\n",
      "241 0.4679562449455261\n",
      "242 0.4678080379962921\n",
      "243 0.46765998005867004\n",
      "244 0.4675121605396271\n",
      "245 0.46736449003219604\n",
      "246 0.4672168791294098\n",
      "247 0.46706944704055786\n",
      "248 0.46692222356796265\n",
      "249 0.46677514910697937\n",
      "250 0.46662819385528564\n",
      "251 0.46648138761520386\n",
      "252 0.4663347005844116\n",
      "253 0.4661881625652313\n",
      "254 0.4660417437553406\n",
      "255 0.4658955931663513\n",
      "256 0.46574944257736206\n",
      "257 0.46560344099998474\n",
      "258 0.46545758843421936\n",
      "259 0.4653119146823883\n",
      "260 0.4651663601398468\n",
      "261 0.46502089500427246\n",
      "262 0.46487560868263245\n",
      "263 0.4647304117679596\n",
      "264 0.4645853042602539\n",
      "265 0.46444040536880493\n",
      "266 0.4642956256866455\n",
      "267 0.464150995016098\n",
      "268 0.4640063941478729\n",
      "269 0.46386194229125977\n",
      "270 0.4637176990509033\n",
      "271 0.4635734260082245\n",
      "272 0.46342939138412476\n",
      "273 0.4632854163646698\n",
      "274 0.4631415605545044\n",
      "275 0.4629978537559509\n",
      "276 0.4628542363643646\n",
      "277 0.46271079778671265\n",
      "278 0.46256735920906067\n",
      "279 0.462424099445343\n",
      "280 0.4622809886932373\n",
      "281 0.46213796734809875\n",
      "282 0.461995005607605\n",
      "283 0.46185219287872314\n",
      "284 0.46170949935913086\n",
      "285 0.46156689524650574\n",
      "286 0.46142441034317017\n",
      "287 0.46128207445144653\n",
      "288 0.4611397981643677\n",
      "289 0.46099764108657837\n",
      "290 0.4608556032180786\n",
      "291 0.460713654756546\n",
      "292 0.4605717957019806\n",
      "293 0.4604301154613495\n",
      "294 0.46028846502304077\n",
      "295 0.460146963596344\n",
      "296 0.460005521774292\n",
      "297 0.45986419916152954\n",
      "298 0.45972296595573425\n",
      "299 0.4595818519592285\n",
      "300 0.4594408869743347\n",
      "301 0.4592999219894409\n",
      "302 0.4591591954231262\n",
      "303 0.45901843905448914\n",
      "304 0.4588778018951416\n",
      "305 0.458737313747406\n",
      "306 0.4585969150066376\n",
      "307 0.4584566056728363\n",
      "308 0.4583164155483246\n",
      "309 0.45817625522613525\n",
      "310 0.45803627371788025\n",
      "311 0.4578963816165924\n",
      "312 0.45775657892227173\n",
      "313 0.4576168358325958\n",
      "314 0.4574772119522095\n",
      "315 0.45733773708343506\n",
      "316 0.45719823241233826\n",
      "317 0.45705893635749817\n",
      "318 0.45691969990730286\n",
      "319 0.4567805826663971\n",
      "320 0.4566415250301361\n",
      "321 0.4565025568008423\n",
      "322 0.4563637673854828\n",
      "323 0.4562250077724457\n",
      "324 0.45608633756637573\n",
      "325 0.45594772696495056\n",
      "326 0.4558092951774597\n",
      "327 0.45567095279693604\n",
      "328 0.45553261041641235\n",
      "329 0.4553944766521454\n",
      "330 0.4552563428878784\n",
      "331 0.4551183581352234\n",
      "332 0.45498037338256836\n",
      "333 0.45484259724617004\n",
      "334 0.4547049105167389\n",
      "335 0.45456719398498535\n",
      "336 0.4544296860694885\n",
      "337 0.4542922079563141\n",
      "338 0.4541548490524292\n",
      "339 0.45401760935783386\n",
      "340 0.4538804590702057\n",
      "341 0.4537433385848999\n",
      "342 0.45360639691352844\n",
      "343 0.45346948504447937\n",
      "344 0.45333266258239746\n",
      "345 0.4531959295272827\n",
      "346 0.4530593454837799\n",
      "347 0.4529228210449219\n",
      "348 0.452786386013031\n",
      "349 0.4526500105857849\n",
      "350 0.452513724565506\n",
      "351 0.4523774981498718\n",
      "352 0.4522414803504944\n",
      "353 0.45210546255111694\n",
      "354 0.45196953415870667\n",
      "355 0.45183372497558594\n",
      "356 0.45169797539711\n",
      "357 0.4515623450279236\n",
      "358 0.45142680406570435\n",
      "359 0.45129138231277466\n",
      "360 0.4511559307575226\n",
      "361 0.4510206878185272\n",
      "362 0.45088547468185425\n",
      "363 0.4507504105567932\n",
      "364 0.4506153166294098\n",
      "365 0.4504804313182831\n",
      "366 0.45034557580947876\n",
      "367 0.450210839509964\n",
      "368 0.450076162815094\n",
      "369 0.44994163513183594\n",
      "370 0.4498071074485779\n",
      "371 0.4496726989746094\n",
      "372 0.44953832030296326\n",
      "373 0.44940415024757385\n",
      "374 0.44926998019218445\n",
      "375 0.4491358995437622\n",
      "376 0.44900190830230713\n",
      "377 0.4488680362701416\n",
      "378 0.4487342834472656\n",
      "379 0.4486005902290344\n",
      "380 0.4484668970108032\n",
      "381 0.44833338260650635\n",
      "382 0.44819995760917664\n",
      "383 0.4480665624141693\n",
      "384 0.44793325662612915\n",
      "385 0.44780007004737854\n",
      "386 0.4476669430732727\n",
      "387 0.4475339353084564\n",
      "388 0.4474009871482849\n",
      "389 0.44726812839508057\n",
      "390 0.44713538885116577\n",
      "391 0.4470027685165405\n",
      "392 0.4468701183795929\n",
      "393 0.4467376172542572\n",
      "394 0.4466051757335663\n",
      "395 0.4464728534221649\n",
      "396 0.4463405907154083\n",
      "397 0.4462084174156189\n",
      "398 0.446076363325119\n",
      "399 0.44594430923461914\n",
      "400 0.445812463760376\n",
      "401 0.4456805884838104\n",
      "402 0.44554880261421204\n",
      "403 0.4454171657562256\n",
      "404 0.4452855587005615\n",
      "405 0.4451541304588318\n",
      "406 0.44502273201942444\n",
      "407 0.44489139318466187\n",
      "408 0.44476011395454407\n",
      "409 0.4446289539337158\n",
      "410 0.4444979131221771\n",
      "411 0.4443669319152832\n",
      "412 0.44423601031303406\n",
      "413 0.4441051483154297\n",
      "414 0.44397443532943726\n",
      "415 0.4438437819480896\n",
      "416 0.4437132179737091\n",
      "417 0.4435827434062958\n",
      "418 0.44345229864120483\n",
      "419 0.44332197308540344\n",
      "420 0.4431917667388916\n",
      "421 0.44306156039237976\n",
      "422 0.44293153285980225\n",
      "423 0.4428015351295471\n",
      "424 0.44267165660858154\n",
      "425 0.44254180788993835\n",
      "426 0.44241201877593994\n",
      "427 0.44228240847587585\n",
      "428 0.44215279817581177\n",
      "429 0.44202330708503723\n",
      "430 0.4418938457965851\n",
      "431 0.44176459312438965\n",
      "432 0.4416353404521942\n",
      "433 0.44150614738464355\n",
      "434 0.44137707352638245\n",
      "435 0.4412480592727661\n",
      "436 0.44111916422843933\n",
      "437 0.44099029898643494\n",
      "438 0.4408615529537201\n",
      "439 0.44073283672332764\n",
      "440 0.4406042695045471\n",
      "441 0.4404757618904114\n",
      "442 0.4403473138809204\n",
      "443 0.440218985080719\n",
      "444 0.44009074568748474\n",
      "445 0.4399625360965729\n",
      "446 0.43983447551727295\n",
      "447 0.43970638513565063\n",
      "448 0.43957847356796265\n",
      "449 0.43945059180259705\n",
      "450 0.439322829246521\n",
      "451 0.4391951262950897\n",
      "452 0.43906745314598083\n",
      "453 0.4389399588108063\n",
      "454 0.4388124644756317\n",
      "455 0.4386851489543915\n",
      "456 0.43855777382850647\n",
      "457 0.43843063712120056\n",
      "458 0.43830347061157227\n",
      "459 0.43817636370658875\n",
      "460 0.4380494952201843\n",
      "461 0.43792253732681274\n",
      "462 0.4377956986427307\n",
      "463 0.43766897916793823\n",
      "464 0.43754228949546814\n",
      "465 0.43741574883461\n",
      "466 0.4372892677783966\n",
      "467 0.4371628761291504\n",
      "468 0.4370364844799042\n",
      "469 0.4369102716445923\n",
      "470 0.436784029006958\n",
      "471 0.43665793538093567\n",
      "472 0.4365319609642029\n",
      "473 0.4364059865474701\n",
      "474 0.43628013134002686\n",
      "475 0.4361543357372284\n",
      "476 0.4360286593437195\n",
      "477 0.43590301275253296\n",
      "478 0.4357774555683136\n",
      "479 0.4356519877910614\n",
      "480 0.43552660942077637\n",
      "481 0.4354013204574585\n",
      "482 0.435276061296463\n",
      "483 0.4351509213447571\n",
      "484 0.4350259006023407\n",
      "485 0.43490085005760193\n",
      "486 0.4347759783267975\n",
      "487 0.4346511662006378\n",
      "488 0.4345264136791229\n",
      "489 0.4344016909599304\n",
      "490 0.43427711725234985\n",
      "491 0.4341525733470917\n",
      "492 0.43402811884880066\n",
      "493 0.4339037537574768\n",
      "494 0.4337794780731201\n",
      "495 0.4336552917957306\n",
      "496 0.43353116512298584\n",
      "497 0.43340709805488586\n",
      "498 0.43328309059143066\n",
      "499 0.433159202337265\n",
      "500 0.43303537368774414\n",
      "501 0.43291163444519043\n",
      "502 0.4327879548072815\n",
      "503 0.4326643645763397\n",
      "504 0.4325408637523651\n",
      "505 0.4324173927307129\n",
      "506 0.4322940707206726\n",
      "507 0.4321708381175995\n",
      "508 0.432047575712204\n",
      "509 0.4319244623184204\n",
      "510 0.43180137872695923\n",
      "511 0.43167844414711\n",
      "512 0.43155553936958313\n",
      "513 0.43143272399902344\n",
      "514 0.4313099980354309\n",
      "515 0.43118733167648315\n",
      "516 0.4310646951198578\n",
      "517 0.43094220757484436\n",
      "518 0.4308197796344757\n",
      "519 0.43069741129875183\n",
      "520 0.4305751323699951\n",
      "521 0.4304529130458832\n",
      "522 0.4303308427333832\n",
      "523 0.4302087128162384\n",
      "524 0.43008673191070557\n",
      "525 0.4299648404121399\n",
      "526 0.429843008518219\n",
      "527 0.42972126603126526\n",
      "528 0.4295995831489563\n",
      "529 0.4294779896736145\n",
      "530 0.4293564558029175\n",
      "531 0.42923498153686523\n",
      "532 0.4291136860847473\n",
      "533 0.428992360830307\n",
      "534 0.42887112498283386\n",
      "535 0.4287499487400055\n",
      "536 0.4286288917064667\n",
      "537 0.42850789427757263\n",
      "538 0.42838698625564575\n",
      "539 0.42826613783836365\n",
      "540 0.42814531922340393\n",
      "541 0.42802464962005615\n",
      "542 0.42790403962135315\n",
      "543 0.4277834892272949\n",
      "544 0.42766305804252625\n",
      "545 0.4275425970554352\n",
      "546 0.42742234468460083\n",
      "547 0.4273020327091217\n",
      "548 0.4271818697452545\n",
      "549 0.4270617663860321\n",
      "550 0.42694178223609924\n",
      "551 0.4268217980861664\n",
      "552 0.42670193314552307\n",
      "553 0.42658212780952454\n",
      "554 0.42646241188049316\n",
      "555 0.4263427257537842\n",
      "556 0.4262232184410095\n",
      "557 0.4261036813259125\n",
      "558 0.4259842336177826\n",
      "559 0.42586490511894226\n",
      "560 0.42574557662010193\n",
      "561 0.42562636733055115\n",
      "562 0.4255072772502899\n",
      "563 0.4253882169723511\n",
      "564 0.425269216299057\n",
      "565 0.4251503050327301\n",
      "566 0.425031453371048\n",
      "567 0.424912691116333\n",
      "568 0.4247939884662628\n",
      "569 0.4246753752231598\n",
      "570 0.4245568513870239\n",
      "571 0.42443832755088806\n",
      "572 0.42431995272636414\n",
      "573 0.4242015779018402\n",
      "574 0.4240833818912506\n",
      "575 0.4239652156829834\n",
      "576 0.42384710907936096\n",
      "577 0.4237290322780609\n",
      "578 0.4236110746860504\n",
      "579 0.4234932065010071\n",
      "580 0.42337536811828613\n",
      "581 0.42325758934020996\n",
      "582 0.42313992977142334\n",
      "583 0.4230223298072815\n",
      "584 0.4229048192501068\n",
      "585 0.4227873682975769\n",
      "586 0.4226699471473694\n",
      "587 0.4225526452064514\n",
      "588 0.4224354028701782\n",
      "589 0.4223182499408722\n",
      "590 0.4222011864185333\n",
      "591 0.42208412289619446\n",
      "592 0.42196714878082275\n",
      "593 0.4218502938747406\n",
      "594 0.4217334985733032\n",
      "595 0.4216167628765106\n",
      "596 0.4215000867843628\n",
      "597 0.42138350009918213\n",
      "598 0.42126694321632385\n",
      "599 0.4211505353450775\n",
      "600 0.4210341274738312\n",
      "601 0.4209178388118744\n",
      "602 0.4208016097545624\n",
      "603 0.42068541049957275\n",
      "604 0.4205693304538727\n",
      "605 0.42045336961746216\n",
      "606 0.42033740878105164\n",
      "607 0.4202215373516083\n",
      "608 0.4201056957244873\n",
      "609 0.4199899435043335\n",
      "610 0.4198743402957916\n",
      "611 0.41975870728492737\n",
      "612 0.4196431636810303\n",
      "613 0.41952770948410034\n",
      "614 0.4194123446941376\n",
      "615 0.4192970097064972\n",
      "616 0.41918179392814636\n",
      "617 0.4190666079521179\n",
      "618 0.41895151138305664\n",
      "619 0.4188365042209625\n",
      "620 0.4187215268611908\n",
      "621 0.41860660910606384\n",
      "622 0.41849175095558167\n",
      "623 0.4183771014213562\n",
      "624 0.4182623326778412\n",
      "625 0.4181477725505829\n",
      "626 0.4180331826210022\n",
      "627 0.41791871190071106\n",
      "628 0.4178043305873871\n",
      "629 0.4176900386810303\n",
      "630 0.4175757169723511\n",
      "631 0.4174615442752838\n",
      "632 0.4173474609851837\n",
      "633 0.4172334372997284\n",
      "634 0.4171193838119507\n",
      "635 0.4170054495334625\n",
      "636 0.4168916344642639\n",
      "637 0.41677790880203247\n",
      "638 0.41666415333747864\n",
      "639 0.41655054688453674\n",
      "640 0.4164370000362396\n",
      "641 0.4163234531879425\n",
      "642 0.4162100553512573\n",
      "643 0.4160967171192169\n",
      "644 0.4159834086894989\n",
      "645 0.41587018966674805\n",
      "646 0.4157570004463196\n",
      "647 0.41564396023750305\n",
      "648 0.4155309498310089\n",
      "649 0.41541796922683716\n",
      "650 0.41530510783195496\n",
      "651 0.41519227623939514\n",
      "652 0.4150795340538025\n",
      "653 0.4149668514728546\n",
      "654 0.4148542881011963\n",
      "655 0.41474175453186035\n",
      "656 0.4146292805671692\n",
      "657 0.4145168364048004\n",
      "658 0.4144045412540436\n",
      "659 0.41429227590560913\n",
      "660 0.41418009996414185\n",
      "661 0.41406792402267456\n",
      "662 0.4139558672904968\n",
      "663 0.41384392976760864\n",
      "664 0.41373196244239807\n",
      "665 0.41362014412879944\n",
      "666 0.4135083556175232\n",
      "667 0.41339659690856934\n",
      "668 0.4132849872112274\n",
      "669 0.4131733477115631\n",
      "670 0.41306185722351074\n",
      "671 0.41295039653778076\n",
      "672 0.41283902525901794\n",
      "673 0.4127276837825775\n",
      "674 0.412616491317749\n",
      "675 0.41250526905059814\n",
      "676 0.4123941957950592\n",
      "677 0.41228315234184265\n",
      "678 0.4121721386909485\n",
      "679 0.4120612144470215\n",
      "680 0.41195037961006165\n",
      "681 0.41183963418006897\n",
      "682 0.4117289185523987\n",
      "683 0.4116182327270508\n",
      "684 0.41150766611099243\n",
      "685 0.41139715909957886\n",
      "686 0.41128671169281006\n",
      "687 0.4111763834953308\n",
      "688 0.41106605529785156\n",
      "689 0.4109557867050171\n",
      "690 0.41084563732147217\n",
      "691 0.41073551774024963\n",
      "692 0.4106254577636719\n",
      "693 0.4105154871940613\n",
      "694 0.41040557622909546\n",
      "695 0.4102957248687744\n",
      "696 0.41018593311309814\n",
      "697 0.41007617115974426\n",
      "698 0.4099665582180023\n",
      "699 0.4098569452762604\n",
      "700 0.40974748134613037\n",
      "701 0.40963801741600037\n",
      "702 0.4095286428928375\n",
      "703 0.40941929817199707\n",
      "704 0.4093100428581238\n",
      "705 0.40920087695121765\n",
      "706 0.4090917706489563\n",
      "707 0.40898266434669495\n",
      "708 0.40887364745140076\n",
      "709 0.4087647497653961\n",
      "710 0.40865588188171387\n",
      "711 0.40854713320732117\n",
      "712 0.4084383547306061\n",
      "713 0.40832966566085815\n",
      "714 0.4082210958003998\n",
      "715 0.4081125259399414\n",
      "716 0.4080040752887726\n",
      "717 0.40789565443992615\n",
      "718 0.40778738260269165\n",
      "719 0.40767908096313477\n",
      "720 0.40757089853286743\n",
      "721 0.4074627459049225\n",
      "722 0.40735459327697754\n",
      "723 0.40724658966064453\n",
      "724 0.4071386456489563\n",
      "725 0.4070308208465576\n",
      "726 0.40692293643951416\n",
      "727 0.40681523084640503\n",
      "728 0.4067075550556183\n",
      "729 0.4065999388694763\n",
      "730 0.40649229288101196\n",
      "731 0.40638479590415955\n",
      "732 0.4062773883342743\n",
      "733 0.40616998076438904\n",
      "734 0.40606269240379333\n",
      "735 0.4059554636478424\n",
      "736 0.40584829449653625\n",
      "737 0.4057411253452301\n",
      "738 0.40563416481018066\n",
      "739 0.40552711486816406\n",
      "740 0.4054202139377594\n",
      "741 0.4053133428096771\n",
      "742 0.405206561088562\n",
      "743 0.4050998091697693\n",
      "744 0.4049931764602661\n",
      "745 0.40488654375076294\n",
      "746 0.40478000044822693\n",
      "747 0.4046735465526581\n",
      "748 0.4045671224594116\n",
      "749 0.40446075797080994\n",
      "750 0.404354453086853\n",
      "751 0.40424826741218567\n",
      "752 0.4041420519351959\n",
      "753 0.4040360152721405\n",
      "754 0.4039299488067627\n",
      "755 0.40382397174835205\n",
      "756 0.4037180542945862\n",
      "757 0.4036122262477875\n",
      "758 0.40350642800331116\n",
      "759 0.403400719165802\n",
      "760 0.40329501032829285\n",
      "761 0.40318939089775085\n",
      "762 0.4030838906764984\n",
      "763 0.40297845005989075\n",
      "764 0.40287303924560547\n",
      "765 0.40276768803596497\n",
      "766 0.40266239643096924\n",
      "767 0.4025571942329407\n",
      "768 0.4024520218372345\n",
      "769 0.4023468792438507\n",
      "770 0.40224188566207886\n",
      "771 0.4021369218826294\n",
      "772 0.4020319879055023\n",
      "773 0.4019271731376648\n",
      "774 0.40182235836982727\n",
      "775 0.4017176628112793\n",
      "776 0.4016129970550537\n",
      "777 0.4015083909034729\n",
      "778 0.40140384435653687\n",
      "779 0.4012993574142456\n",
      "780 0.4011949896812439\n",
      "781 0.4010905921459198\n",
      "782 0.40098631381988525\n",
      "783 0.4008820354938507\n",
      "784 0.4007779061794281\n",
      "785 0.4006738066673279\n",
      "786 0.40056976675987244\n",
      "787 0.40046578645706177\n",
      "788 0.4003618359565735\n",
      "789 0.40025800466537476\n",
      "790 0.4001542031764984\n",
      "791 0.40005043148994446\n",
      "792 0.39994677901268005\n",
      "793 0.39984315633773804\n",
      "794 0.3997395932674408\n",
      "795 0.3996361494064331\n",
      "796 0.3995327055454254\n",
      "797 0.3994293212890625\n",
      "798 0.399325966835022\n",
      "799 0.3992227613925934\n",
      "800 0.3991195857524872\n",
      "801 0.399016410112381\n",
      "802 0.39891335368156433\n",
      "803 0.39881038665771484\n",
      "804 0.39870741963386536\n",
      "805 0.39860451221466064\n",
      "806 0.3985016942024231\n",
      "807 0.39839890599250793\n",
      "808 0.39829620718955994\n",
      "809 0.3981935977935791\n",
      "810 0.39809101819992065\n",
      "811 0.3979884684085846\n",
      "812 0.3978860080242157\n",
      "813 0.39778366684913635\n",
      "814 0.39768123626708984\n",
      "815 0.39757901430130005\n",
      "816 0.39747682213783264\n",
      "817 0.39737460017204285\n",
      "818 0.3972724378108978\n",
      "819 0.39717039465904236\n",
      "820 0.39706847071647644\n",
      "821 0.3969665467739105\n",
      "822 0.3968646824359894\n",
      "823 0.396762877702713\n",
      "824 0.3966611623764038\n",
      "825 0.396559476852417\n",
      "826 0.39645782113075256\n",
      "827 0.3963562548160553\n",
      "828 0.3962547481060028\n",
      "829 0.3961532711982727\n",
      "830 0.39605194330215454\n",
      "831 0.3959505558013916\n",
      "832 0.395849347114563\n",
      "833 0.395748108625412\n",
      "834 0.39564695954322815\n",
      "835 0.3955458700656891\n",
      "836 0.3954448103904724\n",
      "837 0.3953438103199005\n",
      "838 0.39524295926094055\n",
      "839 0.3951421082019806\n",
      "840 0.395041286945343\n",
      "841 0.3949405550956726\n",
      "842 0.3948398530483246\n",
      "843 0.3947392702102661\n",
      "844 0.3946387469768524\n",
      "845 0.39453819394111633\n",
      "846 0.3944377601146698\n",
      "847 0.39433735609054565\n",
      "848 0.3942370116710663\n",
      "849 0.3941367268562317\n",
      "850 0.39403659105300903\n",
      "851 0.3939363956451416\n",
      "852 0.39383628964424133\n",
      "853 0.3937362730503082\n",
      "854 0.3936363160610199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855 0.39353635907173157\n",
      "856 0.3934364914894104\n",
      "857 0.3933367133140564\n",
      "858 0.3932369649410248\n",
      "859 0.39313724637031555\n",
      "860 0.3930376172065735\n",
      "861 0.3929380774497986\n",
      "862 0.3928385376930237\n",
      "863 0.3927391469478607\n",
      "864 0.392639696598053\n",
      "865 0.3925403356552124\n",
      "866 0.3924410343170166\n",
      "867 0.39234185218811035\n",
      "868 0.3922426402568817\n",
      "869 0.392143577337265\n",
      "870 0.3920445144176483\n",
      "871 0.3919455111026764\n",
      "872 0.39184659719467163\n",
      "873 0.39174777269363403\n",
      "874 0.39164888858795166\n",
      "875 0.39155006408691406\n",
      "876 0.3914514183998108\n",
      "877 0.39135274291038513\n",
      "878 0.391254186630249\n",
      "879 0.3911556005477905\n",
      "880 0.3910571336746216\n",
      "881 0.3909587264060974\n",
      "882 0.390860378742218\n",
      "883 0.390762060880661\n",
      "884 0.3906637728214264\n",
      "885 0.3905656039714813\n",
      "886 0.39046749472618103\n",
      "887 0.39036935567855835\n",
      "888 0.3902713358402252\n",
      "889 0.39017337560653687\n",
      "890 0.3900754451751709\n",
      "891 0.3899776041507721\n",
      "892 0.38987982273101807\n",
      "893 0.3897820711135864\n",
      "894 0.38968437910079956\n",
      "895 0.3895867168903351\n",
      "896 0.38948917388916016\n",
      "897 0.3893916606903076\n",
      "898 0.38929420709609985\n",
      "899 0.3891967833042145\n",
      "900 0.38909944891929626\n",
      "901 0.38900214433670044\n",
      "902 0.388904869556427\n",
      "903 0.3888077437877655\n",
      "904 0.38871055841445923\n",
      "905 0.3886135220527649\n",
      "906 0.38851654529571533\n",
      "907 0.3884195387363434\n",
      "908 0.3883226811885834\n",
      "909 0.388225793838501\n",
      "910 0.38812902569770813\n",
      "911 0.3880322575569153\n",
      "912 0.3879355788230896\n",
      "913 0.3878389298915863\n",
      "914 0.38774240016937256\n",
      "915 0.3876458704471588\n",
      "916 0.3875494599342346\n",
      "917 0.38745301961898804\n",
      "918 0.3873567283153534\n",
      "919 0.38726043701171875\n",
      "920 0.3871641755104065\n",
      "921 0.3870680034160614\n",
      "922 0.3869718611240387\n",
      "923 0.38687577843666077\n",
      "924 0.38677978515625\n",
      "925 0.3866838216781616\n",
      "926 0.386587917804718\n",
      "927 0.3864920735359192\n",
      "928 0.38639628887176514\n",
      "929 0.3863005042076111\n",
      "930 0.3862048089504242\n",
      "931 0.38610923290252686\n",
      "932 0.38601362705230713\n",
      "933 0.3859180808067322\n",
      "934 0.3858226537704468\n",
      "935 0.385727196931839\n",
      "936 0.38563188910484314\n",
      "937 0.3855365812778473\n",
      "938 0.38544130325317383\n",
      "939 0.38534611463546753\n",
      "940 0.385250985622406\n",
      "941 0.38515591621398926\n",
      "942 0.3850608468055725\n",
      "943 0.3849658668041229\n",
      "944 0.3848709464073181\n",
      "945 0.3847760856151581\n",
      "946 0.38468125462532043\n",
      "947 0.38458651304244995\n",
      "948 0.38449180126190186\n",
      "949 0.38439714908599854\n",
      "950 0.3843025863170624\n",
      "951 0.3842080235481262\n",
      "952 0.38411352038383484\n",
      "953 0.38401907682418823\n",
      "954 0.3839247226715088\n",
      "955 0.38383039832115173\n",
      "956 0.38373610377311707\n",
      "957 0.38364189863204956\n",
      "958 0.38354769349098206\n",
      "959 0.3834535777568817\n",
      "960 0.38335955142974854\n",
      "961 0.38326552510261536\n",
      "962 0.38317155838012695\n",
      "963 0.3830776512622833\n",
      "964 0.38298383355140686\n",
      "965 0.3828900158405304\n",
      "966 0.3827962875366211\n",
      "967 0.3827025890350342\n",
      "968 0.38260892033576965\n",
      "969 0.3825153410434723\n",
      "970 0.3824218511581421\n",
      "971 0.3823283314704895\n",
      "972 0.38223496079444885\n",
      "973 0.3821415901184082\n",
      "974 0.38204827904701233\n",
      "975 0.38195502758026123\n",
      "976 0.38186177611351013\n",
      "977 0.38176867365837097\n",
      "978 0.38167551159858704\n",
      "979 0.38158243894577026\n",
      "980 0.38148948550224304\n",
      "981 0.3813965320587158\n",
      "982 0.381303608417511\n",
      "983 0.3812108039855957\n",
      "984 0.3811179995536804\n",
      "985 0.3810252547264099\n",
      "986 0.38093262910842896\n",
      "987 0.3808399438858032\n",
      "988 0.38074737787246704\n",
      "989 0.38065481185913086\n",
      "990 0.38056236505508423\n",
      "991 0.3804699182510376\n",
      "992 0.38037756085395813\n",
      "993 0.38028523325920105\n",
      "994 0.38019299507141113\n",
      "995 0.3801007568836212\n",
      "996 0.3800085783004761\n",
      "997 0.3799164593219757\n",
      "998 0.3798244297504425\n",
      "999 0.3797324001789093\n",
      "predict 1 hour  1.0 False\n",
      "predict 7 hours 7.0 True\n"
     ]
    }
   ],
   "source": [
    "# 6. logistic regression\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.\n",
    "        \"\"\"\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[1.0]]))\n",
    "print(\"predict 1 hour \", 1.0, model(hour_var).data[0][0] > 0.5)\n",
    "hour_var = Variable(torch.Tensor([[7.0]]))\n",
    "print(\"predict 7 hours\", 7.0, model(hour_var).data[0][0] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n",
      "0 0.6759634017944336\n",
      "1 0.6759634017944336\n",
      "2 0.6759634017944336\n",
      "3 0.6759634017944336\n",
      "4 0.6759634017944336\n",
      "5 0.6759634017944336\n",
      "6 0.6759634017944336\n",
      "7 0.6759634017944336\n",
      "8 0.6759634017944336\n",
      "9 0.6759634017944336\n",
      "10 0.6759634017944336\n",
      "11 0.6759634017944336\n",
      "12 0.6759634017944336\n",
      "13 0.6759634017944336\n",
      "14 0.6759634017944336\n",
      "15 0.6759634017944336\n",
      "16 0.6759634017944336\n",
      "17 0.6759634017944336\n",
      "18 0.6759634017944336\n",
      "19 0.6759634017944336\n",
      "20 0.6759634017944336\n",
      "21 0.6759634017944336\n",
      "22 0.6759634017944336\n",
      "23 0.6759634017944336\n",
      "24 0.6759634017944336\n",
      "25 0.6759634017944336\n",
      "26 0.6759634017944336\n",
      "27 0.6759634017944336\n",
      "28 0.6759634017944336\n",
      "29 0.6759634017944336\n",
      "30 0.6759634017944336\n",
      "31 0.6759634017944336\n",
      "32 0.6759634017944336\n",
      "33 0.6759634017944336\n",
      "34 0.6759634017944336\n",
      "35 0.6759634017944336\n",
      "36 0.6759634017944336\n",
      "37 0.6759634017944336\n",
      "38 0.6759634017944336\n",
      "39 0.6759634017944336\n",
      "40 0.6759634017944336\n",
      "41 0.6759634017944336\n",
      "42 0.6759634017944336\n",
      "43 0.6759634017944336\n",
      "44 0.6759634017944336\n",
      "45 0.6759634017944336\n",
      "46 0.6759634017944336\n",
      "47 0.6759634017944336\n",
      "48 0.6759634017944336\n",
      "49 0.6759634017944336\n",
      "50 0.6759634017944336\n",
      "51 0.6759634017944336\n",
      "52 0.6759634017944336\n",
      "53 0.6759634017944336\n",
      "54 0.6759634017944336\n",
      "55 0.6759634017944336\n",
      "56 0.6759634017944336\n",
      "57 0.6759634017944336\n",
      "58 0.6759634017944336\n",
      "59 0.6759634017944336\n",
      "60 0.6759634017944336\n",
      "61 0.6759634017944336\n",
      "62 0.6759634017944336\n",
      "63 0.6759634017944336\n",
      "64 0.6759634017944336\n",
      "65 0.6759634017944336\n",
      "66 0.6759634017944336\n",
      "67 0.6759634017944336\n",
      "68 0.6759634017944336\n",
      "69 0.6759634017944336\n",
      "70 0.6759634017944336\n",
      "71 0.6759634017944336\n",
      "72 0.6759634017944336\n",
      "73 0.6759634017944336\n",
      "74 0.6759634017944336\n",
      "75 0.6759634017944336\n",
      "76 0.6759634017944336\n",
      "77 0.6759634017944336\n",
      "78 0.6759634017944336\n",
      "79 0.6759634017944336\n",
      "80 0.6759634017944336\n",
      "81 0.6759634017944336\n",
      "82 0.6759634017944336\n",
      "83 0.6759634017944336\n",
      "84 0.6759634017944336\n",
      "85 0.6759634017944336\n",
      "86 0.6759634017944336\n",
      "87 0.6759634017944336\n",
      "88 0.6759634017944336\n",
      "89 0.6759634017944336\n",
      "90 0.6759634017944336\n",
      "91 0.6759634017944336\n",
      "92 0.6759634017944336\n",
      "93 0.6759634017944336\n",
      "94 0.6759634017944336\n",
      "95 0.6759634017944336\n",
      "96 0.6759634017944336\n",
      "97 0.6759634017944336\n",
      "98 0.6759634017944336\n",
      "99 0.6759634017944336\n"
     ]
    }
   ],
   "source": [
    "# 7. diabets logistic\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "\n",
    "print(x_data.data.shape)\n",
    "print(y_data.data.shape)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 23 inputs \n",
      "-0.6471 -0.2161 -0.1803 -0.3535 -0.7920 -0.0760 -0.8548 -0.8333\n",
      " 0.5294  0.5377  0.4426 -0.2525 -0.6690  0.2101 -0.0640 -0.4000\n",
      "-0.5294 -0.1658  0.4098 -0.6162  0.0000 -0.1267 -0.7959 -0.5667\n",
      " 0.0588  0.0653 -0.1475  0.0000  0.0000 -0.0700 -0.7421 -0.3000\n",
      "-0.7647 -0.0854  0.0164  0.0000  0.0000 -0.1863 -0.6183 -0.9667\n",
      " 0.0588  0.2261 -0.0820  0.0000  0.0000 -0.0075 -0.1153 -0.6000\n",
      " 0.1765  0.1156  0.1475 -0.4545  0.0000 -0.1803 -0.9462 -0.3667\n",
      "-0.6471  0.8794  0.1475 -0.5556 -0.5272  0.0849 -0.7182 -0.5000\n",
      "-0.7647  0.1960  0.0000  0.0000  0.0000 -0.4158 -0.3561  0.7000\n",
      "-0.4118  0.0653  0.3443 -0.3939  0.0000  0.1773 -0.8224 -0.4333\n",
      "-0.7647 -0.2563  0.0000  0.0000  0.0000  0.0000 -0.9795 -0.9667\n",
      "-0.1765  0.0653 -0.0164 -0.5152  0.0000 -0.2101 -0.8138 -0.7333\n",
      "-0.8824  0.1658  0.1475 -0.4343  0.0000 -0.1833 -0.8924  0.0000\n",
      " 0.4118  0.5176  0.1475 -0.1919 -0.3593  0.2459 -0.4330 -0.4333\n",
      " 0.1765  0.0151  0.4098 -0.2525  0.0000  0.3592 -0.0965 -0.4333\n",
      "-0.7647  0.2965  0.0000  0.0000  0.0000  0.1475 -0.8070 -0.3333\n",
      "-0.8824 -0.0653  0.1475 -0.3737  0.0000 -0.0939 -0.7976 -0.9333\n",
      "-0.1765  0.9497  0.1148 -0.4343  0.0000  0.0700 -0.4304 -0.3333\n",
      " 0.0000  0.2965  0.3115  0.0000  0.0000 -0.0700 -0.4663 -0.7333\n",
      "-0.6471  0.2663  0.4426 -0.1717 -0.4444  0.1714 -0.4654 -0.8000\n",
      " 0.0000  0.1357  0.3115 -0.6768  0.0000 -0.0760 -0.3202  0.0000\n",
      "-0.0588  0.2060  0.2787  0.0000  0.0000 -0.2548 -0.7173  0.4333\n",
      "-0.5294  0.3467  0.1803  0.0000  0.0000 -0.2906 -0.8301  0.3000\n",
      "[torch.FloatTensor of size 23x8]\n",
      " labels \n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    1\n",
      "    0\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "[torch.FloatTensor of size 23x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8-1. dataset loader\n",
    "\n",
    "\n",
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Run your training process\n",
    "print(epoch, i, \"inputs\", inputs.data, \"labels\", labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.7185835242271423\n",
      "0 1 0.743821382522583\n",
      "0 2 0.7104381918907166\n",
      "0 3 0.693465530872345\n",
      "0 4 0.7602843046188354\n",
      "0 5 0.7435920834541321\n",
      "0 6 0.7185668349266052\n",
      "0 7 0.7602136731147766\n",
      "0 8 0.7604142427444458\n",
      "0 9 0.7435073256492615\n",
      "0 10 0.7852462530136108\n",
      "0 11 0.743628203868866\n",
      "0 12 0.7186658978462219\n",
      "0 13 0.7685456871986389\n",
      "0 14 0.7268033623695374\n",
      "0 15 0.7020373344421387\n",
      "0 16 0.7693687677383423\n",
      "0 17 0.7690266966819763\n",
      "0 18 0.7692772150039673\n",
      "0 19 0.7351671457290649\n",
      "0 20 0.7352761626243591\n",
      "0 21 0.7689331769943237\n",
      "0 22 0.7439591884613037\n",
      "0 23 0.7429404854774475\n",
      "1 0 0.7354966402053833\n",
      "1 1 0.7438467741012573\n",
      "1 2 0.7019084095954895\n",
      "1 3 0.7687605023384094\n",
      "1 4 0.7520692944526672\n",
      "1 5 0.7019105553627014\n",
      "1 6 0.7352033853530884\n",
      "1 7 0.7435030341148376\n",
      "1 8 0.7774457931518555\n",
      "1 9 0.7438135147094727\n",
      "1 10 0.7352443933486938\n",
      "1 11 0.7436439990997314\n",
      "1 12 0.7772394418716431\n",
      "1 13 0.7525378465652466\n",
      "1 14 0.7434481382369995\n",
      "1 15 0.7520435452461243\n",
      "1 16 0.6851035356521606\n",
      "1 17 0.7602207660675049\n",
      "1 18 0.7356456518173218\n",
      "1 19 0.7350642681121826\n",
      "1 20 0.7523049712181091\n",
      "1 21 0.7683782577514648\n",
      "1 22 0.735562264919281\n",
      "1 23 0.7546657919883728\n"
     ]
    }
   ],
   "source": [
    "# 8-2. dataset load logistic\n",
    "\n",
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.data[0])\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 =  0.356674943939\n",
      "loss2 =  2.30258509299\n",
      "PyTorch Loss1 =  \n",
      " 0.4170\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "PyTorch Loss2= \n",
      " 1.8406\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Y_pred1= \n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Y_pred2= \n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Batch Loss1 =  \n",
      " 0.4966\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Batch Loss2= \n",
      " 1.2389\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9-1 softmax loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Cross entropy example\n",
    "import numpy as np\n",
    "# One hot\n",
    "# 0: 1 0 0\n",
    "# 1: 0 1 0\n",
    "# 2: 0 0 1\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
    "print(\"loss1 = \", np.sum(-Y * np.log(Y_pred1)))\n",
    "print(\"loss2 = \", np.sum(-Y * np.log(Y_pred2)))\n",
    "\n",
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([0]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\n",
    "Y_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"PyTorch Loss1 = \", l1.data, \"\\nPyTorch Loss2=\", l2.data)\n",
    "\n",
    "print(\"Y_pred1=\", torch.max(Y_pred1.data, 1)[1])\n",
    "print(\"Y_pred2=\", torch.max(Y_pred2.data, 1)[1])\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.9],\n",
    "                                 [1.1, 0.1, 0.2],\n",
    "                                 [0.2, 2.1, 0.1]]))\n",
    "\n",
    "\n",
    "Y_pred2 = Variable(torch.Tensor([[0.8, 0.2, 0.3],\n",
    "                                 [0.2, 0.3, 0.5],\n",
    "                                 [0.2, 0.2, 0.5]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298479\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.297069\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.302607\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.298725\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.302594\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.300864\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.289707\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.305090\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.291956\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.306223\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.301998\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.298461\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.307546\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.301401\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.296962\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.302528\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.305516\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.304045\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.284206\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.296022\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.299988\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.297812\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.291286\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.305716\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.286712\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.291094\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.294182\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.291362\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.297626\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.280185\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.284103\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.288819\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.275988\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.292259\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.288845\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.282470\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.293992\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.271616\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.283623\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.284817\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.286570\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.285668\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.293706\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.270662\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.286729\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.280771\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.273911\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.283735\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.263990\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.271905\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.270368\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.268345\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.267612\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.260761\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.269756\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.267339\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.245576\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.253635\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.237267\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.244596\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.232601\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.239309\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.220330\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.224800\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.212692\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.210917\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.218715\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.187621\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.222691\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.182013\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.166700\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.156542\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.166879\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.137254\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.108476\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.125259\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.083276\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.050751\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.995844\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.998551\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.870067\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.868936\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.856856\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.817446\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.734431\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.643513\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.687330\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.615124\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.564682\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.438267\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.528302\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.379940\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.321822\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.268555\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.149159\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.241117\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.166930\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.875516\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.994837\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.037101\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.130834\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.908704\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.994294\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.888638\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.773144\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.942737\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.816077\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.891281\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.683207\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.721060\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.793215\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.613326\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.643714\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.666867\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.628480\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.981020\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.744856\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.527272\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.727038\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.642093\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.766194\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.608955\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.445881\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.697734\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.492192\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.682988\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.581019\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.553986\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.494418\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.642271\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.487001\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.550546\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.502927\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.509528\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.597501\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.790505\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.460202\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.736533\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.763593\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.829912\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.761776\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.590410\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.514326\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.676946\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.509336\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.492998\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.569205\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.578865\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.399761\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.638712\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.397197\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.543565\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.540581\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.568431\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.699259\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.446601\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.351379\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.508316\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.563538\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.596420\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.469774\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.556306\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.460642\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.469639\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.551755\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.836338\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.383677\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.490343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.629471\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.711863\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.426627\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.461771\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.569290\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.760307\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.673574\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.481676\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.439564\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.459715\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.622063\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.304983\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.939513\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.511723\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.268162\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.593233\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.396420\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.303456\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.334687\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.628324\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.476483\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.371066\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.396746\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.464869\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.609942\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.385349\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.350978\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.396767\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.386691\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.517773\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.228196\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.488855\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.459202\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.627838\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.640414\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.302027\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.226568\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.447271\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.416919\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.488992\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.413905\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.361326\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.426443\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.232302\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.434039\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.431584\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.236120\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.300252\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.330419\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.533776\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.469094\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.524211\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.389595\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.369933\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.501148\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.426787\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.446879\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.520535\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.359685\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.270861\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.692291\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.404777\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.338528\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.271079\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.301273\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.382640\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.301076\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.314555\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.351765\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.181938\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.230180\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.331023\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.438732\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.355878\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.255288\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.242762\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.320785\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.240593\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.297377\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.306924\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.387166\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.218688\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.356611\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.252266\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.218673\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.287030\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.183272\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.387806\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.515799\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.292069\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.325362\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.274099\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.282532\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.196874\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.222060\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.301664\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.238283\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.214242\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.314408\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.236043\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.218515\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.377118\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.244349\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.295485\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.163942\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.299399\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.302367\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.341740\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.261446\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.417952\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.203910\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.187439\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.374608\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.208189\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.310627\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.246819\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.239906\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.192244\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.381134\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.136464\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.279398\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.402262\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.252283\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.189342\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.262539\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.247363\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.295231\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.241857\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.296306\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.246541\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.235011\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.388872\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.298524\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.394859\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.396098\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.282226\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.157536\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.126726\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.244831\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.247601\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.352694\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.279570\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.338215\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.537027\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.334368\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.256294\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.226164\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.207710\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.292515\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.196640\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.175953\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.296444\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.151100\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.190963\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.308771\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.206203\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.194909\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.337570\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.366530\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.132591\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.209342\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.149060\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.236998\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.193303\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.298502\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.192914\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.324459\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.370436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.252922\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.228058\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.438608\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.265203\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.301236\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.502402\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.116455\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.383611\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.447286\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.388001\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.217791\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.219702\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.363162\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.170444\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.178112\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.054461\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.324728\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.152842\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.208122\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.244024\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.203511\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.172378\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.225969\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.206334\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.162751\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.279654\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.280344\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.110512\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.143385\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.138992\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.162483\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.234706\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.174954\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.269542\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.461728\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.291866\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.149851\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.185239\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.309427\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.200302\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.049673\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.096788\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.122760\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.308217\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.187629\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.171494\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.174096\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.312503\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.115148\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.150425\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.274119\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.320689\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.257813\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.159351\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.194431\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.326190\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.315785\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.175030\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.179447\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.176269\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.113502\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.412789\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.209163\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.204072\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.163077\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.181752\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.255289\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.658773\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.094781\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.103421\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.064018\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.172160\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.155621\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.115302\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.234278\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.177113\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.242528\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.148539\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.178481\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.147707\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.237964\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.248231\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.285275\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.058373\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.248635\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.156362\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.150653\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.176876\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.253630\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.191562\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.072770\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.087186\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.094563\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.160333\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.317384\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.146856\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.283102\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.207252\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.198188\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.379904\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.150514\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.144482\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.131822\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.265748\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.265586\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.256016\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.209802\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.081900\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.322511\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.181655\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.156088\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.186109\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.193004\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.191255\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.067599\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.346998\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.223724\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.135708\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.148650\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.059603\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.126578\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.151828\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.206010\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.270663\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.217307\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.397555\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.111271\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.196595\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.259846\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.112504\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.081807\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.115098\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.154423\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.257327\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.147411\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.140184\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.161862\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.098188\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.231301\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.184145\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.082728\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.248830\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.109861\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.191696\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.114605\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.220523\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.311116\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.122485\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.337987\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.281606\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.076142\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.094362\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.332297\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.106435\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.068404\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.129168\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.169338\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.140327\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.101069\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.099911\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.124377\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.106475\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.217503\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.235567\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.069457\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.059467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.173381\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.185687\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.110115\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.047674\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.294275\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.303104\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.068157\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.402424\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.127292\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.112820\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.193279\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.092721\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.071464\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.089813\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.086753\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.191834\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.143986\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.194206\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.095993\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.080402\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.112786\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.166699\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.107089\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.162377\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.170238\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.102571\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.105053\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.110938\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.128156\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.129186\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.064525\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.172103\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.121196\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.369075\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.234044\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.085115\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.123555\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.113287\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.100782\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.056207\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.093520\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.091826\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.203487\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.228768\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.064963\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.272010\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.062734\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.201807\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.093136\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.288223\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.080674\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.285705\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.207590\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.049518\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.038370\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.206841\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.193747\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.372076\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.133626\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.052370\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.168142\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.019699\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.081790\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.069228\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.077906\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.163684\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.039867\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.391087\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.221599\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.067957\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.166232\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.172173\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.142964\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.104477\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.062024\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.411734\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.147086\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.073133\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.131370\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.057426\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.165730\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.215104\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.105914\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.064591\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.232981\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.084581\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.080451\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.192828\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.074541\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.106929\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.063957\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.053156\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.169944\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.256452\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.290613\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.081861\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.284016\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.164551\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.327472\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.148724\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.105583\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.084131\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.126064\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.137955\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.109031\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.125482\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.214530\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.196864\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.051060\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.250936\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.078597\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.087680\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.272334\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.123749\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.125397\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.073937\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.089131\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.051382\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.184692\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.109971\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.048351\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.067948\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.189466\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.157300\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.150380\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.070802\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.181470\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.113903\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.307301\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.035348\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.106418\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.148399\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.379984\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.197850\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.137612\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.063293\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.249470\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.123574\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.149213\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.173733\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.237127\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.121019\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.107892\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.050751\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.033118\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.122014\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.157475\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.068118\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.075528\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.170304\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.162076\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.182225\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.041689\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.198970\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.029438\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.044289\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.044022\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.101443\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.055012\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.042766\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.056624\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.046892\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.121706\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.121001\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.074590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.066363\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.092153\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.113126\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.079544\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.154340\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.079017\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.186419\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.042345\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.161607\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.069224\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.029599\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.049986\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.102450\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.077999\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.133109\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.166762\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.165334\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.126635\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.125889\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.167826\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.090678\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.114126\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.155025\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.093736\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.088570\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.308555\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.066971\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.331626\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.029959\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.051511\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.084939\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.264365\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.080053\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.121168\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.204147\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.117133\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.033549\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.089582\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.087366\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.086557\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.109989\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.055105\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.253634\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.121955\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.188158\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.039875\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.199976\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.198735\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.197036\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.217542\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.073248\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.206687\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.278366\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.299413\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.026859\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.279889\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.063377\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.070706\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.127059\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.050915\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.017960\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.066736\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.080418\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.157877\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.089722\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.032402\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.226948\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.063654\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.084644\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.044786\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.161318\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.048700\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.072611\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.041474\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.108037\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.062473\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.074895\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.028699\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.108828\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.080468\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.256562\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.057340\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.145615\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.061094\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.071275\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.046855\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.094665\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.029893\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.072292\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.196997\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.235835\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.041216\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.025178\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.149821\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.119816\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.235313\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.153023\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.024007\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.099500\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.086204\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.168527\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.155919\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.082824\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.095425\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.065306\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.254492\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.066075\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.022093\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.068982\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.058945\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.060687\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.029473\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.120409\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.046785\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.303026\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.132240\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.201656\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.083863\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.110150\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.069577\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.189142\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.044692\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.114785\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.029102\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.125866\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.059182\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.040599\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.140812\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.128010\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.155273\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.207059\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.016930\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.106173\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.108821\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.066816\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.087981\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.123396\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.029168\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.172031\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.178899\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.094468\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.101570\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.074968\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.113490\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.174242\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.069360\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.077413\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.050099\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.063604\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.094242\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.158510\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.068492\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.092214\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.101315\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.023948\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.055625\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.048802\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.101858\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.016670\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.021717\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.132724\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.161567\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.096733\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.166350\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.248911\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.039454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.090051\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.034477\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.038356\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.025269\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.145742\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.138391\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.126656\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.024742\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.123196\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.142367\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.043123\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.033794\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.126679\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-035117e4bb24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-035117e4bb24>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/python3-numpy/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/python3-numpy/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/python3-numpy/lib/python3.5/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/python3-numpy/lib/python3.5/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/python3-numpy/lib/python3.5/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;31m# PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 9-2 softmax mnist\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/venv/py3/lib/python3.5/site-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.319500\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.292898\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.289500\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.285129\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.266376\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.243713\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.234079\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.207976\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.153191\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.154187\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.068593\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.973876\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.819871\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.472609\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.427214\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.271433\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.040491\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.963183\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.578554\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.716940\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.616458\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.680554\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.671474\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.549853\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.550331\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.568847\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.775745\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.713361\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.456040\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.468118\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.366575\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.411288\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.461424\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.319005\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.366806\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.472161\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.411977\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.335832\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.268582\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.194591\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.234607\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.243424\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.231890\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.213154\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.312843\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.345121\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.353682\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.174929\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.283369\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.250054\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.149553\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.499040\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.404402\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.368118\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.239730\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.220660\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.277580\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.243382\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.306894\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.232832\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.245371\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.189789\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.180816\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.285042\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.400420\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.235899\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.385471\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.338033\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.271979\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.264078\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.197347\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.083029\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.289959\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.148299\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.329583\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.297957\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.160340\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.259238\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.254287\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.245344\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.207933\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.412107\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.381221\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.220475\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.147174\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.340176\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.242217\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.163514\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.147610\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.221505\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.264034\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.315813\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.213931\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.282161\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.183387\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.163482\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.271329\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.151835\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.281422\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.175198\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.224516\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.251557\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.151162\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.243011\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.167748\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.149320\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.144181\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.286863\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.251616\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.248016\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.150746\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.236572\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.208494\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.197652\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.107299\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.153498\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.081115\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.180961\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.091930\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.045127\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.218829\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.197604\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.138613\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.108098\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.108994\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.167014\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.159385\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.157705\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.228376\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.094163\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.109521\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.101697\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.075986\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.187923\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.230340\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.272848\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.094638\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.188555\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.381812\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.126586\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.141601\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.053264\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.160830\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.157339\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.080134\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.126318\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.149316\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.234589\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.240630\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.133914\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.144887\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.124998\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.056365\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.075997\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.101578\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.080933\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.167279\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.071709\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.095112\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.162824\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.074361\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.173251\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.027242\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.033174\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.061316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.099933\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.075369\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.169033\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.182880\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.109051\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.044440\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.167501\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.114766\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.134146\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.074208\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.182562\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.063717\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.159961\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.078906\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.096927\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.153420\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.194011\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.093960\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.068611\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.066739\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.145373\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.352213\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.061210\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.160229\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.157200\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.079381\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.276837\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.118461\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.053543\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.099186\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.153220\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.030736\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.101097\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.192526\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.067998\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.155902\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.157049\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.088015\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.078568\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.248617\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.117223\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.131687\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.062394\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.074754\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.060174\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.049994\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.285175\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.137290\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.065255\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.191325\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.087608\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.084890\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.083806\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.062789\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.128109\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.264873\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.047817\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.094705\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.096504\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.050870\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.222472\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.036131\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.110932\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.042302\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.151190\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.135430\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.044451\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.124114\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.208191\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.056039\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.068089\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.058068\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.113426\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.443175\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.091129\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.196697\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.141363\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.202581\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.147798\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.077561\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.077704\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.050609\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.201562\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.017174\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.142661\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.081450\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.069460\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.163370\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.175726\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.118095\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.034196\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.168664\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.142337\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.123768\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.016827\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.089755\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.143577\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.096365\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.028097\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.134558\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.040511\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.065449\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.025457\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.057380\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.071880\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.090790\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.083272\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.127900\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.071639\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.173730\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.114263\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.109378\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.071955\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.052253\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.117883\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.030766\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.121572\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.060481\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.019942\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.046082\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.481436\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.067045\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.046540\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.201305\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.200685\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.074989\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.077334\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.098673\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.058917\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.114092\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.044848\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.070916\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.029073\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.093272\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.111100\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.127038\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.145050\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.087151\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.056982\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.029800\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.023250\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.105908\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.132918\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.071170\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.160229\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.112352\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.057968\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.097559\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.043264\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.039161\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.224339\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.072705\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.168395\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.091917\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.039492\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.182653\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.036523\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.190664\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.129730\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.103435\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.116435\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.053442\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.078466\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.145091\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.079698\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.043784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.301186\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.023737\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.108017\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.148332\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.018325\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.041269\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.033969\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.175850\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.068201\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.015735\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.066413\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.086220\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.036130\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.039361\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.045031\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.146137\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.054742\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.335858\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.049244\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.203828\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.040820\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.130529\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.042800\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.117958\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.114959\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.080418\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.141002\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.094705\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.088132\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.087633\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.022551\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.108147\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.046268\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.094620\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.145125\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.064634\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.042073\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.101659\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.092228\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.097469\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.078514\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.095867\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.186267\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.088264\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.070833\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.118796\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.032113\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.088123\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.039798\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.060427\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.105020\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.062975\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.050640\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.064254\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.033240\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.044064\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.053817\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.100870\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.035276\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.082342\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.045015\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.076815\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.063787\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.079857\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.031207\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.035611\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.097038\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.127656\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.023711\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.064525\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.140153\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.056664\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.055372\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.096721\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.153107\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.062206\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.064806\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.033050\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.083129\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.028939\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.041684\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.049232\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.052447\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.042567\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.108604\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.111051\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.093211\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.034293\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.017853\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.076945\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.076906\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.035362\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.129166\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.156236\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.041388\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.197338\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.061373\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.071534\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.116460\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.073609\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.096247\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.077630\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.101332\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.064890\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.019656\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.135147\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.136189\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.066140\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.071442\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.139751\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.022598\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.059979\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.158284\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.055290\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.156833\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.115468\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.043290\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.080649\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.016989\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.094018\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.110375\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.031110\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.020757\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.049054\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.072312\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.170764\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.035996\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.067730\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.108827\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.033493\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.050476\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.028826\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.030492\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.014689\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.085793\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.118025\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.082710\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.064172\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.141202\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.028511\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.067121\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.060131\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.116644\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.046088\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.043558\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.019468\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.081163\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.084232\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.011824\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.035339\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.092668\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.060145\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.015248\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.124036\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.052811\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.068079\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.141820\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.043681\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.042535\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.162155\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.031053\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.071442\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.090939\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.140628\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.137647\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.059493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.030309\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.080539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.092254\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.092281\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.046978\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.055710\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.034637\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.024366\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.012413\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.063499\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.066098\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.026826\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.058143\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.021718\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.170595\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.065993\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.021270\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.055679\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.037750\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.034487\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.024442\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.008375\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.005892\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.203405\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.060876\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.009878\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.013046\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.038864\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.133661\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.070578\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.072621\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.098526\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.024049\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.060728\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.059617\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.033489\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.175788\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.122280\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.040964\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.009694\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.086564\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.107304\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.105027\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.097990\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.069312\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.016145\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.115559\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.142973\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.117500\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.120356\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.181979\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.042607\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.015542\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.028856\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.112778\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.035275\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.024436\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.118470\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.032398\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.073512\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.047127\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.128615\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.104090\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.034722\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.063515\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.124990\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.016381\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.076570\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.022028\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.054596\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.118649\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.123594\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.293739\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.083474\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.034505\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.218766\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.045046\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.037910\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.072627\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.044304\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.024884\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.037258\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.220392\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.199344\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.059446\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.168427\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.170328\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.022032\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.139913\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.046230\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.040734\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.012360\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.028054\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.069169\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.040073\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.027303\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.079048\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.200502\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.115812\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.031177\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.085078\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.041391\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.144360\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.120164\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.038906\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.060906\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.032339\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.041564\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.094066\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.143849\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.063088\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.249415\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.006349\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.118475\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.149144\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.008153\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.082293\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.020462\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.024679\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.152227\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.081140\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.019691\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.026271\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.087780\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.049732\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.033752\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.021520\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.034577\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.067866\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.062516\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.025548\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.018176\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.074669\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.055687\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.023387\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.005197\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.121379\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.042901\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.056875\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.031945\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.090942\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.056233\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.098662\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.040258\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.286747\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.027901\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.034036\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.009046\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.147152\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.020642\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.041730\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.023387\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.028066\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.023861\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.048502\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.026142\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.071551\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.149221\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.150711\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.044112\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.119000\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.021861\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.061582\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.082551\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.130991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.039185\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.030273\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.082167\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.092133\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.091059\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.022530\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.014088\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.053925\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.031718\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.028522\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.011386\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.021276\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.098737\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.028933\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.143953\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.019793\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.110389\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.004315\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.085256\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.030260\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.142096\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.142454\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.011547\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.111340\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.049012\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.084939\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.041761\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.019575\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.026935\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.013912\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.047479\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.063594\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.008460\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.022787\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.096486\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.068760\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.214638\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.065860\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.047827\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.122671\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.031101\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.028266\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.026155\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.091773\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.063156\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.019378\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.029735\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.063686\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.027203\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.016271\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.011368\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.036039\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.024338\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.048983\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.033344\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.068562\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.025826\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.026822\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.021874\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.025617\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.044654\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.027678\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.032088\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.148115\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.019336\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.020008\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.235599\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.030561\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.049527\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.070319\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.036905\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.005065\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.074027\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.041793\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.016051\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.066669\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.049932\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.049464\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.063075\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.002741\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.114674\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.044013\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.075896\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.079862\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.050623\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.051143\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.114933\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.088026\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.037043\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.047335\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.084180\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.049132\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.014203\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.224388\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.100551\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.052730\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.038912\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.084479\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.030861\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.125755\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.013471\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.026178\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.132855\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.194531\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.077173\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.090650\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.021115\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.012634\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.060285\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.099381\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.028307\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.074885\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.034596\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.056150\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.017533\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.049518\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.166162\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.118106\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.026697\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.079560\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.051149\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.004416\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.026102\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.055140\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.017434\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.069179\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.083857\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.052098\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.045157\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.075654\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.091774\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.057015\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.129297\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.075406\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.087319\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.031040\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.123638\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.022042\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.037169\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.086366\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.017090\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.019275\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.077720\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.006906\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.110044\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.045003\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.049341\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.041987\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.090718\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.109346\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.127390\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.081450\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.069100\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.057261\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.055514\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.152230\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.028581\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.155312\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.057369\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.045232\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.031805\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.090113\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.053424\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.025752\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.040208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.313964\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.032920\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.037503\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.143344\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.045236\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.022686\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.030187\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.040202\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.224778\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.052128\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.063841\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.058738\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.014848\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.037866\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.049599\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.037353\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.096749\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.082595\n",
      "\n",
      "Test set: Average loss: 0.0587, Accuracy: 9818/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10-1 cnn_mnist\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/venv/python3-numpy/lib/python3.5/site-packages/ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307018\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.304096\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.306845\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.294322\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.293642\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.299750\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.297143\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.293474\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.308960\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.293329\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.295993\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.275862\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.278942\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.278620\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.267714\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.274187\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.253995\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.240003\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.196830\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.188442\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.146185\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.065796\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.941316\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.571188\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.306324\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.335007\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.111429\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.823280\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.912197\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.605546\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.662602\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.565496\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.571774\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.436419\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.569887\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.430588\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.401395\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.626777\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.377842\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.440244\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.674592\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.308511\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.395276\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.532758\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.354390\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.396223\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.205814\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.377143\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.281245\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.477971\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.383921\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.442148\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.554058\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.466465\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.321529\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.513646\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.359831\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.408984\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.322879\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.266806\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.536137\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.321444\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.398997\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.201267\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.290150\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.234232\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.281553\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.338404\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.186745\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.255398\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.167816\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.312088\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.286722\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.415766\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.232125\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.228537\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.303269\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.195401\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.224490\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.124902\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.333573\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.410663\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.124085\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.170901\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.139400\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.228903\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.282111\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.372647\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.187647\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.164901\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.224093\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.205108\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.189749\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.170377\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.436229\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.165050\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.365463\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.417825\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.101336\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.288599\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.201511\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.050326\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.226089\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.124752\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.197366\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.187207\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.206650\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.156396\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.084259\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.197158\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.157336\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.173377\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.155872\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.107331\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.465928\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.156869\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.138185\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.154598\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.271594\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.132973\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.135416\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.136861\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.267478\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.339124\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.124964\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.152322\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.202904\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.175544\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.133499\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.134709\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.063868\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.045335\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.065662\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.136603\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.154175\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.100793\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.224010\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.287536\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.204100\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.183615\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.253729\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.118104\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.177474\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.043110\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.173818\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.129143\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.109224\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.238536\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.131757\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.214313\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.114255\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.077972\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.351973\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.125723\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.033825\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.105623\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.195694\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.219296\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.027119\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.057483\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.064749\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.161379\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.097235\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.057925\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.098929\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.079046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.174722\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.087917\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.092652\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.074834\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.199519\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.063857\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.045979\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.083560\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.153686\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.044878\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.048620\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.095115\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.124644\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.055137\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.475606\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.169598\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.048228\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.098648\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.103859\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.108960\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.203730\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.069082\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.123354\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.021946\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.186722\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.062597\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.074730\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.037634\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.052524\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.167269\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.065841\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.050896\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.043481\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.056092\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.148154\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.095762\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.067871\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.145485\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.132577\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.134220\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.100364\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.224570\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.050384\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.037164\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.148159\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.084376\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.164662\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.156661\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.085031\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.047526\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.082835\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.075905\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.070375\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.073869\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.075829\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.250716\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.158600\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.024090\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.154084\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.077896\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.088134\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.171082\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.352600\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.034850\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.057061\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.043584\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.070852\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.113876\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.206972\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.075087\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.111583\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.106547\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.081416\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.211699\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.041514\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.036431\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.089421\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.042489\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.108357\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.168975\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.114823\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.078630\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.015368\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.056212\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.032429\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.069393\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.079506\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.060105\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.094373\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.103680\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.088179\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.121906\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.046815\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.010830\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.108817\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.124910\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.089108\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.069168\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.208666\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.065862\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.080345\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.122759\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.035972\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.052841\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.187055\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.073142\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.054696\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.078452\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.065523\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.151163\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.110425\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.050394\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.152344\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.073082\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.059096\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.105032\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.050034\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.043204\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.100932\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.112506\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.044811\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.081642\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.105670\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.099127\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.035225\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.048465\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.223933\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.151158\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.038264\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.203123\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.060791\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.081074\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.033835\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.014294\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.033880\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.026244\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.063277\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.124755\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.071167\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.124689\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.091976\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.133907\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.173143\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.093681\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.045544\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.063365\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.066248\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.092331\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.048395\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.082567\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.059622\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.028818\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.150244\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.089569\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.091847\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.100596\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.061663\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.050154\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.060596\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.050739\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.084905\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.087381\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.033906\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.076527\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.052692\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.066209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.074527\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.235752\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.178542\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.037945\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.097308\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.014558\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.011038\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.068189\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.145726\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.064552\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.122019\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.025413\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.057187\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.028228\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.038791\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.187048\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.091173\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.018344\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.043165\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.070477\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.022279\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.045159\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.028719\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.185852\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.017740\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.031441\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.183248\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.042278\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.129786\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.068957\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.171013\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.417712\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.033020\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.050038\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.016376\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.064082\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.041298\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.067112\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.180971\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.149411\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.040424\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.010883\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.029675\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.027556\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.078551\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.066679\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.012054\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.132080\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.076872\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.119413\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.126049\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.033494\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.072589\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.156089\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.026983\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.069201\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.113801\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.035420\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.032193\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.027856\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.274709\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.119219\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.208268\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.118177\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.024481\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.060469\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.281828\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.083216\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.088743\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.014164\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.004839\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.035454\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.035492\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.128162\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.117717\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.016615\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.113121\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.041201\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.071415\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.144538\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.011644\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.097587\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.051141\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.108261\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.020473\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.012873\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.094557\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.114393\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.102895\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.167571\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.024103\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.058740\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.160563\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.053441\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.168508\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.195385\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.016253\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.027585\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.036017\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.019957\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.121908\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.052124\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.163654\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.090784\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.023084\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.036353\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.012553\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.024928\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.022687\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.068421\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.075017\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.036106\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.043561\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.088487\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.198054\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.114606\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.026937\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.064030\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.029964\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.120037\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.032715\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.042158\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.094627\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.106445\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.098841\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.094392\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.144960\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.096624\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.038457\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.186828\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.045229\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.150759\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.058438\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.183538\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.067948\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.028186\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.008304\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.091385\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.071068\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.160218\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.102231\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.070265\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.093164\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.022233\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.033476\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.007181\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.072025\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.070944\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.048082\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.167989\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.041802\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.049497\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.131957\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.032426\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.054392\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.057815\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.077807\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.055830\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.077749\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.047506\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.018143\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.036783\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.126328\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.095880\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.106518\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.147403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.011020\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.126791\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.179019\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.035597\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.012565\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.050702\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.025370\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.045355\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.072800\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.150834\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.082033\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.097189\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.070942\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.095503\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.011033\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.214240\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.009554\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.007438\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.015305\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.031626\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.034404\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.064219\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.171933\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.011396\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.018699\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.061440\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.077987\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.030562\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.014395\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.017689\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.043312\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.026831\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.007824\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.013556\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.063055\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.014346\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.018548\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.087410\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.150771\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.055989\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.016892\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.103067\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.027766\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.029551\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.004764\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.032157\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.025448\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.157499\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.054903\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.047298\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.146012\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.204556\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.026899\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.108671\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.044203\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.122597\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.028593\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.029371\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.062485\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.023863\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.032620\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.202209\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.051322\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.022777\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.010039\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.032835\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.009271\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.118396\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.173450\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.051331\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.067872\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.026886\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.033386\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.047640\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.088858\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.018668\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005450\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.041482\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.011638\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.025325\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.004013\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.014192\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.181465\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.069339\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.039777\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.019886\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.069201\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.018625\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.040180\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.061195\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.100805\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.156676\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.006622\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.012841\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.012138\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.038259\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.024167\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.019249\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.136089\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.047891\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.065284\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.053476\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.035313\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.023503\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.079275\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.007531\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.024582\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.074474\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.118654\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.030612\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.041148\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.008662\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.015568\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.198611\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.055655\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.069958\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.153106\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.021386\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.036232\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.052084\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.020839\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.200302\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.105471\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.049178\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.092396\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.036967\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.060135\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.034419\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.019386\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.031729\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.097248\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.011374\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.087884\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.289529\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.037190\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.003379\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.018575\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.017802\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.111051\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.053752\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.083027\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.014548\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.063337\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.054911\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.029907\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.030090\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.043987\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.109033\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.106422\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.022923\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.062054\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.003475\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.051514\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.206827\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.017063\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.104879\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.035702\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.018698\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.076886\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.024963\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.021336\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.020386\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.005934\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.041441\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.004883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.153940\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.007791\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.020003\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.039882\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.155849\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.023670\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.058468\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.036051\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.046500\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.005258\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.048907\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.013880\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.081901\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.246136\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.047625\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.080753\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.038142\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.046658\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.207374\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.013421\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.020589\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.051055\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.053429\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.034265\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.167813\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.054262\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.055259\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.008023\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.043675\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.004581\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.008428\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.053489\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.147030\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.035162\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.096032\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.017276\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.003706\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.019115\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.179926\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.027346\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.026029\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.023085\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.016097\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.139211\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.035592\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.013723\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.036906\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.007224\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.052033\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.020759\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.165976\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.153039\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.008797\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.069531\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.063420\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.042219\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.140843\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.008357\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.061282\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.006896\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.028504\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.074033\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.030739\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.003625\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.062016\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.072796\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.133896\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.157713\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.037647\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.107369\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.039842\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.082867\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.086189\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.009836\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.055684\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.035152\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.035401\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.004595\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.057988\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.005853\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.051998\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.010313\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.020190\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.091668\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.047915\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.014577\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.086669\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.037366\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.148602\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.088910\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.017011\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.073151\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.024166\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.018969\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.043799\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.113756\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.023148\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.025183\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.099962\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.038411\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.014100\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.036010\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.020650\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.042202\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.076338\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.100102\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.018957\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.022270\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.087266\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.012213\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.049361\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.061314\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.007491\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.017857\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.023706\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.090661\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.022552\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.044995\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.034506\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.009330\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.032116\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.024124\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.115220\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.023096\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.013970\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.033734\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.026460\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.015549\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.022041\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.032918\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.064512\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.071837\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.098638\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.087456\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.013344\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.145007\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.105431\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.042882\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.025772\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.105987\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.020793\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.010360\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.023190\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.036530\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.084759\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.011806\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.108862\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.097510\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.020444\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.034107\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.147853\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.041347\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.062093\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.045743\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.046239\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.007453\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.094671\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.019144\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.131294\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.041369\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.014660\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.130665\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.021786\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.011251\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.055305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.048706\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.019799\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.141657\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.018098\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.052734\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.071334\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.007973\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.049153\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.051178\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.017710\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.051585\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.003457\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.060592\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.037384\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.017641\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.028857\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.052315\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.013909\n",
      "\n",
      "Test set: Average loss: 0.0475, Accuracy: 9841/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11-1  toy inception mnist\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "##    , gpu pc .\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
