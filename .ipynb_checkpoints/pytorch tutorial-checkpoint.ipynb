{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "http://bit.ly/PyTorchZeroAll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 0.0\n",
      "\t 1.0 2.0 0.0 4.0\n",
      "\t 2.0 4.0 0.0 16.0\n",
      "\t 3.0 6.0 0.0 36.0\n",
      "MSE= 18.6666666667\n",
      "w= 0.1\n",
      "\t 1.0 2.0 0.1 3.61\n",
      "\t 2.0 4.0 0.2 14.44\n",
      "\t 3.0 6.0 0.3 32.49\n",
      "MSE= 16.8466666667\n",
      "w= 0.2\n",
      "\t 1.0 2.0 0.2 3.24\n",
      "\t 2.0 4.0 0.4 12.96\n",
      "\t 3.0 6.0 0.6 29.16\n",
      "MSE= 15.12\n",
      "w= 0.3\n",
      "\t 1.0 2.0 0.3 2.89\n",
      "\t 2.0 4.0 0.6 11.56\n",
      "\t 3.0 6.0 0.9 26.01\n",
      "MSE= 13.4866666667\n",
      "w= 0.4\n",
      "\t 1.0 2.0 0.4 2.56\n",
      "\t 2.0 4.0 0.8 10.24\n",
      "\t 3.0 6.0 1.2 23.04\n",
      "MSE= 11.9466666667\n",
      "w= 0.5\n",
      "\t 1.0 2.0 0.5 2.25\n",
      "\t 2.0 4.0 1.0 9.0\n",
      "\t 3.0 6.0 1.5 20.25\n",
      "MSE= 10.5\n",
      "w= 0.6\n",
      "\t 1.0 2.0 0.6 1.96\n",
      "\t 2.0 4.0 1.2 7.84\n",
      "\t 3.0 6.0 1.8 17.64\n",
      "MSE= 9.14666666667\n",
      "w= 0.7\n",
      "\t 1.0 2.0 0.7 1.69\n",
      "\t 2.0 4.0 1.4 6.76\n",
      "\t 3.0 6.0 2.1 15.21\n",
      "MSE= 7.88666666667\n",
      "w= 0.8\n",
      "\t 1.0 2.0 0.8 1.44\n",
      "\t 2.0 4.0 1.6 5.76\n",
      "\t 3.0 6.0 2.4 12.96\n",
      "MSE= 6.72\n",
      "w= 0.9\n",
      "\t 1.0 2.0 0.9 1.21\n",
      "\t 2.0 4.0 1.8 4.84\n",
      "\t 3.0 6.0 2.7 10.89\n",
      "MSE= 5.64666666667\n",
      "w= 1.0\n",
      "\t 1.0 2.0 1.0 1.0\n",
      "\t 2.0 4.0 2.0 4.0\n",
      "\t 3.0 6.0 3.0 9.0\n",
      "MSE= 4.66666666667\n",
      "w= 1.1\n",
      "\t 1.0 2.0 1.1 0.81\n",
      "\t 2.0 4.0 2.2 3.24\n",
      "\t 3.0 6.0 3.3 7.29\n",
      "MSE= 3.78\n",
      "w= 1.2\n",
      "\t 1.0 2.0 1.2 0.64\n",
      "\t 2.0 4.0 2.4 2.56\n",
      "\t 3.0 6.0 3.6 5.76\n",
      "MSE= 2.98666666667\n",
      "w= 1.3\n",
      "\t 1.0 2.0 1.3 0.49\n",
      "\t 2.0 4.0 2.6 1.96\n",
      "\t 3.0 6.0 3.9 4.41\n",
      "MSE= 2.28666666667\n",
      "w= 1.4\n",
      "\t 1.0 2.0 1.4 0.36\n",
      "\t 2.0 4.0 2.8 1.44\n",
      "\t 3.0 6.0 4.2 3.24\n",
      "MSE= 1.68\n",
      "w= 1.5\n",
      "\t 1.0 2.0 1.5 0.25\n",
      "\t 2.0 4.0 3.0 1.0\n",
      "\t 3.0 6.0 4.5 2.25\n",
      "MSE= 1.16666666667\n",
      "w= 1.6\n",
      "\t 1.0 2.0 1.6 0.16\n",
      "\t 2.0 4.0 3.2 0.64\n",
      "\t 3.0 6.0 4.8 1.44\n",
      "MSE= 0.746666666667\n",
      "w= 1.7\n",
      "\t 1.0 2.0 1.7 0.09\n",
      "\t 2.0 4.0 3.4 0.36\n",
      "\t 3.0 6.0 5.1 0.81\n",
      "MSE= 0.42\n",
      "w= 1.8\n",
      "\t 1.0 2.0 1.8 0.04\n",
      "\t 2.0 4.0 3.6 0.16\n",
      "\t 3.0 6.0 5.4 0.36\n",
      "MSE= 0.186666666667\n",
      "w= 1.9\n",
      "\t 1.0 2.0 1.9 0.01\n",
      "\t 2.0 4.0 3.8 0.04\n",
      "\t 3.0 6.0 5.7 0.09\n",
      "MSE= 0.0466666666667\n",
      "w= 2.0\n",
      "\t 1.0 2.0 2.0 0.0\n",
      "\t 2.0 4.0 4.0 0.0\n",
      "\t 3.0 6.0 6.0 0.0\n",
      "MSE= 0.0\n",
      "w= 2.1\n",
      "\t 1.0 2.0 2.1 0.01\n",
      "\t 2.0 4.0 4.2 0.04\n",
      "\t 3.0 6.0 6.3 0.09\n",
      "MSE= 0.0466666666667\n",
      "w= 2.2\n",
      "\t 1.0 2.0 2.2 0.04\n",
      "\t 2.0 4.0 4.4 0.16\n",
      "\t 3.0 6.0 6.6 0.36\n",
      "MSE= 0.186666666667\n",
      "w= 2.3\n",
      "\t 1.0 2.0 2.3 0.09\n",
      "\t 2.0 4.0 4.6 0.36\n",
      "\t 3.0 6.0 6.9 0.81\n",
      "MSE= 0.42\n",
      "w= 2.4\n",
      "\t 1.0 2.0 2.4 0.16\n",
      "\t 2.0 4.0 4.8 0.64\n",
      "\t 3.0 6.0 7.2 1.44\n",
      "MSE= 0.746666666667\n",
      "w= 2.5\n",
      "\t 1.0 2.0 2.5 0.25\n",
      "\t 2.0 4.0 5.0 1.0\n",
      "\t 3.0 6.0 7.5 2.25\n",
      "MSE= 1.16666666667\n",
      "w= 2.6\n",
      "\t 1.0 2.0 2.6 0.36\n",
      "\t 2.0 4.0 5.2 1.44\n",
      "\t 3.0 6.0 7.8 3.24\n",
      "MSE= 1.68\n",
      "w= 2.7\n",
      "\t 1.0 2.0 2.7 0.49\n",
      "\t 2.0 4.0 5.4 1.96\n",
      "\t 3.0 6.0 8.1 4.41\n",
      "MSE= 2.28666666667\n",
      "w= 2.8\n",
      "\t 1.0 2.0 2.8 0.64\n",
      "\t 2.0 4.0 5.6 2.56\n",
      "\t 3.0 6.0 8.4 5.76\n",
      "MSE= 2.98666666667\n",
      "w= 2.9\n",
      "\t 1.0 2.0 2.9 0.81\n",
      "\t 2.0 4.0 5.8 3.24\n",
      "\t 3.0 6.0 8.7 7.29\n",
      "MSE= 3.78\n",
      "w= 3.0\n",
      "\t 1.0 2.0 3.0 1.0\n",
      "\t 2.0 4.0 6.0 4.0\n",
      "\t 3.0 6.0 9.0 9.0\n",
      "MSE= 4.66666666667\n",
      "w= 3.1\n",
      "\t 1.0 2.0 3.1 1.21\n",
      "\t 2.0 4.0 6.2 4.84\n",
      "\t 3.0 6.0 9.3 10.89\n",
      "MSE= 5.64666666667\n",
      "w= 3.2\n",
      "\t 1.0 2.0 3.2 1.44\n",
      "\t 2.0 4.0 6.4 5.76\n",
      "\t 3.0 6.0 9.6 12.96\n",
      "MSE= 6.72\n",
      "w= 3.3\n",
      "\t 1.0 2.0 3.3 1.69\n",
      "\t 2.0 4.0 6.6 6.76\n",
      "\t 3.0 6.0 9.9 15.21\n",
      "MSE= 7.88666666667\n",
      "w= 3.4\n",
      "\t 1.0 2.0 3.4 1.96\n",
      "\t 2.0 4.0 6.8 7.84\n",
      "\t 3.0 6.0 10.2 17.64\n",
      "MSE= 9.14666666667\n",
      "w= 3.5\n",
      "\t 1.0 2.0 3.5 2.25\n",
      "\t 2.0 4.0 7.0 9.0\n",
      "\t 3.0 6.0 10.5 20.25\n",
      "MSE= 10.5\n",
      "w= 3.6\n",
      "\t 1.0 2.0 3.6 2.56\n",
      "\t 2.0 4.0 7.2 10.24\n",
      "\t 3.0 6.0 10.8 23.04\n",
      "MSE= 11.9466666667\n",
      "w= 3.7\n",
      "\t 1.0 2.0 3.7 2.89\n",
      "\t 2.0 4.0 7.4 11.56\n",
      "\t 3.0 6.0 11.1 26.01\n",
      "MSE= 13.4866666667\n",
      "w= 3.8\n",
      "\t 1.0 2.0 3.8 3.24\n",
      "\t 2.0 4.0 7.6 12.96\n",
      "\t 3.0 6.0 11.4 29.16\n",
      "MSE= 15.12\n",
      "w= 3.9\n",
      "\t 1.0 2.0 3.9 3.61\n",
      "\t 2.0 4.0 7.8 14.44\n",
      "\t 3.0 6.0 11.7 32.49\n",
      "MSE= 16.8466666667\n",
      "w= 4.0\n",
      "\t 1.0 2.0 4.0 4.0\n",
      "\t 2.0 4.0 8.0 16.0\n",
      "\t 3.0 6.0 12.0 36.0\n",
      "MSE= 18.6666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl4VOX5//H3nT2BQAgECCEhbLLv\nYRNQ1KLgAloXwBWX8nXp9rPVtvb7rdZWaxfbulQpFVTU4m5FRYUKsigCAVnCHpJAEiAJBJIACVnm\n/v2RwaYxgQEycyYz9+u65srkzDM5n+vA5M5znnOeR1QVY4wx5nRCnA5gjDGmebCCYYwxxiNWMIwx\nxnjECoYxxhiPWMEwxhjjESsYxhhjPGIFwxhjjEesYBhjjPGIFQxjjDEeCXM6QFNq166dpqamOh3D\nGGOajXXr1h1U1QRP2gZUwUhNTSU9Pd3pGMYY02yIyB5P29opKWOMMR6xgmGMMcYjVjCMMcZ4xAqG\nMcYYj1jBMMYY4xErGMYYYzxiBcMYY4xHgr5gVFTVMHv5br7cfdDpKMYYc8aWbi9k7spsKqtdXt9X\n0BeMsBDhhRXZzFmR7XQUY4w5Y88v2828VTmEh4rX92UFIzSE69M6s3RHIftLyp2OY4wxHttddJQ1\n2cVMHZ6CiBUMn5ialoJL4a30PKejGGOMx95Ym0tYiHDdsM4+2Z8VDCClbQxje7TjjbW51LjU6TjG\nGHNaJ6preHtdHt/p04GE2Eif7NMKhtu0EcnkHylnxa4ip6MYY8xpLd5aQPGxSqaNSPbZPq1guE3o\n24H4FhG8vibX6SjGGHNar6/JJSkumnE9PZqZvElYwXCLDAvl2qFJ/HtbAUVlJ5yOY4wxjdp76Dgr\nMw9yQ1oyoSHeH+w+yQpGHVOHp1DtUt5eZ4Pfxhj/9Ub6XkIEbhjum8Huk6xg1NGjfUtGpMbzxtq9\nqNrgtzHG/1TXuHgrPY/xvdqT2Drap/v2WsEQkbkiUigiGXW2vSEiG9yPHBHZ0Mh7c0Rks7udT5fQ\nmzYimZxDx1mVdciXuzXGGI8s2V5IYdkJpg333WD3Sd7sYbwETKy7QVWnqupgVR0MvAO8e4r3X+Ru\nm+bFjN9y+YBEWkWF2eC3McYvvb42l/axkVzcu73P9+21gqGqy4Hihl6T2lsSbwDme2v/ZysqPJRr\nhiTxScYBDh+rdDqOMcZ8Y39JOZ/vKOT6tM6Ehfp+RMGpMYxxQIGq7mrkdQUWicg6EZnpw1wATB+Z\nQmWNi3e/zvf1ro0xplFvrs3DpTBteIoj+3eqYEzn1L2Lsao6FJgE3CciFzTWUERmiki6iKQXFTXN\nTXe9O7ZicHIcr6+xwW9jjH+ocSlvpucyrmc7kuNjHMng84IhImHAd4E3Gmujqvnur4XAe8CIU7Sd\nrappqpqWkNB0N7BMH5HMrsKjrN97uMl+pjHGnK0Vu4rIP1LuWO8CnOlhfAfYrqoN3uwgIi1EJPbk\nc+BSIKOhtt505cBOtIgIZb4Nfhtj/MDra3Jp2yKCCX07OJbBm5fVzgdWAb1EJE9E7nS/NI16p6NE\npJOILHR/2wFYKSIbgTXAR6r6ibdyNqZFZBiTByfx4aZ9lJRX+Xr3xhjzjcKyCv69rYBrh3UmIsy5\n2+fCvPWDVXV6I9tnNLBtH3C5+3kWMMhbuc7E9BHJzF+zlwUb8rlldKrTcYwxQertdXlUu5SpDtx7\nUZfd6X0KA5Ja0zexFfPX5NrgtzHGES6X8sbaXEZ0jad7QktHs1jBOAURYfqIZLbuL2VjXonTcYwx\nQWhV1iH2HDrOdB9OY94YKxincfWQJFpEhPLKqj1ORzHGBKF5q3KIbxHBpP6JTkexgnE6sVHhXDM0\niQ827bM7v40xPrW/pJzFWwu4IS2ZqPBQp+NYwfDELaNSqax28Wa6XWJrjPGdf67eiwI3jXTu3ou6\nrGB4oFfHWEZ0jefV1Xtw2ZrfxhgfqKx2MX9NLhf3au/Ynd31WcHw0C2jupBbXM6ynbbmtzHG+z7Z\ncoCDR09w8+guTkf5hhUMD13WryMJsZG88pUNfhtjvO/VVXtIiY/hQh+u2X06VjA8FBEWwvThySzd\nUUhu8XGn4xhjAtj2A6WsySnm5lEphPhwze7TsYJxBqaPTCFEhFdXWy/DGOM9r6zaQ2RYCNcPc/7e\ni7qsYJyBxNbRTOjTgTfX5lJRVeN0HGNMACqrqOK9r/O5alAn2rSIcDrOf7GCcYZuHd2Fw8er+GjT\nfqejGGMC0Lvr8zleWcOtfjTYfZIVjDM0untbuie0sMFvY0yTU1Ve+WoPgzq3ZmDnOKfjfIsVjDMk\nItwyqgsbco+w2eaXMsY0oVVZh8gsPOq3s2NbwTgL3x3WmZiIUF75KsfpKMaYAPLqV3uIiwnnyoHO\nzxvVECsYZ6FVVDhXD0ni/Q37OHLc5pcyxpy7AyUVfLqlgKl+Mm9UQ6xgnKVbRnXhRLWLt9c1uNKs\nMcackflr9uJS5aaR/jfYfZI3l2idKyKFIpJRZ9sjIpIvIhvcj8sbee9EEdkhIpki8nNvZTwXfRJb\nMTy1Da98ZfNLGWPOTVWNi/lr9jL+vARS2vrHvFEN8WYP4yVgYgPb/6Kqg92PhfVfFJFQ4G/AJKAv\nMF1E+nox51m7eVQX9hw6zorMg05HMcY0Y4u2FFBYdoJb/PBS2rq8VjBUdTlQfBZvHQFkqmqWqlYC\nrwNTmjRcE5nUP5F2LSOY92WO01GMMc3Yy6tySI6P5sLz2jsd5ZScGMP4vohscp+yatPA60lA3YUn\n8tzb/E5EWAg3juzCkh2FZB885nQcY0wzlJFfwprsYm4Z1YVQP5o3qiG+LhjPA92BwcB+4Mlz/YEi\nMlNE0kUkvajI91OP3zwqhfCQEF76Itvn+zbGNH9zv8gmJiKUqcP9Y5GkU/FpwVDVAlWtUVUX8A9q\nTz/Vlw/UnXGrs3tbYz9ztqqmqWpaQoLvpwFuHxvFVYM68da6PErKq3y+f2NM81VYWsEHG/dxQ1oy\nraPDnY5zWj4tGCJS926Ua4CMBpqtBXqKSFcRiQCmAQt8ke9s3TE2leOVNby+Zq/TUYwxzcgrX+2h\n2qXMOD/V6Sge8eZltfOBVUAvEckTkTuBP4jIZhHZBFwE/D93204ishBAVauB7wOfAtuAN1V1i7dy\nNoV+nVozqls8L3+ZQ3WNy+k4xphmoKKqhtdW7+WS3h1IbdfC6TgeCfPWD1bV6Q1sntNI233A5XW+\nXwh865Jbf3bn2G58b146n2w5wJUDOzkdxxjj5/71dT7Fxyq5c2xXp6N4zO70biIX925Pl7YxzFlp\ng9/GmFNTVeZ+kU2fxFaM6hbvdByPWcFoIqEhwu3np/L13iOs33vY6TjGGD+2MvMgOwuOcufYroj4\n96W0dVnBaELXpyUTGxXGXOtlGGNOYc7KbNq1jOSqQf45K21jrGA0oRaRYUwbnszHGQfIP1LudBxj\njB/KLCzj8x1F3DKqC5Fh/jkrbWOsYDSx285PRVWZtyrH6SjGGD/04hc5RISFcNMo/79Rrz4rGE2s\nc5sYJvVPZP7qvRw7Ue10HGOMHzl8rJJ31udxzeAk2rWMdDrOGbOC4QV3jE2ltKKad9bbWhnGmP/4\n55q9VFS5uH1sqtNRzooVDC8YmtKGQclxvPhFjq2VYYwBate8mLcqh7E92tG7Yyun45wVKxheICLc\nObYr2QePsXRHodNxjDF+YOHm/RSUnmhWN+rVZwXDSyb170hi6yi7kc8Yg6oyZ2U23RJacOF5vp8k\ntalYwfCS8NAQbh2dype7D7F1X6nTcYwxDkrfc5hNeSXcPqYrIX6+5sWpWMHwohtHpBATEco/VmQ5\nHcUY46C/L8siLiaca4f65VpwHrOC4UWtY8KZPiKFBRv3kXf4uNNxjDEO2FVQxr+3FXDb6FRiIrw2\n36tPWMHwsjvHdkWAF1bYWIYxwWjWsiyiwkO4rZmseXEqVjC8rFNcNFcPSeL1tXspPlbpdBxjjA/t\nO1LO+xvymTY8hfgWEU7HOWdWMHzg7gu7UVHl4uUvc5yOYozxoTkrs1HgrnHN91Lauqxg+ECP9rFM\n6NuBl1flcLzSpgsxJhgcOV7J/DV7mTKoE53bxDgdp0lYwfCRuy/szpHjVby+JtfpKMYYH5i3ag/H\nK2v4nwu7Ox2lyXhzTe+5IlIoIhl1tv1RRLaLyCYReU9E4hp5b4577e8NIpLurYy+NKxLG0Z0jeeF\nFVlU2brfxgS08soaXvoyh0t6t6dXx1in4zQZb/YwXgIm1tu2GOivqgOBncAvTvH+i1R1sKqmeSmf\nz91zYXf2lVSwYMM+p6MYY7zozfRcio9Vcvf4wOldgBcLhqouB4rrbVukqidP4n8FdPbW/v3R+F4J\n9O4Yy6xlu21SQmMCVFWNi9nLs0jr0obhqc1nvW5PODmGcQfwcSOvKbBIRNaJyEwfZvIqEeHuC7uz\nq/AoS7bbpITGBKKPNu0n/0g5dwfQ2MVJjhQMEfklUA281kiTsao6FJgE3CciF5ziZ80UkXQRSS8q\nKvJC2qZ15cBEkuKieX7ZbqejGGOamKoya9luerZvycW92zsdp8n5vGCIyAzgSuAmVW3wvIyq5ru/\nFgLvASMa+3mqOltV01Q1LSHB/2eBDAsNYeYF3Vi35zBrc4pP/wZjTLPx+Y4ith8o4+4LuzfrSQYb\n49OCISITgQeByara4ORKItJCRGJPPgcuBTIaattc3ZCWTHyLCGZ9br0MYwLJ88t206l1FJMHd3I6\nild487La+cAqoJeI5InIncCzQCyw2H3J7Cx3204istD91g7AShHZCKwBPlLVT7yV0wnREaHMOD+V\nz7YXsuNAmdNxjDFNYN2ew6zJLuaucd0IDw3MW9y8NnWiqk5vYPOcRtruAy53P88CBnkrl7+4dXQX\nZi3bzd+X7ebPUwc7HccYc45mLdtNXEw400YkOx3FawKzDDYDcTERTB+Rwvsb97H3kE19bkxztv1A\nKYu3FnBrAExhfipWMBw084JuhIYIf1ua6XQUY8w5eOazTFpGhnHHmFSno3iVFQwHdWgVxY0jUnhn\nfR65xdbLMKY52llQxsKM/cw4P5W4mOY/hfmpWMFw2N0XdidEhOc+t16GMc3R05/tIiY8lDvHBsYU\n5qdiBcNhHVtHMW1EMm+lWy/DmOZmV0EZH23ez23np9ImABZIOh0rGH7gnvEnexl2X4YxzcnTSzKJ\nCQ/lrnHdnI7iE1Yw/EBi62imDk/m7XW55B8pdzqOMcYDmYVlfLhpH7eenxoQy696wgqGn7jHPQ3y\nc3bFlDHNwjNLMokOD+V7QdK7ACsYfqNTXDQ3pCXzZnou+6yXYYxf2110lA827uOW0V2CpncBVjD8\nyr0X9QDgeRvLMMavPbskk8iwUGYGUe8CrGD4laS4aK4blswba3PZX2K9DGP8UVbRUd7fkM8to7vQ\ntmWk03F8ygqGn7l3fHdcqjaTrTF+6tmlmUSEhQTV2MVJVjD8THJ8DNendWb+2lwOlFQ4HccYU0fO\nwWO8v2Eft4zqQkJscPUuwAqGX7p3fA9crtqVu4wx/uOZJZmEhwozLwi85Vc9YQXDDyXHx3Dt0M78\nc81eCkqtl2GMP9hz6Bj/2pDPTSODs3cBVjD81n0X9aDGpXbFlDF+4pklmYSFCP9zYfCNXZxkBcNP\npbSN4Ya0zvxz9V6bY8oYh+0qKOPd9XncOroL7WOjnI7jGK8WDBGZKyKFIpJRZ1u8iCwWkV3ur20a\nee9t7ja7ROQ2b+b0Vz+8pCci8Nd/73I6ijFB7U+LdtAiIox7x/dwOoqjvN3DeAmYWG/bz4HPVLUn\n8Jn7+/8iIvHAw8BIYATwcGOFJZAlto5mxvmpvPt1nq39bYxDvt57mE+3FDDzgm5BMSPtqXhUMESk\nu4hEup+PF5Efikjc6d6nqsuB4nqbpwAvu5+/DFzdwFsvAxararGqHgYW8+3CExTuGd+dlpFh/GnR\nDqejGBN0VJXff7Kddi0juCMI1rs4HU97GO8ANSLSA5gNJAP/PMt9dlDV/e7nB4AODbRJAnLrfJ/n\n3hZ04mIiuPvC7izeWsC6PYedjmNMUFmx6yBfZRXzg4t70iIycNfq9pSnBcOlqtXANcAzqvoAkHiu\nO1dVBfRcfoaIzBSRdBFJLyoqOtdIfun2Mam0axnJ7z/ZTu0hM8Z4m8ul/OHT7XRuE830ESlOx/EL\nnhaMKhGZDtwGfOjeFn6W+ywQkUQA99fCBtrkU9uLOamze9u3qOpsVU1T1bSEhISzjOTfYiLC+NEl\nPViTXcyynYFZFI3xNwsz9pORX8pPLj2PiDC7oBQ8Lxi3A6OBx1Q1W0S6Aq+c5T4XUFt4cH99v4E2\nnwKXikgb92D3pe5tQWvq8BSS46P5wyc7cLmsl2GMN1XVuHhy0U56dYhl8qCgPBveII8KhqpuVdUf\nqup89y/wWFX9/eneJyLzgVVALxHJE5E7gSeACSKyC/iO+3tEJE1EXnDvrxj4DbDW/XjUvS1oRYSF\n8JMJvdi6v5QPN+8//RuMMWftrfQ8sg8e44HLehEaIk7H8RviyTlxEfkcmAyEAeuoPY30hare79V0\nZygtLU3T09OdjuE1Lpdy+dMrKK+q4d/3X0h4qHWTjWlq5ZU1jP/TUjq3ieHtu0cjEtgFQ0TWqWqa\nJ209/Y3TWlVLge8C81R1JLW9A+NDISHCgxN7sefQcd5Ym3v6NxhjztjLq3IoKD3Bzyb2DvhicaY8\nLRhh7gHqG/jPoLdxwEW92jM8tQ1PfbaL8soap+MYE1BKjlfx3NJMLuqVwIiu8U7H8TueFoxHqR10\n3q2qa0WkG2DzVThARHhwYm+Kyk7w4pfZTscxJqD8ffluSiuqeeCy3k5H8UueDnq/paoDVfUe9/dZ\nqnqtd6OZxgxPjeeS3u15/vPdHDle6XQcYwJCYWkFc7/IZsrgTvTt1MrpOH7J06lBOovIe+6JBAtF\n5B0R6eztcKZxD0zsxbET1Tz1mXX0jGkKf/x0BzUu5f4J5zkdxW95ekrqRWrvn+jkfnzg3mYc0rtj\nK6YOT+GVVXvILDzqdBxjmrXNeSW8vT6P28d0pUvbFk7H8VueFowEVX1RVavdj5eAwLytuhn5yaXn\nERUeyuMLtzkdxZhmS1X5zYdbiY+J4PsXB/f05afjacE4JCI3i0io+3EzcMibwczptWsZyQ8u7sGS\n7YU2ZYgxZ+njjAOsySnm/kvPo1XU2c54FBw8LRh3UHtJ7QFgP3AdMMNLmcwZmDEmlZT4GH774Vaq\na1xOxzGmWamoquHxhdvo3TGWqWnJp39DkPP0Kqk9qjpZVRNUtb2qXg3YVVJ+IDIslIcu78OuwqPM\nX7PX6TjGNCtzv8gm73A5/3dlX8Js5oTTOpcj5FfTggSzy/p1YFS3eP68eCclx6ucjmNMs1BYVsHf\nlmTynT4dGNOjndNxmoVzKRh2z7yfEBH+78q+HCmv4ukldpmtMZ548tOdVNa4+OUVfZyO0mycS8Gw\nObb9SL9OrZmalszLX+aQVWSX2RpzKhn5Jby5LpfbRqfStZ1dRuupUxYMESkTkdIGHmXU3o9h/MhP\nLu1ll9kacxonL6ONiw7nB5f0dDpOs3LKgqGqsaraqoFHrKraArd+JiE2kvsu6sG/txWyYpddZmtM\nQz7dcoDV2cXcf2kvWkfbZbRnwi4LCDC3j0klOT6a3364zS6zNaaeE9U1PLZwG+d1aMn04XYZ7Zmy\nghFgosJDeWhSH3YUlPG6rZlhzH958YsccovtMtqzZUcsAE3s35GRXeP506IdFB+z2WyNAdhfUs4z\nn+3ikt7tGdfTZjY6Gz4vGCLSS0Q21HmUisiP67UZLyIlddr8ytc5mzMR4dEp/TlaUc3vbADcGAAe\n/WAr1S7l4av6OR2l2fL5wLWq7gAGA4hIKJAPvNdA0xWqeqUvswWSXh1juXNcV/6+LIvr05Jt9TAT\n1JZuL+TjjAP89NLzSGkb43ScZsvpU1KXULuK3x6HcwSkH13Sk6S4aP73X5uprLYBcBOcyitr+NWC\nDLontOB7F3RzOk6z5nTBmAbMb+S10SKyUUQ+FpFG+5AiMlNE0kUkvajILiWtKyYijF9P7sfOgqPM\nWWnLuZrg9OzSXeQWl/PbqwcQGRbqdJxmzbGCISIRwGTgrQZeXg90UdVBwDPAvxr7Oao6W1XTVDUt\nIcEGsur7Tt8OTOjbgac+20lu8XGn4xjjU5mFZcxensV3hyQxuntbp+M0e072MCYB61W1oP4Lqlqq\nqkfdzxcC4SJis4OdpUcm90MQHlmwBVWb0cUEB1Xll+9lEB0eykM2X1STcLJgTKeR01Ei0lFExP18\nBLU5bcGms5QUF83/m9CTz7YXsmjrt+qzMQHp3fX5rM4u5ueT+tCuZaTTcQKCIwVDRFoAE4B362y7\nW0Tudn97HZAhIhuBp4Fpan8an5Pbx3SlV4dYfr1gC8dOVDsdxxivOnK8kscXbmNIShzT7I7uJuNI\nwVDVY6raVlVL6mybpaqz3M+fVdV+qjpIVUep6pdO5Awk4aEhPHZNf/aVVPDUZzYFuglsv/9kB0fK\nq3js6gGEhNhKDE3F6aukjA+lpcYzbXgyc1Zms/1AqdNxjPGKdXsOM3/NXm4/P5W+nVo5HSegWMEI\nMj+b2JvW0eH88r0MXC47y2cCS3WNi1++t5nE1lH8eMJ5TscJOFYwgkybFhH8YlLv2r/C1toa4Caw\n1Paey3j4qr60jLQVGJqaFYwgdN2wzpzfvS2Pf7SNvMN2b4YJDJmFR3ly8U4m9O3AZf06Oh0nIFnB\nCEIiwu+vHQjAz97ZZPdmmGavxqX89K2NxESE8tg1/XFflW+amBWMIJUcH8NDV/Thi8xDvLbaTk2Z\n5u0fK7LYkHuEX0/uR/vYKKfjBCwrGEHsxhEpjO3RjscXbrNpQ0yzlVlYxp8X72Riv45MHtTJ6TgB\nzQpGEBMRfn/dQEJEePDtTXbVlGl2qmtc/OStTbSICOU3V9upKG+zghHkkuKi+eUVfViVdYjXVtss\n86Z5mb0ii425R3h0Sn8SYm36D2+zgmGYNjyZcT3b8fjC7ew9ZKemTPOws6CMvy7exeUDOnLlwESn\n4wQFKxjmm6umwkKEB97eaKemjN+rrnHx07c20jIqjEen2KkoX7GCYQDoFBfN/17Zh9XZxbzylZ2a\nMv7t78uz2JRXwm+m9LeZaH3ICob5xg1pyVx4XgJPfLydPYeOOR3HmAbtOFDGX/+9kysGJnKFnYry\nKSsY5hsiwhPXDiAsVHjgrU3U2Kkp42eq3KeiWkWF8+jkRlduNl5iBcP8l8TW0Tx8VT/W5BTz/OeZ\nTscx5r88uWgnm/NLeOya/rS1U1E+ZwXDfMu1Q5OYPKgTf/n3LtbmFDsdxxgAlu8sYtay3dw4MoWJ\n/e1UlBOsYJhvEREeu6Y/SXHR/Gj+1xw5Xul0JBPkCssquP/NDfTqEMuvruzrdJyg5VjBEJEcEdks\nIhtEJL2B10VEnhaRTBHZJCJDncgZrGKjwnn2xiEUHT1hExQaR7lcyv1vbOToiWqeuXEIUeGhTkcK\nWk73MC5S1cGqmtbAa5OAnu7HTOB5nyYzDOwcx88m9ubTLQW8apfaGofMWr6blZkHeeSqfpzXIdbp\nOEHN6YJxKlOAeVrrKyBOROzEpY/dMaYr43sl8JuPtrF1ny3ranxr3Z7DPLmo9hLaqcOTnY4T9Jws\nGAosEpF1IjKzgdeTgNw63+e5txkfCgkR/nT9IOKiw/nB/PUcr6x2OpIJEiXlVfxw/tckto7id98d\nYHdz+wEnC8ZYVR1K7amn+0TkgrP5ISIyU0TSRSS9qKioaRMaANq1jOSvUweTdfAYjyzY4nQcEwRU\nlZ+/s4mC0gqemT6EVlHhTkcyOFgwVDXf/bUQeA8YUa9JPlC3D9rZva3+z5mtqmmqmpaQkOCtuEHv\n/B7tuG98D95Mz+P9Dd/6ZzCmSf1zzV4+zjjATy/rxZCUNk7HMW6OFAwRaSEisSefA5cCGfWaLQBu\ndV8tNQooUdX9Po5q6vjxd3qS1qUNv3wvw6YOMV6z40AZj36wlXE92zFzXDen45g6nOphdABWishG\nYA3wkap+IiJ3i8jd7jYLgSwgE/gHcK8zUc1JYaEhPDV9CCECd79q4xmm6ZWUV3HPq+uIjQrnzzcM\nJiTExi38iQTS9fVpaWmanv6tWzpME1u6o5A7XlrLFQMSeWb6EBuMNE2ixqXc9fJaVuw6yGt3jWRk\nt7ZORwoKIrKukVsbvsWfL6s1fuqiXu158LLefLhpP88v2+10HBMgnly0g6U7inh4cj8rFn7KCoY5\nK3df2I2rBnXij5/uYMn2AqfjmGbug437eO7z3UwfkcLNI1OcjmMaYQXDnBUR4Q/XDqRvYit+NH8D\nu4uOOh3JNFNb9pXwwNsbSevShl9P7menOP2YFQxz1qIjQpl9axoRYSF8b146pRVVTkcyzcyhoyeY\nOW8dbWIieP7mYUSE2a8kf2b/OuacJMVF89xNQ9l76Dg/fn2DLbpkPFZV4+Le19Zz8OgJ/n7LMBJi\nbX0Lf2cFw5yzkd3a8vDkfizZXsiTi3Y4Hcc0E7/5cCurs4t54toBDOwc53Qc44EwpwOYwHDzyBS2\n7ivhuc930yexFVcN6uR0JOPHXl+zl3mr9vC9cV25Zkhnp+MYD1kPwzQJEeHXk/uT1qUND7y9kY25\nR5yOZPzU6qxD/N/7GYzr2Y6fTeztdBxzBqxgmCYTERbC8zcPo13LSG5/aS1ZduWUqWfb/lLumpdO\nSnwMz0wfQlio/QpqTuxfyzSphNhI5t1RO4/krXPXUFha4XAi4y9yi49z29w1tIgIY96dI4mLiXA6\nkjlDVjBMk+uW0JIXZwyn+Fglt7241i63NbX/F+auoaKqhpfvGEFSXLTTkcxZsIJhvGJQchyzbh7G\nroIyZs5Lp6KqxulIxiHHTlRz+0tryT9SzpwZw+nV0ZZZba6sYBivueC8BJ68YRBfZRXz/96wezSC\nUVWNi3teW8/mvCM8e+NQhqfGOx3JnAMrGMarpgxO4v+u7MvHGQd4eEEGgTQ7sjk1l0t58O1NLN9Z\nxO++O4AJfTs4HcmcI7sPw3hkdmKWAAAPSUlEQVTdnWO7UlR2glnLdtM+NoofXtLT6UjGB574ZDvv\nfZ3PA5f1Yupwm1AwEFjBMD7xs4m9KCo7wZ8X76RtywhuGtnF6UjGi2Yv383s5VnMOD+Ve8d3dzqO\naSJWMIxPiAhPXDuAI8cr+eV7GQjCjTaNdUD6x/IsHl+4nSsHJvKrK/va7LMBxMYwjM+Eh4bwt5uG\ncnHv9jz03mZe/jLH6Uimif1taSaPLdzGFQMT+ctUW2I10Pi8YIhIsogsFZGtIrJFRH7UQJvxIlIi\nIhvcj1/5OqfxjqjwUGbdPIxL+3bg4QVb+MfyLKcjmSagqvxl8U7++OkOrhmSxFNTBxNud3EHHCdO\nSVUDP1HV9SISC6wTkcWqurVeuxWqeqUD+YyXRYTV9jR+/MYGHlu4jcoaF/dd1MPpWOYsqSp/+HQH\nz3++m+uHdeaJawcSaj2LgOTzgqGq+4H97udlIrINSALqFwwTwMJDQ3hq6mAiQkP446c7qKx28ePv\n9LTz3c2MqvLbj7YxZ2U2N41M4TdT+ttpqADm6KC3iKQCQ4DVDbw8WkQ2AvuAn6rqlkZ+xkxgJkBK\nig2iNidhoSH86fpBhIUIT322i8oaFw9e1suKRjPhcimPfLCFeav2MOP8VB6+yga4A51jBUNEWgLv\nAD9W1dJ6L68HuqjqURG5HPgX0ODF+6o6G5gNkJaWZneFNTOhIcLvrx1IeFgIz3++m8pqF/97RR/7\nxePnXC7lofc28/raXGZe0I1fTOpt/2ZBwJGCISLh1BaL11T13fqv1y0gqrpQRJ4TkXaqetCXOY1v\nhIQIj13dn4jQEOaszKa0vIrHrhlg6zv7qYqqGn7y1kY+2rSf71/Ug59cep4ViyDh84Ihtf+z5gDb\nVPXPjbTpCBSoqorICGqv5jrkw5jGx0SEh6/qS6vocJ7+bBd7i48z6+ZhtGlhU2D7k8KyCr43bx2b\n8o7wi0m9+Z8L7aa8YOJED2MMcAuwWUQ2uLc9BKQAqOos4DrgHhGpBsqBaWqTEAU8EeH+CefRrV0L\nHnx7E9c89wVzZgyne0JLp6MZahc/uvOltRw+XsWsm4dxWb+OTkcyPiaB9Hs4LS1N09PTnY5hmsC6\nPcXMnLeOqhoXz988jDE92jkdKah9tq2AH87/mpZRYcy5bTj9k1o7Hck0ERFZp6ppnrS1k8TGLw3r\nEs+/7htDh1ZR3DZ3DfPX7HU6UlBSVV5YkcVd89LpmtCC9+8ba8UiiFnBMH4rOT6Gd+49nzE92vGL\ndzfz2w+32poaPlRV4+Kh9zL47UfbuKxvR978n9F0bB3ldCzjICsYxq+1igpnzm1pzDg/lRdWZjNz\nXjpHjlc6HSvgHTx6ghkv1vbs7h3fneduGkpMhM1VGuysYBi/FxYawiOT+/GbKf1YtrOISU+tYNVu\nu2jOW5buKGTiX5ezNucwf7xuIA9O7G13bxvACoZpRm4Zncp7944hOjyUG1/4it9/sp3KapfTsQJG\nRVUNjyzYwu0vrqVti0g++P5Yrk9LdjqW8SNWMEyzMqBzaz784VimpiXz/Oe7uW7Wl2QfPOZ0rGZv\nx4Eyrv7bF7z0ZQ4zzk/l/e+PoVfHWKdjGT9jBcM0OzERYTxx7UCev2koew4d54qnV/Dm2lxbL/ws\nqCovf5nDVc+u5ODRE7x4+3AemdyPqPBQp6MZP2SjWKbZmjQgkcEpcdz/xkYefGcTn+8s5PFrBhAX\nY3eHe+Lg0RM8+PYmlmwv5KJeCfzhukEkxEY6Hcv4MSsYpllLbB3Nq3eNZPbyLJ5ctIPVWcU8cFkv\nrk9LtjUZGlFd4+K11Xt5ctEOKqpdPHJVX247P9XmgzKnZXd6m4CxZV8JD7+/hfQ9hxmQ1JpHJvdl\nWJd4p2P5lS8zD/LrD7ayo6CMMT3a8shV/ejZwcYqgtmZ3OltBcMEFFVlwcZ9/G7hdg6UVnDNkCR+\nPqk3HVoF9w1neYeP8/jCbSzcfIDObaL53yv6cFm/jtarMGdUMOyUlAkoIsKUwUl8p08Hnvs8k38s\nz+bTLQf4wcU9uWNsKpFhwTWYW15Zw6xlu5m1bDcicP+E85h5QTcb1DZnxXoYJqDtOXSM3360jcVb\nC0htG8O9F/VgyuBOAV84KqpqeGd9Hs8t3U3+kXKuGtSJX0zqTae4aKejGT9jp6SMqWf5ziJ+9/F2\ntu0vpX1sJDPGpHLTyC60jg53OlqTKj5WySur9jBvVQ6HjlUyqHNrHrq8DyO7tXU6mvFTVjCMaYCq\nsjLzILOXZ7Fi10FaRIQydXgKd4xNpXObGKfjnZOcg8eYszKbt9blUlHl4pLe7fneBd0Y2TXexinM\nKVnBMOY0tu4r5YUVWSzYuA8FrhiQyIwxqQxJjms2v2BdLiV9z2Hmrszm060HCA8J4ZohSdw1rqtd\n+WQ8ZgXDGA/tO1LOS1/m8M/Vezl6opqkuGgu69eRSQM6Miyljd9Nuldd42JtzmE+ydjPJ1sOUFB6\ngtbR4dw8KoXbRqfSPsivBjNnzu8LhohMBJ4CQoEXVPWJeq9HAvOAYdSu5T1VVXNO93OtYJizVVpR\nxaItBXy8eT8rdh2kssZFQmwkl/XrwKT+iYzsGk9YqDMz6VRWu1iVdYhPMvazaEsBh45VEhkWwvhe\nCUzqn8iEvh1oEWkXPJqz49cFQ0RCgZ3ABCAPWAtMV9WtddrcCwxU1btFZBpwjapOPd3PtoJhmkJZ\nRRVLthfyScYBPt9RRHlVDW1iwhmeGk//pNb0T2pF/06tvfLXvKpyoLSCjPxSMvJL2LKvlDXZhyit\nqKZFRCgX9+nApP4dGd8rwdanME3C3+/DGAFkqmoWgIi8DkwBttZpMwV4xP38beBZERENpPNnxm/F\nRoUzZXASUwYnUV5Zw7KdhSzaUsCG3CMs2lrwTbuE2Ej6d2pF/6TWnNchlvgWEbSODqd1dDitosKJ\njQr71imtGpdytKKakvIqSsqrKK2o4tCxSrbvLyVjXylb8ks4dKx2gSgR6J7Qkkv7deSyfh0Z17Od\n3T9hHOVEwUgCcut8nweMbKyNqlaLSAnQFjjok4TGuEVHhDKxfyIT+ycCtb2PbfvLyMgvIWNfCVvy\nS1m2s4iGVo4VgdjIMFpFh6Nae9rr6IlqGvqzJyxE6Nkhlot7t/+mF9O7Yys71WT8SrP/3ygiM4GZ\nACkpKQ6nMYEuNiqcEV3jGdH1P3NUVVTVkH3w2H96DXW+lrp7EwK0ig6n1Tc9kLDar9HhxMWEk9q2\nhfUejN9zomDkA3WX8ers3tZQmzwRCQNaUzv4/S2qOhuYDbVjGE2e1pjTiAoPpU9iK6djGON1Tlz2\nsRboKSJdRSQCmAYsqNdmAXCb+/l1wBIbvzDGGGf5vIfhHpP4PvAptZfVzlXVLSLyKJCuqguAOcAr\nIpIJFFNbVIwxxjjIkTEMVV0ILKy37Vd1nlcA1/s6lzHGmMbZmt7GGGM8YgXDGGOMR6xgGGOM8YgV\nDGOMMR6xgmGMMcYjATW9uYgUAXvO8u3t8M+pRyzXmbFcZ8ZynZlAzNVFVRM8aRhQBeNciEi6pzM2\n+pLlOjOW68xYrjMT7LnslJQxxhiPWMEwxhjjESsY/zHb6QCNsFxnxnKdGct1ZoI6l41hGGOM8Yj1\nMIwxxngk6AqGiEwUkR0ikikiP2/g9UgRecP9+moRSfWTXDNEpEhENrgfd/kg01wRKRSRjEZeFxF5\n2p15k4gM9XYmD3ONF5GSOsfqVw2180KuZBFZKiJbRWSLiPyogTY+P2Ye5vL5MRORKBFZIyIb3bl+\n3UAbn38ePczl889jnX2HisjXIvJhA69593ipatA8qJ1OfTfQDYgANgJ967W5F5jlfj4NeMNPcs0A\nnvXx8boAGApkNPL65cDHgACjgNV+kms88KED/78SgaHu57HAzgb+HX1+zDzM5fNj5j4GLd3Pw4HV\nwKh6bZz4PHqSy+efxzr7vh/4Z0P/Xt4+XsHWwxgBZKpqlqpWAq8DU+q1mQK87H7+NnCJiIgf5PI5\nVV1O7XokjZkCzNNaXwFxIpLoB7kcoar7VXW9+3kZsI3a9enr8vkx8zCXz7mPwVH3t+HuR/1BVZ9/\nHj3M5QgR6QxcAbzQSBOvHq9gKxhJQG6d7/P49gfnmzaqWg2UAG39IBfAte7TGG+LSHIDr/uap7md\nMNp9SuFjEenn6527TwUMofav07ocPWanyAUOHDP36ZUNQCGwWFUbPV4+/Dx6kguc+Tz+FXgQcDXy\nulePV7AVjObsAyBVVQcCi/nPXxHm29ZTO93BIOAZ4F++3LmItATeAX6sqqW+3PepnCaXI8dMVWtU\ndTDQGRghIv19sd/T8SCXzz+PInIlUKiq67y9r8YEW8HIB+r+JdDZva3BNiISBrQGDjmdS1UPqeoJ\n97cvAMO8nMkTnhxPn1PV0pOnFLR2dcdwEWnni32LSDi1v5RfU9V3G2jiyDE7XS4nj5l7n0eApcDE\nei858Xk8bS6HPo9jgMkikkPtaeuLReTVem28eryCrWCsBXqKSFcRiaB2UGhBvTYLgNvcz68Dlqh7\nBMnJXPXOc0+m9jy00xYAt7qv/BkFlKjqfqdDiUjHk+dtRWQEtf/Pvf5Lxr3POcA2Vf1zI818fsw8\nyeXEMRORBBGJcz+PBiYA2+s18/nn0ZNcTnweVfUXqtpZVVOp/R2xRFVvrtfMq8fLkTW9naKq1SLy\nfeBTaq9MmquqW0TkUSBdVRdQ+8F6RUQyqR1YneYnuX4oIpOBaneuGd7OJSLzqb16pp2I5AEPUzsA\niKrOonZd9suBTOA4cLu3M3mY6zrgHhGpBsqBaT4o+lD7F+AtwGb3+W+Ah4CUOtmcOGae5HLimCUC\nL4tIKLUF6k1V/dDpz6OHuXz+eWyML4+X3eltjDHGI8F2SsoYY8xZsoJhjDHGI1YwjDHGeMQKhjHG\nGI9YwTDGGOMRKxjGGGM8YgXDGGOMR6xgGOMFIvKAiPzQ/fwvIrLE/fxiEXnN2XTGnB0rGMZ4xwpg\nnPt5GtDSPZ/TOGC5Y6mMOQdWMIzxjnXAMBFpBZwAVlFbOMZRW0yMaXaCai4pY3xFVatEJJvaOYa+\nBDYBFwE98I+JI405Y9bDMMZ7VgA/pfYU1ArgbuBrH02EaEyTs4JhjPesoHbm01WqWgBUYKejTDNm\ns9UaY4zxiPUwjDHGeMQKhjHGGI9YwTDGGOMRKxjGGGM8YgXDGGOMR6xgGGOM8YgVDGOMMR6xgmGM\nMcYj/x+VfY9anXKhIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febcc27bef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. basics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "\n",
    "# our model for the forward pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "w_list = []\n",
    "mse_list = []\n",
    "\n",
    "for w in np.arange(0.0, 4.1, 0.1):\n",
    "    print(\"w=\", w)\n",
    "    l_sum = 0\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred_val = forward(x_val)\n",
    "        l = loss(x_val, y_val)\n",
    "        l_sum += l\n",
    "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
    "    print(\"MSE=\", l_sum / 3)\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum / 3)\n",
    "\n",
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.23\n",
      "progress: 0 w= 1.26 loss= 4.92\n",
      "\tgrad:  1.0 2.0 -1.48\n",
      "\tgrad:  2.0 4.0 -5.8\n",
      "\tgrad:  3.0 6.0 -12.0\n",
      "progress: 1 w= 1.45 loss= 2.69\n",
      "\tgrad:  1.0 2.0 -1.09\n",
      "\tgrad:  2.0 4.0 -4.29\n",
      "\tgrad:  3.0 6.0 -8.87\n",
      "progress: 2 w= 1.6 loss= 1.47\n",
      "\tgrad:  1.0 2.0 -0.81\n",
      "\tgrad:  2.0 4.0 -3.17\n",
      "\tgrad:  3.0 6.0 -6.56\n",
      "progress: 3 w= 1.7 loss= 0.8\n",
      "\tgrad:  1.0 2.0 -0.6\n",
      "\tgrad:  2.0 4.0 -2.34\n",
      "\tgrad:  3.0 6.0 -4.85\n",
      "progress: 4 w= 1.78 loss= 0.44\n",
      "\tgrad:  1.0 2.0 -0.44\n",
      "\tgrad:  2.0 4.0 -1.73\n",
      "\tgrad:  3.0 6.0 -3.58\n",
      "progress: 5 w= 1.84 loss= 0.24\n",
      "\tgrad:  1.0 2.0 -0.33\n",
      "\tgrad:  2.0 4.0 -1.28\n",
      "\tgrad:  3.0 6.0 -2.65\n",
      "progress: 6 w= 1.88 loss= 0.13\n",
      "\tgrad:  1.0 2.0 -0.24\n",
      "\tgrad:  2.0 4.0 -0.95\n",
      "\tgrad:  3.0 6.0 -1.96\n",
      "progress: 7 w= 1.91 loss= 0.07\n",
      "\tgrad:  1.0 2.0 -0.18\n",
      "\tgrad:  2.0 4.0 -0.7\n",
      "\tgrad:  3.0 6.0 -1.45\n",
      "progress: 8 w= 1.93 loss= 0.04\n",
      "\tgrad:  1.0 2.0 -0.13\n",
      "\tgrad:  2.0 4.0 -0.52\n",
      "\tgrad:  3.0 6.0 -1.07\n",
      "progress: 9 w= 1.95 loss= 0.02\n",
      "predict (after training) 4 hours 7.804863933862125\n"
     ]
    }
   ],
   "source": [
    "# 2. manual gradient\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0  # a random guess: random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "# compute gradient\n",
    "def gradient(x, y):  # d_loss/d_w\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad\n",
    "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
    "        l = loss(x_val, y_val)\n",
    "\n",
    "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", \"4 hours\", forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.840000152587891\n",
      "\tgrad:  3.0 6.0 -16.228801727294922\n",
      "progress: 0 7.315943717956543\n",
      "\tgrad:  1.0 2.0 -1.478623867034912\n",
      "\tgrad:  2.0 4.0 -5.796205520629883\n",
      "\tgrad:  3.0 6.0 -11.998146057128906\n",
      "progress: 1 3.9987640380859375\n",
      "\tgrad:  1.0 2.0 -1.0931644439697266\n",
      "\tgrad:  2.0 4.0 -4.285204887390137\n",
      "\tgrad:  3.0 6.0 -8.870372772216797\n",
      "progress: 2 2.1856532096862793\n",
      "\tgrad:  1.0 2.0 -0.8081896305084229\n",
      "\tgrad:  2.0 4.0 -3.1681032180786133\n",
      "\tgrad:  3.0 6.0 -6.557973861694336\n",
      "progress: 3 1.1946394443511963\n",
      "\tgrad:  1.0 2.0 -0.5975041389465332\n",
      "\tgrad:  2.0 4.0 -2.3422164916992188\n",
      "\tgrad:  3.0 6.0 -4.848389625549316\n",
      "progress: 4 0.6529689431190491\n",
      "\tgrad:  1.0 2.0 -0.4417421817779541\n",
      "\tgrad:  2.0 4.0 -1.7316293716430664\n",
      "\tgrad:  3.0 6.0 -3.58447265625\n",
      "progress: 5 0.35690122842788696\n",
      "\tgrad:  1.0 2.0 -0.3265852928161621\n",
      "\tgrad:  2.0 4.0 -1.2802143096923828\n",
      "\tgrad:  3.0 6.0 -2.650045394897461\n",
      "progress: 6 0.195076122879982\n",
      "\tgrad:  1.0 2.0 -0.24144840240478516\n",
      "\tgrad:  2.0 4.0 -0.9464778900146484\n",
      "\tgrad:  3.0 6.0 -1.9592113494873047\n",
      "progress: 7 0.10662525147199631\n",
      "\tgrad:  1.0 2.0 -0.17850565910339355\n",
      "\tgrad:  2.0 4.0 -0.699742317199707\n",
      "\tgrad:  3.0 6.0 -1.4484672546386719\n",
      "progress: 8 0.0582793727517128\n",
      "\tgrad:  1.0 2.0 -0.1319713592529297\n",
      "\tgrad:  2.0 4.0 -0.5173273086547852\n",
      "\tgrad:  3.0 6.0 -1.070866584777832\n",
      "progress: 9 0.03185431286692619\n",
      "predict (after training) 4 7.804864406585693\n"
     ]
    }
   ],
   "source": [
    "# 3.auto-gradient\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Any random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\", 4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17.44133186340332\n",
      "1 7.856596946716309\n",
      "2 3.5884222984313965\n",
      "3 1.6870458126068115\n",
      "4 0.8393201231956482\n",
      "5 0.4606672525405884\n",
      "6 0.2908515930175781\n",
      "7 0.21402084827423096\n",
      "8 0.1786036491394043\n",
      "9 0.16163897514343262\n",
      "10 0.152906596660614\n",
      "11 0.14785592257976532\n",
      "12 0.1444604992866516\n",
      "13 0.14181920886039734\n",
      "14 0.1395292729139328\n",
      "15 0.1374119073152542\n",
      "16 0.13538718223571777\n",
      "17 0.1334192305803299\n",
      "18 0.13149185478687286\n",
      "19 0.12959778308868408\n",
      "20 0.12773314118385315\n",
      "21 0.12589684128761292\n",
      "22 0.1240869089961052\n",
      "23 0.12230344116687775\n",
      "24 0.12054561823606491\n",
      "25 0.1188134178519249\n",
      "26 0.11710581928491592\n",
      "27 0.11542265117168427\n",
      "28 0.11376374959945679\n",
      "29 0.11212897300720215\n",
      "30 0.11051750183105469\n",
      "31 0.1089291125535965\n",
      "32 0.10736353695392609\n",
      "33 0.10582072287797928\n",
      "34 0.10429979860782623\n",
      "35 0.1028008759021759\n",
      "36 0.10132341086864471\n",
      "37 0.09986723214387894\n",
      "38 0.09843217581510544\n",
      "39 0.0970175564289093\n",
      "40 0.09562300145626068\n",
      "41 0.09424875676631927\n",
      "42 0.09289439767599106\n",
      "43 0.09155930578708649\n",
      "44 0.09024355560541153\n",
      "45 0.08894652128219604\n",
      "46 0.08766829967498779\n",
      "47 0.08640819787979126\n",
      "48 0.08516654372215271\n",
      "49 0.08394254744052887\n",
      "50 0.08273608982563019\n",
      "51 0.08154697716236115\n",
      "52 0.08037498593330383\n",
      "53 0.0792200043797493\n",
      "54 0.0780813992023468\n",
      "55 0.0769592747092247\n",
      "56 0.0758533626794815\n",
      "57 0.07476320117712021\n",
      "58 0.07368868589401245\n",
      "59 0.07262969762086868\n",
      "60 0.07158592343330383\n",
      "61 0.0705571174621582\n",
      "62 0.06954317539930344\n",
      "63 0.06854354590177536\n",
      "64 0.06755855679512024\n",
      "65 0.06658764183521271\n",
      "66 0.06563068926334381\n",
      "67 0.06468743085861206\n",
      "68 0.0637577623128891\n",
      "69 0.06284146010875702\n",
      "70 0.06193827465176582\n",
      "71 0.061048176139593124\n",
      "72 0.06017085537314415\n",
      "73 0.059306077659130096\n",
      "74 0.058453671634197235\n",
      "75 0.05761367827653885\n",
      "76 0.05678568780422211\n",
      "77 0.05596965551376343\n",
      "78 0.05516521632671356\n",
      "79 0.054372381418943405\n",
      "80 0.05359102413058281\n",
      "81 0.0528208389878273\n",
      "82 0.05206174775958061\n",
      "83 0.05131348595023155\n",
      "84 0.050576094537973404\n",
      "85 0.049849189817905426\n",
      "86 0.049132831394672394\n",
      "87 0.04842673987150192\n",
      "88 0.047730691730976105\n",
      "89 0.04704464226961136\n",
      "90 0.04636858403682709\n",
      "91 0.04570230096578598\n",
      "92 0.04504537209868431\n",
      "93 0.044398050755262375\n",
      "94 0.04375993460416794\n",
      "95 0.04313105344772339\n",
      "96 0.042511217296123505\n",
      "97 0.04190019145607948\n",
      "98 0.0412980318069458\n",
      "99 0.04070453345775604\n",
      "100 0.040119580924510956\n",
      "101 0.039543069899082184\n",
      "102 0.038974612951278687\n",
      "103 0.03841447830200195\n",
      "104 0.037862397730350494\n",
      "105 0.03731829673051834\n",
      "106 0.03678204119205475\n",
      "107 0.03625340014696121\n",
      "108 0.035732343792915344\n",
      "109 0.03521881625056267\n",
      "110 0.034712642431259155\n",
      "111 0.034213852137327194\n",
      "112 0.03372206911444664\n",
      "113 0.03323742747306824\n",
      "114 0.032759759575128555\n",
      "115 0.0322890430688858\n",
      "116 0.03182494640350342\n",
      "117 0.031367480754852295\n",
      "118 0.03091679885983467\n",
      "119 0.03047247603535652\n",
      "120 0.030034443363547325\n",
      "121 0.029602782800793648\n",
      "122 0.029177404940128326\n",
      "123 0.028758082538843155\n",
      "124 0.02834481932222843\n",
      "125 0.02793738804757595\n",
      "126 0.027535904198884964\n",
      "127 0.027140123769640923\n",
      "128 0.02675015851855278\n",
      "129 0.02636566385626793\n",
      "130 0.025986770167946815\n",
      "131 0.02561323717236519\n",
      "132 0.025245200842618942\n",
      "133 0.024882333353161812\n",
      "134 0.024524759501218796\n",
      "135 0.024172276258468628\n",
      "136 0.023824885487556458\n",
      "137 0.023482564836740494\n",
      "138 0.023144997656345367\n",
      "139 0.02281247079372406\n",
      "140 0.02248460054397583\n",
      "141 0.02216145023703575\n",
      "142 0.02184293419122696\n",
      "143 0.02152903378009796\n",
      "144 0.021219607442617416\n",
      "145 0.02091461606323719\n",
      "146 0.02061404474079609\n",
      "147 0.020317796617746353\n",
      "148 0.020025819540023804\n",
      "149 0.019738001748919487\n",
      "150 0.01945432648062706\n",
      "151 0.019174795597791672\n",
      "152 0.01889922469854355\n",
      "153 0.018627583980560303\n",
      "154 0.018359864130616188\n",
      "155 0.018096012994647026\n",
      "156 0.017835913226008415\n",
      "157 0.017579590901732445\n",
      "158 0.017326977103948593\n",
      "159 0.017077893018722534\n",
      "160 0.0168325062841177\n",
      "161 0.016590602695941925\n",
      "162 0.0163522120565176\n",
      "163 0.01611720770597458\n",
      "164 0.01588553749024868\n",
      "165 0.015657218173146248\n",
      "166 0.015432205982506275\n",
      "167 0.015210392884910107\n",
      "168 0.014991851523518562\n",
      "169 0.014776375144720078\n",
      "170 0.014563973993062973\n",
      "171 0.014354677870869637\n",
      "172 0.01414844673126936\n",
      "173 0.013945081271231174\n",
      "174 0.01374468021094799\n",
      "175 0.013547129929065704\n",
      "176 0.01335243508219719\n",
      "177 0.01316053606569767\n",
      "178 0.012971358373761177\n",
      "179 0.012784969061613083\n",
      "180 0.012601207010447979\n",
      "181 0.012420130893588066\n",
      "182 0.012241658754646778\n",
      "183 0.012065684422850609\n",
      "184 0.011892290785908699\n",
      "185 0.011721384711563587\n",
      "186 0.011552928946912289\n",
      "187 0.01138689648360014\n",
      "188 0.011223210953176022\n",
      "189 0.0110619580373168\n",
      "190 0.010902946814894676\n",
      "191 0.010746268555521965\n",
      "192 0.010591799393296242\n",
      "193 0.010439584963023663\n",
      "194 0.010289587080478668\n",
      "195 0.01014168094843626\n",
      "196 0.009995932690799236\n",
      "197 0.009852273389697075\n",
      "198 0.009710678830742836\n",
      "199 0.009571133181452751\n",
      "200 0.009433613158762455\n",
      "201 0.009298047982156277\n",
      "202 0.009164420887827873\n",
      "203 0.0090326564386487\n",
      "204 0.008902833797037601\n",
      "205 0.008774898014962673\n",
      "206 0.008648823946714401\n",
      "207 0.008524488657712936\n",
      "208 0.008402013219892979\n",
      "209 0.008281254209578037\n",
      "210 0.008162278681993484\n",
      "211 0.00804494135081768\n",
      "212 0.007929297164082527\n",
      "213 0.007815337739884853\n",
      "214 0.007703045383095741\n",
      "215 0.00759236142039299\n",
      "216 0.007483197376132011\n",
      "217 0.007375672925263643\n",
      "218 0.007269670721143484\n",
      "219 0.007165207993239164\n",
      "220 0.0070622386410832405\n",
      "221 0.006960694212466478\n",
      "222 0.006860669702291489\n",
      "223 0.006762063596397638\n",
      "224 0.006664905697107315\n",
      "225 0.006569091696292162\n",
      "226 0.00647469749674201\n",
      "227 0.006381649989634752\n",
      "228 0.006289972923696041\n",
      "229 0.006199562456458807\n",
      "230 0.006110461428761482\n",
      "231 0.006022636778652668\n",
      "232 0.005936088971793652\n",
      "233 0.0058508035726845264\n",
      "234 0.005766656249761581\n",
      "235 0.005683802533894777\n",
      "236 0.005602139513939619\n",
      "237 0.005521642509847879\n",
      "238 0.005442289635539055\n",
      "239 0.005364065058529377\n",
      "240 0.00528699392452836\n",
      "241 0.0052109635435044765\n",
      "242 0.005136088002473116\n",
      "243 0.005062300246208906\n",
      "244 0.00498951505869627\n",
      "245 0.004917806945741177\n",
      "246 0.0048471237532794476\n",
      "247 0.004777466878294945\n",
      "248 0.004708828404545784\n",
      "249 0.004641154780983925\n",
      "250 0.004574434366077185\n",
      "251 0.004508696962147951\n",
      "252 0.0044439006596803665\n",
      "253 0.004380058031529188\n",
      "254 0.0043170819990336895\n",
      "255 0.004255044274032116\n",
      "256 0.00419390294700861\n",
      "257 0.004133657086640596\n",
      "258 0.0040742214769124985\n",
      "259 0.004015673417598009\n",
      "260 0.003957973327487707\n",
      "261 0.003901079297065735\n",
      "262 0.0038449973799288273\n",
      "263 0.00378975085914135\n",
      "264 0.003735298989340663\n",
      "265 0.0036816394422203302\n",
      "266 0.003628707956522703\n",
      "267 0.003576532006263733\n",
      "268 0.003525134176015854\n",
      "269 0.0034744683653116226\n",
      "270 0.00342455692589283\n",
      "271 0.003375324187800288\n",
      "272 0.00332680344581604\n",
      "273 0.0032790168188512325\n",
      "274 0.003231886774301529\n",
      "275 0.003185416106134653\n",
      "276 0.0031396537087857723\n",
      "277 0.0030945357866585255\n",
      "278 0.0030500718858093023\n",
      "279 0.003006229642778635\n",
      "280 0.002963027451187372\n",
      "281 0.0029204427264630795\n",
      "282 0.0028784689493477345\n",
      "283 0.0028370963409543037\n",
      "284 0.002796333283185959\n",
      "285 0.0027561497408896685\n",
      "286 0.0027165384963154793\n",
      "287 0.002677496988326311\n",
      "288 0.002639015670865774\n",
      "289 0.002601097570732236\n",
      "290 0.002563710091635585\n",
      "291 0.0025268415920436382\n",
      "292 0.0024905449245125055\n",
      "293 0.0024547383654862642\n",
      "294 0.002419479889795184\n",
      "295 0.002384706400334835\n",
      "296 0.0023504160344600677\n",
      "297 0.002316639758646488\n",
      "298 0.0022833384573459625\n",
      "299 0.0022505291271954775\n",
      "300 0.0022181791719049215\n",
      "301 0.0021863135043531656\n",
      "302 0.002154899761080742\n",
      "303 0.0021239276975393295\n",
      "304 0.0020933980122208595\n",
      "305 0.0020632953383028507\n",
      "306 0.002033672761172056\n",
      "307 0.0020044129341840744\n",
      "308 0.001975610386580229\n",
      "309 0.0019472218118607998\n",
      "310 0.00191925757098943\n",
      "311 0.001891660038381815\n",
      "312 0.0018644683295860887\n",
      "313 0.001837685238569975\n",
      "314 0.0018112739780917764\n",
      "315 0.0017852382734417915\n",
      "316 0.001759576378390193\n",
      "317 0.0017342794453725219\n",
      "318 0.001709377160295844\n",
      "319 0.0016848173690959811\n",
      "320 0.0016606012359261513\n",
      "321 0.0016367319040000439\n",
      "322 0.0016132035525515676\n",
      "323 0.0015900080325081944\n",
      "324 0.0015671711880713701\n",
      "325 0.0015446551842615008\n",
      "326 0.001522444887086749\n",
      "327 0.001500587211921811\n",
      "328 0.001479003462009132\n",
      "329 0.0014577293768525124\n",
      "330 0.001436814432963729\n",
      "331 0.0014161424478515983\n",
      "332 0.0013958070194348693\n",
      "333 0.0013757201377302408\n",
      "334 0.0013559521175920963\n",
      "335 0.001336471876129508\n",
      "336 0.0013172696344554424\n",
      "337 0.00129833840765059\n",
      "338 0.0012796793598681688\n",
      "339 0.0012612813152372837\n",
      "340 0.0012431723298504949\n",
      "341 0.0012252959422767162\n",
      "342 0.0012076866114512086\n",
      "343 0.0011903250124305487\n",
      "344 0.0011732224375009537\n",
      "345 0.0011563603766262531\n",
      "346 0.0011397401103749871\n",
      "347 0.0011233687400817871\n",
      "348 0.0011072184424847364\n",
      "349 0.0010913002770394087\n",
      "350 0.001075628912076354\n",
      "351 0.0010601805988699198\n",
      "352 0.0010449381079524755\n",
      "353 0.0010299105197191238\n",
      "354 0.0010151270544156432\n",
      "355 0.001000520191155374\n",
      "356 0.000986145343631506\n",
      "357 0.0009719650261104107\n",
      "358 0.0009580061305314302\n",
      "359 0.0009442305308766663\n",
      "360 0.000930660986341536\n",
      "361 0.0009172935970127583\n",
      "362 0.0009041068842634559\n",
      "363 0.0008910989854484797\n",
      "364 0.0008783075609244406\n",
      "365 0.0008656788850203156\n",
      "366 0.0008532365900464356\n",
      "367 0.0008409688016399741\n",
      "368 0.0008288936223834753\n",
      "369 0.0008169771754182875\n",
      "370 0.000805242802016437\n",
      "371 0.0007936734473332763\n",
      "372 0.0007822448387742043\n",
      "373 0.0007710185018368065\n",
      "374 0.0007599343080073595\n",
      "375 0.0007490161806344986\n",
      "376 0.0007382576586678624\n",
      "377 0.0007276319665834308\n",
      "378 0.0007171953329816461\n",
      "379 0.000706877326592803\n",
      "380 0.0006967108929529786\n",
      "381 0.0006867018528282642\n",
      "382 0.000676845433190465\n",
      "383 0.0006670992588624358\n",
      "384 0.0006575178122147918\n",
      "385 0.0006480602896772325\n",
      "386 0.0006387463072314858\n",
      "387 0.0006295843049883842\n",
      "388 0.0006205262034200132\n",
      "389 0.0006116185104474425\n",
      "390 0.0006028309580869973\n",
      "391 0.0005941679701209068\n",
      "392 0.000585618254262954\n",
      "393 0.0005772046279162169\n",
      "394 0.0005689072422683239\n",
      "395 0.0005607278435491025\n",
      "396 0.0005526691093109548\n",
      "397 0.0005447311559692025\n",
      "398 0.0005369099089875817\n",
      "399 0.0005291853449307382\n",
      "400 0.0005215764977037907\n",
      "401 0.0005140926223248243\n",
      "402 0.0005066942539997399\n",
      "403 0.000499411893542856\n",
      "404 0.000492232502438128\n",
      "405 0.0004851563717238605\n",
      "406 0.0004781822790391743\n",
      "407 0.00047132058534771204\n",
      "408 0.00046453476534225047\n",
      "409 0.00045786285772919655\n",
      "410 0.000451292289653793\n",
      "411 0.00044480233918875456\n",
      "412 0.0004384178901091218\n",
      "413 0.0004321024753153324\n",
      "414 0.0004259012930560857\n",
      "415 0.00041977237560786307\n",
      "416 0.00041374930879101157\n",
      "417 0.0004078026977367699\n",
      "418 0.00040193641325458884\n",
      "419 0.000396157760405913\n",
      "420 0.0003904680488631129\n",
      "421 0.0003848582855425775\n",
      "422 0.000379318546038121\n",
      "423 0.0003738718223758042\n",
      "424 0.0003684990806505084\n",
      "425 0.00036320515209808946\n",
      "426 0.0003579824697226286\n",
      "427 0.0003528391825966537\n",
      "428 0.000347773137036711\n",
      "429 0.0003427664050832391\n",
      "430 0.0003378461697138846\n",
      "431 0.0003329890896566212\n",
      "432 0.0003282040706835687\n",
      "433 0.0003234902978874743\n",
      "434 0.00031884171767160296\n",
      "435 0.00031425050110556185\n",
      "436 0.0003097458102274686\n",
      "437 0.0003052878601010889\n",
      "438 0.0003008985077030957\n",
      "439 0.000296573038212955\n",
      "440 0.00029231561347842216\n",
      "441 0.0002881116815842688\n",
      "442 0.0002839675871655345\n",
      "443 0.00027988950023427606\n",
      "444 0.00027586991200223565\n",
      "445 0.0002719081239774823\n",
      "446 0.00026799499755725265\n",
      "447 0.00026414802414365113\n",
      "448 0.0002603486063890159\n",
      "449 0.0002566027396824211\n",
      "450 0.0002529219491407275\n",
      "451 0.0002492796629667282\n",
      "452 0.0002456975635141134\n",
      "453 0.00024217390455305576\n",
      "454 0.00023869099095463753\n",
      "455 0.0002352591691305861\n",
      "456 0.00023188053455669433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457 0.0002285386435687542\n",
      "458 0.0002252663834951818\n",
      "459 0.0002220229071099311\n",
      "460 0.0002188296348322183\n",
      "461 0.0002156851114705205\n",
      "462 0.00021259045752231032\n",
      "463 0.0002095341624226421\n",
      "464 0.00020652673265431076\n",
      "465 0.00020355184096843004\n",
      "466 0.00020062322437297553\n",
      "467 0.00019774770771618932\n",
      "468 0.00019490676640998572\n",
      "469 0.00019210169557482004\n",
      "470 0.0001893401495181024\n",
      "471 0.00018661764624994248\n",
      "472 0.00018393853679299355\n",
      "473 0.0001812897389754653\n",
      "474 0.00017869124712888151\n",
      "475 0.0001761269522830844\n",
      "476 0.00017358886543661356\n",
      "477 0.0001710950309643522\n",
      "478 0.00016863209020812064\n",
      "479 0.00016620874521322548\n",
      "480 0.00016382525791414082\n",
      "481 0.0001614715438336134\n",
      "482 0.00015915391850285232\n",
      "483 0.0001568618172314018\n",
      "484 0.00015460007125511765\n",
      "485 0.00015238129708450288\n",
      "486 0.00015019637066870928\n",
      "487 0.00014803642989136279\n",
      "488 0.00014591318904422224\n",
      "489 0.00014381157234311104\n",
      "490 0.00014174595708027482\n",
      "491 0.0001397097366861999\n",
      "492 0.0001376984582748264\n",
      "493 0.00013572142051998526\n",
      "494 0.00013377011055126786\n",
      "495 0.00013184694398660213\n",
      "496 0.00012995295401196927\n",
      "497 0.0001280884607695043\n",
      "498 0.00012624984083231539\n",
      "499 0.0001244355516973883\n",
      "predict (after training) 4 7.98717737197876\n"
     ]
    }
   ],
   "source": [
    "# 5. linear regression\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (after training)\", 4, model(hour_var).data[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.030257225036621\n",
      "1 1.02210533618927\n",
      "2 1.014214038848877\n",
      "3 1.006577491760254\n",
      "4 0.999190092086792\n",
      "5 0.9920457601547241\n",
      "6 0.9851385354995728\n",
      "7 0.978462278842926\n",
      "8 0.9720110297203064\n",
      "9 0.9657788276672363\n",
      "10 0.9597592949867249\n",
      "11 0.9539466500282288\n",
      "12 0.9483346939086914\n",
      "13 0.9429175853729248\n",
      "14 0.9376891851425171\n",
      "15 0.9326438307762146\n",
      "16 0.9277755618095398\n",
      "17 0.9230787754058838\n",
      "18 0.9185477495193481\n",
      "19 0.9141770601272583\n",
      "20 0.9099611639976501\n",
      "21 0.9058948159217834\n",
      "22 0.9019728302955627\n",
      "23 0.8981900811195374\n",
      "24 0.8945416212081909\n",
      "25 0.8910225033760071\n",
      "26 0.8876281380653381\n",
      "27 0.8843540549278259\n",
      "28 0.881195604801178\n",
      "29 0.8781484961509705\n",
      "30 0.8752084970474243\n",
      "31 0.8723716139793396\n",
      "32 0.8696339726448059\n",
      "33 0.8669915199279785\n",
      "34 0.8644406795501709\n",
      "35 0.8619779944419861\n",
      "36 0.8595998883247375\n",
      "37 0.8573029637336731\n",
      "38 0.855084240436554\n",
      "39 0.8529403805732727\n",
      "40 0.8508685827255249\n",
      "41 0.8488659262657166\n",
      "42 0.8469294309616089\n",
      "43 0.8450567126274109\n",
      "44 0.8432450890541077\n",
      "45 0.8414920568466187\n",
      "46 0.8397952914237976\n",
      "47 0.8381525874137878\n",
      "48 0.8365615010261536\n",
      "49 0.8350201845169067\n",
      "50 0.8335264921188354\n",
      "51 0.8320785164833069\n",
      "52 0.8306743502616882\n",
      "53 0.829312264919281\n",
      "54 0.8279903531074524\n",
      "55 0.8267071843147278\n",
      "56 0.825461208820343\n",
      "57 0.8242506980895996\n",
      "58 0.8230742812156677\n",
      "59 0.8219306468963623\n",
      "60 0.8208183646202087\n",
      "61 0.819736123085022\n",
      "62 0.8186827898025513\n",
      "63 0.8176570534706116\n",
      "64 0.8166579604148865\n",
      "65 0.8156843781471252\n",
      "66 0.8147351145744324\n",
      "67 0.8138093948364258\n",
      "68 0.8129059076309204\n",
      "69 0.8120242357254028\n",
      "70 0.8111631870269775\n",
      "71 0.8103219270706177\n",
      "72 0.8094998002052307\n",
      "73 0.8086958527565002\n",
      "74 0.8079094290733337\n",
      "75 0.8071398735046387\n",
      "76 0.806386411190033\n",
      "77 0.8056485056877136\n",
      "78 0.8049254417419434\n",
      "79 0.8042166829109192\n",
      "80 0.8035215735435486\n",
      "81 0.8028396368026733\n",
      "82 0.8021703958511353\n",
      "83 0.8015131950378418\n",
      "84 0.8008677363395691\n",
      "85 0.8002332448959351\n",
      "86 0.7996096611022949\n",
      "87 0.7989963293075562\n",
      "88 0.7983930110931396\n",
      "89 0.7977991104125977\n",
      "90 0.7972143292427063\n",
      "91 0.7966383695602417\n",
      "92 0.7960708141326904\n",
      "93 0.7955113649368286\n",
      "94 0.7949597835540771\n",
      "95 0.7944155931472778\n",
      "96 0.7938787937164307\n",
      "97 0.7933487296104431\n",
      "98 0.7928253412246704\n",
      "99 0.7923083901405334\n",
      "100 0.7917975783348083\n",
      "101 0.791292667388916\n",
      "102 0.7907934784889221\n",
      "103 0.7902997136116028\n",
      "104 0.7898112535476685\n",
      "105 0.7893278002738953\n",
      "106 0.7888492345809937\n",
      "107 0.7883754968643188\n",
      "108 0.7879061698913574\n",
      "109 0.7874411940574646\n",
      "110 0.786980390548706\n",
      "111 0.7865236401557922\n",
      "112 0.786070704460144\n",
      "113 0.7856215238571167\n",
      "114 0.7851759791374207\n",
      "115 0.7847338914871216\n",
      "116 0.7842952013015747\n",
      "117 0.7838595509529114\n",
      "118 0.7834271788597107\n",
      "119 0.7829978466033936\n",
      "120 0.7825711965560913\n",
      "121 0.7821474075317383\n",
      "122 0.7817263603210449\n",
      "123 0.7813078165054321\n",
      "124 0.7808916568756104\n",
      "125 0.7804781198501587\n",
      "126 0.7800669074058533\n",
      "127 0.779657781124115\n",
      "128 0.7792509198188782\n",
      "129 0.778846025466919\n",
      "130 0.7784432768821716\n",
      "131 0.7780424356460571\n",
      "132 0.7776435017585754\n",
      "133 0.7772464156150818\n",
      "134 0.7768508791923523\n",
      "135 0.7764571905136108\n",
      "136 0.7760650515556335\n",
      "137 0.7756747007369995\n",
      "138 0.7752857208251953\n",
      "139 0.7748981714248657\n",
      "140 0.7745121717453003\n",
      "141 0.7741276025772095\n",
      "142 0.7737442255020142\n",
      "143 0.7733622789382935\n",
      "144 0.7729815244674683\n",
      "145 0.7726020812988281\n",
      "146 0.772223711013794\n",
      "147 0.7718465924263\n",
      "148 0.7714705467224121\n",
      "149 0.7710955142974854\n",
      "150 0.7707216143608093\n",
      "151 0.7703486084938049\n",
      "152 0.7699767351150513\n",
      "153 0.7696056962013245\n",
      "154 0.7692357301712036\n",
      "155 0.7688664793968201\n",
      "156 0.7684983015060425\n",
      "157 0.7681307792663574\n",
      "158 0.7677642703056335\n",
      "159 0.7673984169960022\n",
      "160 0.7670334577560425\n",
      "161 0.7666692137718201\n",
      "162 0.7663057446479797\n",
      "163 0.7659429311752319\n",
      "164 0.7655808925628662\n",
      "165 0.7652194499969482\n",
      "166 0.7648587822914124\n",
      "167 0.7644987106323242\n",
      "168 0.7641392350196838\n",
      "169 0.763780415058136\n",
      "170 0.7634221315383911\n",
      "171 0.7630645632743835\n",
      "172 0.762707531452179\n",
      "173 0.7623510956764221\n",
      "174 0.7619951367378235\n",
      "175 0.7616397142410278\n",
      "176 0.7612849473953247\n",
      "177 0.7609306573867798\n",
      "178 0.7605766654014587\n",
      "179 0.760223388671875\n",
      "180 0.7598705291748047\n",
      "181 0.7595180869102478\n",
      "182 0.7591662406921387\n",
      "183 0.758814811706543\n",
      "184 0.7584637999534607\n",
      "185 0.7581132650375366\n",
      "186 0.7577630877494812\n",
      "187 0.7574133276939392\n",
      "188 0.7570641040802002\n",
      "189 0.7567152380943298\n",
      "190 0.7563667297363281\n",
      "191 0.7560186386108398\n",
      "192 0.7556709051132202\n",
      "193 0.755323588848114\n",
      "194 0.7549766898155212\n",
      "195 0.7546300888061523\n",
      "196 0.7542839050292969\n",
      "197 0.7539380192756653\n",
      "198 0.7535925507545471\n",
      "199 0.7532474398612976\n",
      "200 0.7529025673866272\n",
      "201 0.7525581121444702\n",
      "202 0.7522140741348267\n",
      "203 0.7518702745437622\n",
      "204 0.7515268325805664\n",
      "205 0.7511837482452393\n",
      "206 0.7508409023284912\n",
      "207 0.750498354434967\n",
      "208 0.7501562237739563\n",
      "209 0.7498142719268799\n",
      "210 0.7494727373123169\n",
      "211 0.749131441116333\n",
      "212 0.7487905025482178\n",
      "213 0.7484498023986816\n",
      "214 0.7481095194816589\n",
      "215 0.7477694153785706\n",
      "216 0.747429609298706\n",
      "217 0.747090220451355\n",
      "218 0.7467508912086487\n",
      "219 0.7464120388031006\n",
      "220 0.746073305606842\n",
      "221 0.7457349896430969\n",
      "222 0.7453969120979309\n",
      "223 0.7450590133666992\n",
      "224 0.7447214126586914\n",
      "225 0.7443841695785522\n",
      "226 0.7440471053123474\n",
      "227 0.7437103986740112\n",
      "228 0.7433739304542542\n",
      "229 0.7430376410484314\n",
      "230 0.7427017688751221\n",
      "231 0.7423660755157471\n",
      "232 0.7420305609703064\n",
      "233 0.7416953444480896\n",
      "234 0.7413604259490967\n",
      "235 0.7410257458686829\n",
      "236 0.7406913638114929\n",
      "237 0.7403571009635925\n",
      "238 0.7400231957435608\n",
      "239 0.7396894693374634\n",
      "240 0.7393561601638794\n",
      "241 0.7390229105949402\n",
      "242 0.7386899590492249\n",
      "243 0.7383573651313782\n",
      "244 0.738024890422821\n",
      "245 0.7376925945281982\n",
      "246 0.7373606562614441\n",
      "247 0.737028956413269\n",
      "248 0.7366974353790283\n",
      "249 0.7363662123680115\n",
      "250 0.736035168170929\n",
      "251 0.7357044816017151\n",
      "252 0.735373854637146\n",
      "253 0.7350435853004456\n",
      "254 0.7347134947776794\n",
      "255 0.7343837022781372\n",
      "256 0.7340541481971741\n",
      "257 0.7337247729301453\n",
      "258 0.7333955764770508\n",
      "259 0.7330666780471802\n",
      "260 0.7327380776405334\n",
      "261 0.7324095964431763\n",
      "262 0.7320814728736877\n",
      "263 0.731753408908844\n",
      "264 0.7314257025718689\n",
      "265 0.7310981750488281\n",
      "266 0.7307709455490112\n",
      "267 0.7304437756538391\n",
      "268 0.7301170229911804\n",
      "269 0.729790449142456\n",
      "270 0.7294639945030212\n",
      "271 0.7291378378868103\n",
      "272 0.7288119792938232\n",
      "273 0.7284863591194153\n",
      "274 0.7281607389450073\n",
      "275 0.7278355956077576\n",
      "276 0.7275104522705078\n",
      "277 0.7271857261657715\n",
      "278 0.7268612384796143\n",
      "279 0.7265368103981018\n",
      "280 0.7262126803398132\n",
      "281 0.725888729095459\n",
      "282 0.7255650162696838\n",
      "283 0.7252416014671326\n",
      "284 0.7249183654785156\n",
      "285 0.7245954275131226\n",
      "286 0.724272608757019\n",
      "287 0.7239500284194946\n",
      "288 0.7236276865005493\n",
      "289 0.7233054637908936\n",
      "290 0.7229836583137512\n",
      "291 0.7226618528366089\n",
      "292 0.72234046459198\n",
      "293 0.7220191955566406\n",
      "294 0.7216982245445251\n",
      "295 0.7213773727416992\n",
      "296 0.7210567593574524\n",
      "297 0.7207363843917847\n",
      "298 0.720416247844696\n",
      "299 0.7200962901115417\n",
      "300 0.7197765707969666\n",
      "301 0.7194570302963257\n",
      "302 0.7191377282142639\n",
      "303 0.7188186645507812\n",
      "304 0.7184999585151672\n",
      "305 0.7181812524795532\n",
      "306 0.7178627848625183\n",
      "307 0.7175445556640625\n",
      "308 0.7172266244888306\n",
      "309 0.7169088125228882\n",
      "310 0.7165912389755249\n",
      "311 0.7162739038467407\n",
      "312 0.7159568667411804\n",
      "313 0.7156399488449097\n",
      "314 0.7153232097625732\n",
      "315 0.7150067687034607\n",
      "316 0.7146904468536377\n",
      "317 0.7143744230270386\n",
      "318 0.7140586376190186\n",
      "319 0.7137430310249329\n",
      "320 0.7134275436401367\n",
      "321 0.7131124138832092\n",
      "322 0.7127974033355713\n",
      "323 0.712482750415802\n",
      "324 0.7121680974960327\n",
      "325 0.7118538022041321\n",
      "326 0.7115396857261658\n",
      "327 0.7112258076667786\n",
      "328 0.7109121084213257\n",
      "329 0.7105986475944519\n",
      "330 0.7102853059768677\n",
      "331 0.7099723219871521\n",
      "332 0.7096594572067261\n",
      "333 0.7093468904495239\n",
      "334 0.7090343832969666\n",
      "335 0.7087222337722778\n",
      "336 0.7084102034568787\n",
      "337 0.7080984711647034\n",
      "338 0.7077869772911072\n",
      "339 0.7074755430221558\n",
      "340 0.7071643471717834\n",
      "341 0.706853449344635\n",
      "342 0.7065427303314209\n",
      "343 0.7062322497367859\n",
      "344 0.7059219479560852\n",
      "345 0.7056118845939636\n",
      "346 0.7053020596504211\n",
      "347 0.7049923539161682\n",
      "348 0.7046828866004944\n",
      "349 0.7043735980987549\n",
      "350 0.7040646076202393\n",
      "351 0.7037558555603027\n",
      "352 0.7034471035003662\n",
      "353 0.7031387090682983\n",
      "354 0.7028305530548096\n",
      "355 0.7025225758552551\n",
      "356 0.7022148370742798\n",
      "357 0.701907217502594\n",
      "358 0.7015998959541321\n",
      "359 0.7012927532196045\n",
      "360 0.7009857296943665\n",
      "361 0.7006790637969971\n",
      "362 0.7003725171089172\n",
      "363 0.7000662088394165\n",
      "364 0.6997601389884949\n",
      "365 0.6994541883468628\n",
      "366 0.6991485953330994\n",
      "367 0.6988431811332703\n",
      "368 0.6985377669334412\n",
      "369 0.6982327699661255\n",
      "370 0.6979278922080994\n",
      "371 0.6976231932640076\n",
      "372 0.6973187923431396\n",
      "373 0.697014570236206\n",
      "374 0.6967106461524963\n",
      "375 0.6964067816734314\n",
      "376 0.6961031556129456\n",
      "377 0.6957997679710388\n",
      "378 0.6954965591430664\n",
      "379 0.6951934695243835\n",
      "380 0.6948907971382141\n",
      "381 0.6945881843566895\n",
      "382 0.6942858695983887\n",
      "383 0.6939836740493774\n",
      "384 0.6936817765235901\n",
      "385 0.6933799982070923\n",
      "386 0.6930783987045288\n",
      "387 0.692777156829834\n",
      "388 0.6924760341644287\n",
      "389 0.6921751499176025\n",
      "390 0.6918743252754211\n",
      "391 0.6915738582611084\n",
      "392 0.69127357006073\n",
      "393 0.6909734010696411\n",
      "394 0.6906735301017761\n",
      "395 0.6903738975524902\n",
      "396 0.6900743842124939\n",
      "397 0.6897751092910767\n",
      "398 0.6894760131835938\n",
      "399 0.6891770958900452\n",
      "400 0.6888784766197205\n",
      "401 0.6885799765586853\n",
      "402 0.6882817149162292\n",
      "403 0.6879836916923523\n",
      "404 0.6876859068870544\n",
      "405 0.6873883008956909\n",
      "406 0.6870908737182617\n",
      "407 0.6867936253547668\n",
      "408 0.6864965558052063\n",
      "409 0.6861996650695801\n",
      "410 0.6859031319618225\n",
      "411 0.6856067180633545\n",
      "412 0.6853104829788208\n",
      "413 0.6850143671035767\n",
      "414 0.6847185492515564\n",
      "415 0.6844229102134705\n",
      "416 0.6841275691986084\n",
      "417 0.6838323473930359\n",
      "418 0.6835373640060425\n",
      "419 0.6832426190376282\n",
      "420 0.6829479932785034\n",
      "421 0.6826536059379578\n",
      "422 0.6823593974113464\n",
      "423 0.6820653676986694\n",
      "424 0.6817715764045715\n",
      "425 0.6814780235290527\n",
      "426 0.6811846494674683\n",
      "427 0.6808913946151733\n",
      "428 0.6805984973907471\n",
      "429 0.6803056001663208\n",
      "430 0.680013120174408\n",
      "431 0.6797206401824951\n",
      "432 0.6794285178184509\n",
      "433 0.6791365742683411\n",
      "434 0.6788447499275208\n",
      "435 0.6785531640052795\n",
      "436 0.6782618761062622\n",
      "437 0.6779706478118896\n",
      "438 0.6776796579360962\n",
      "439 0.6773889064788818\n",
      "440 0.6770983338356018\n",
      "441 0.6768079400062561\n",
      "442 0.6765177249908447\n",
      "443 0.6762278079986572\n",
      "444 0.675938069820404\n",
      "445 0.6756483912467957\n",
      "446 0.6753590703010559\n",
      "447 0.6750699281692505\n",
      "448 0.6747809648513794\n",
      "449 0.6744920611381531\n",
      "450 0.6742035746574402\n",
      "451 0.6739150881767273\n",
      "452 0.6736270189285278\n",
      "453 0.6733390092849731\n",
      "454 0.6730512380599976\n",
      "455 0.6727636456489563\n",
      "456 0.6724762320518494\n",
      "457 0.6721889972686768\n",
      "458 0.671902060508728\n",
      "459 0.6716153025627136\n",
      "460 0.6713285446166992\n",
      "461 0.671042263507843\n",
      "462 0.6707560420036316\n",
      "463 0.6704700589179993\n",
      "464 0.6701842546463013\n",
      "465 0.6698986887931824\n",
      "466 0.6696131825447083\n",
      "467 0.6693280339241028\n",
      "468 0.6690430045127869\n",
      "469 0.6687581539154053\n",
      "470 0.6684735417366028\n",
      "471 0.6681890487670898\n",
      "472 0.6679049134254456\n",
      "473 0.6676207184791565\n",
      "474 0.6673370003700256\n",
      "475 0.6670533418655396\n",
      "476 0.6667699217796326\n",
      "477 0.6664866805076599\n",
      "478 0.6662036180496216\n",
      "479 0.6659206748008728\n",
      "480 0.6656380891799927\n",
      "481 0.6653556227684021\n",
      "482 0.6650733351707458\n",
      "483 0.6647911667823792\n",
      "484 0.6645092964172363\n",
      "485 0.6642276048660278\n",
      "486 0.6639460921287537\n",
      "487 0.6636648178100586\n",
      "488 0.6633837223052979\n",
      "489 0.6631027460098267\n",
      "490 0.6628220677375793\n",
      "491 0.6625416278839111\n",
      "492 0.6622613072395325\n",
      "493 0.6619811058044434\n",
      "494 0.6617011427879333\n",
      "495 0.6614214181900024\n",
      "496 0.6611418724060059\n",
      "497 0.6608625054359436\n",
      "498 0.6605833172798157\n",
      "499 0.6603043079376221\n",
      "500 0.6600255966186523\n",
      "501 0.6597469449043274\n",
      "502 0.6594685912132263\n",
      "503 0.6591904163360596\n",
      "504 0.6589123606681824\n",
      "505 0.6586345434188843\n",
      "506 0.6583569049835205\n",
      "507 0.6580795049667358\n",
      "508 0.6578022837638855\n",
      "509 0.6575252413749695\n",
      "510 0.6572483777999878\n",
      "511 0.6569716930389404\n",
      "512 0.6566952466964722\n",
      "513 0.6564189195632935\n",
      "514 0.6561428308486938\n",
      "515 0.6558669209480286\n",
      "516 0.6555911898612976\n",
      "517 0.6553156971931458\n",
      "518 0.6550403833389282\n",
      "519 0.6547653079032898\n",
      "520 0.6544902920722961\n",
      "521 0.6542155742645264\n",
      "522 0.6539410352706909\n",
      "523 0.653666615486145\n",
      "524 0.6533924341201782\n",
      "525 0.6531184911727905\n",
      "526 0.6528446078300476\n",
      "527 0.6525710225105286\n",
      "528 0.6522976160049438\n",
      "529 0.6520243883132935\n",
      "530 0.6517513990402222\n",
      "531 0.6514784693717957\n",
      "532 0.651205837726593\n",
      "533 0.6509333848953247\n",
      "534 0.6506611108779907\n",
      "535 0.6503890156745911\n",
      "536 0.6501170992851257\n",
      "537 0.6498453617095947\n",
      "538 0.649573802947998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539 0.6493025422096252\n",
      "540 0.649031400680542\n",
      "541 0.6487604379653931\n",
      "542 0.6484897136688232\n",
      "543 0.6482189893722534\n",
      "544 0.647948682308197\n",
      "545 0.6476784348487854\n",
      "546 0.6474084258079529\n",
      "547 0.6471385955810547\n",
      "548 0.6468689441680908\n",
      "549 0.646599531173706\n",
      "550 0.6463301777839661\n",
      "551 0.64606112241745\n",
      "552 0.6457922458648682\n",
      "553 0.6455235481262207\n",
      "554 0.6452550292015076\n",
      "555 0.6449866890907288\n",
      "556 0.644718587398529\n",
      "557 0.6444506049156189\n",
      "558 0.6441828012466431\n",
      "559 0.6439152359962463\n",
      "560 0.6436478495597839\n",
      "561 0.6433806419372559\n",
      "562 0.6431135535240173\n",
      "563 0.6428467631340027\n",
      "564 0.6425800919532776\n",
      "565 0.6423134803771973\n",
      "566 0.6420472264289856\n",
      "567 0.6417811512947083\n",
      "568 0.6415152549743652\n",
      "569 0.6412495970726013\n",
      "570 0.6409839987754822\n",
      "571 0.6407186388969421\n",
      "572 0.6404534578323364\n",
      "573 0.6401883959770203\n",
      "574 0.639923632144928\n",
      "575 0.63965904712677\n",
      "576 0.6393945217132568\n",
      "577 0.6391302943229675\n",
      "578 0.6388661861419678\n",
      "579 0.6386023163795471\n",
      "580 0.6383386850357056\n",
      "581 0.6380751729011536\n",
      "582 0.6378118395805359\n",
      "583 0.6375486254692078\n",
      "584 0.6372857093811035\n",
      "585 0.6370229125022888\n",
      "586 0.6367602944374084\n",
      "587 0.6364978551864624\n",
      "588 0.6362356543540955\n",
      "589 0.6359735727310181\n",
      "590 0.6357117295265198\n",
      "591 0.635450005531311\n",
      "592 0.6351885795593262\n",
      "593 0.6349271535873413\n",
      "594 0.6346660852432251\n",
      "595 0.6344051361083984\n",
      "596 0.6341443061828613\n",
      "597 0.6338836550712585\n",
      "598 0.6336233019828796\n",
      "599 0.6333630681037903\n",
      "600 0.6331030130386353\n",
      "601 0.6328431963920593\n",
      "602 0.632583498954773\n",
      "603 0.6323239207267761\n",
      "604 0.6320646405220032\n",
      "605 0.6318054795265198\n",
      "606 0.6315464377403259\n",
      "607 0.6312877535820007\n",
      "608 0.6310290694236755\n",
      "609 0.6307706236839294\n",
      "610 0.6305124759674072\n",
      "611 0.6302544474601746\n",
      "612 0.6299965381622314\n",
      "613 0.6297388076782227\n",
      "614 0.6294812560081482\n",
      "615 0.6292240023612976\n",
      "616 0.628966748714447\n",
      "617 0.6287097930908203\n",
      "618 0.6284529566764832\n",
      "619 0.6281964182853699\n",
      "620 0.6279399394989014\n",
      "621 0.627683699131012\n",
      "622 0.6274276375770569\n",
      "623 0.6271717548370361\n",
      "624 0.6269159913063049\n",
      "625 0.6266603469848633\n",
      "626 0.6264050006866455\n",
      "627 0.6261498928070068\n",
      "628 0.6258949041366577\n",
      "629 0.6256400346755981\n",
      "630 0.6253853440284729\n",
      "631 0.6251308917999268\n",
      "632 0.6248766183853149\n",
      "633 0.6246224641799927\n",
      "634 0.62436842918396\n",
      "635 0.6241146922111511\n",
      "636 0.6238610744476318\n",
      "637 0.6236076951026917\n",
      "638 0.6233543753623962\n",
      "639 0.6231013536453247\n",
      "640 0.6228485107421875\n",
      "641 0.6225957870483398\n",
      "642 0.6223432421684265\n",
      "643 0.6220909357070923\n",
      "644 0.6218387484550476\n",
      "645 0.6215866804122925\n",
      "646 0.6213348507881165\n",
      "647 0.6210831999778748\n",
      "648 0.6208317279815674\n",
      "649 0.6205804347991943\n",
      "650 0.6203293204307556\n",
      "651 0.6200783848762512\n",
      "652 0.6198275685310364\n",
      "653 0.6195769906044006\n",
      "654 0.6193265914916992\n",
      "655 0.6190763711929321\n",
      "656 0.6188262701034546\n",
      "657 0.6185764670372009\n",
      "658 0.6183266043663025\n",
      "659 0.6180770993232727\n",
      "660 0.6178277730941772\n",
      "661 0.6175785064697266\n",
      "662 0.6173295378684998\n",
      "663 0.6170806884765625\n",
      "664 0.6168320178985596\n",
      "665 0.6165834665298462\n",
      "666 0.6163351535797119\n",
      "667 0.616087019443512\n",
      "668 0.6158390045166016\n",
      "669 0.6155911684036255\n",
      "670 0.6153435707092285\n",
      "671 0.6150961518287659\n",
      "672 0.6148488521575928\n",
      "673 0.614601731300354\n",
      "674 0.6143547892570496\n",
      "675 0.6141080260276794\n",
      "676 0.6138613820075989\n",
      "677 0.6136149764060974\n",
      "678 0.613368809223175\n",
      "679 0.6131226420402527\n",
      "680 0.6128767132759094\n",
      "681 0.61263108253479\n",
      "682 0.6123855113983154\n",
      "683 0.6121400594711304\n",
      "684 0.6118948459625244\n",
      "685 0.6116498112678528\n",
      "686 0.6114048957824707\n",
      "687 0.6111602187156677\n",
      "688 0.6109156608581543\n",
      "689 0.61067134141922\n",
      "690 0.6104270815849304\n",
      "691 0.6101831197738647\n",
      "692 0.6099392175674438\n",
      "693 0.6096956133842468\n",
      "694 0.6094520688056946\n",
      "695 0.6092088222503662\n",
      "696 0.6089655756950378\n",
      "697 0.6087225675582886\n",
      "698 0.6084797382354736\n",
      "699 0.6082370281219482\n",
      "700 0.6079946160316467\n",
      "701 0.60775226354599\n",
      "702 0.6075100898742676\n",
      "703 0.6072681546211243\n",
      "704 0.6070263385772705\n",
      "705 0.6067847609519958\n",
      "706 0.606543242931366\n",
      "707 0.6063019037246704\n",
      "708 0.6060607433319092\n",
      "709 0.6058197617530823\n",
      "710 0.6055790781974792\n",
      "711 0.6053383946418762\n",
      "712 0.6050978899002075\n",
      "713 0.6048576235771179\n",
      "714 0.6046175956726074\n",
      "715 0.6043776273727417\n",
      "716 0.6041377782821655\n",
      "717 0.6038981676101685\n",
      "718 0.6036587357521057\n",
      "719 0.6034194231033325\n",
      "720 0.6031802892684937\n",
      "721 0.6029413938522339\n",
      "722 0.6027026176452637\n",
      "723 0.602463960647583\n",
      "724 0.6022254824638367\n",
      "725 0.6019872426986694\n",
      "726 0.6017491817474365\n",
      "727 0.6015111804008484\n",
      "728 0.6012734174728394\n",
      "729 0.6010358333587646\n",
      "730 0.6007983088493347\n",
      "731 0.6005610227584839\n",
      "732 0.6003239154815674\n",
      "733 0.60008704662323\n",
      "734 0.5998502373695374\n",
      "735 0.599613606929779\n",
      "736 0.5993770956993103\n",
      "737 0.5991408228874207\n",
      "738 0.5989046692848206\n",
      "739 0.5986688137054443\n",
      "740 0.5984330177307129\n",
      "741 0.598197340965271\n",
      "742 0.5979619026184082\n",
      "743 0.5977266430854797\n",
      "744 0.5974915027618408\n",
      "745 0.5972565412521362\n",
      "746 0.5970216989517212\n",
      "747 0.5967870950698853\n",
      "748 0.5965526103973389\n",
      "749 0.596318244934082\n",
      "750 0.5960841178894043\n",
      "751 0.5958501696586609\n",
      "752 0.595616340637207\n",
      "753 0.5953826904296875\n",
      "754 0.5951492190361023\n",
      "755 0.5949159264564514\n",
      "756 0.5946827530860901\n",
      "757 0.5944496989250183\n",
      "758 0.5942169427871704\n",
      "759 0.5939842462539673\n",
      "760 0.5937516689300537\n",
      "761 0.5935193300247192\n",
      "762 0.5932871699333191\n",
      "763 0.5930551290512085\n",
      "764 0.592823326587677\n",
      "765 0.5925915837287903\n",
      "766 0.5923600792884827\n",
      "767 0.5921286940574646\n",
      "768 0.5918974876403809\n",
      "769 0.5916664600372314\n",
      "770 0.5914354920387268\n",
      "771 0.5912047624588013\n",
      "772 0.5909742712974548\n",
      "773 0.5907438397407532\n",
      "774 0.5905136466026306\n",
      "775 0.5902835130691528\n",
      "776 0.5900535583496094\n",
      "777 0.589823842048645\n",
      "778 0.589594304561615\n",
      "779 0.5893648862838745\n",
      "780 0.5891355276107788\n",
      "781 0.5889064073562622\n",
      "782 0.5886775255203247\n",
      "783 0.588448703289032\n",
      "784 0.5882200002670288\n",
      "785 0.5879915952682495\n",
      "786 0.587763249874115\n",
      "787 0.58753502368927\n",
      "788 0.5873070359230042\n",
      "789 0.5870792865753174\n",
      "790 0.5868515968322754\n",
      "791 0.586624026298523\n",
      "792 0.5863966941833496\n",
      "793 0.5861694812774658\n",
      "794 0.5859425067901611\n",
      "795 0.5857155919075012\n",
      "796 0.5854889154434204\n",
      "797 0.5852622985839844\n",
      "798 0.5850359201431274\n",
      "799 0.5848096013069153\n",
      "800 0.5845835208892822\n",
      "801 0.5843576788902283\n",
      "802 0.5841318368911743\n",
      "803 0.5839062929153442\n",
      "804 0.5836808681488037\n",
      "805 0.583455502986908\n",
      "806 0.5832303166389465\n",
      "807 0.5830053687095642\n",
      "808 0.5827805995941162\n",
      "809 0.582555890083313\n",
      "810 0.5823312997817993\n",
      "811 0.5821070075035095\n",
      "812 0.5818828344345093\n",
      "813 0.5816587209701538\n",
      "814 0.5814348459243774\n",
      "815 0.5812111496925354\n",
      "816 0.5809875726699829\n",
      "817 0.5807641744613647\n",
      "818 0.5805409550666809\n",
      "819 0.5803177952766418\n",
      "820 0.5800948739051819\n",
      "821 0.5798720717430115\n",
      "822 0.5796494483947754\n",
      "823 0.5794270038604736\n",
      "824 0.5792046189308167\n",
      "825 0.5789825916290283\n",
      "826 0.5787604451179504\n",
      "827 0.5785385370254517\n",
      "828 0.578316867351532\n",
      "829 0.5780953764915466\n",
      "830 0.5778740048408508\n",
      "831 0.5776527523994446\n",
      "832 0.5774316787719727\n",
      "833 0.5772107243537903\n",
      "834 0.5769899487495422\n",
      "835 0.576769232749939\n",
      "836 0.5765488147735596\n",
      "837 0.5763285756111145\n",
      "838 0.5761083364486694\n",
      "839 0.5758883357048035\n",
      "840 0.5756685733795166\n",
      "841 0.5754488706588745\n",
      "842 0.575229287147522\n",
      "843 0.5750099420547485\n",
      "844 0.5747906565666199\n",
      "845 0.5745715498924255\n",
      "846 0.5743526816368103\n",
      "847 0.5741338729858398\n",
      "848 0.5739152431488037\n",
      "849 0.5736967921257019\n",
      "850 0.5734785199165344\n",
      "851 0.5732602477073669\n",
      "852 0.5730423331260681\n",
      "853 0.5728244781494141\n",
      "854 0.5726066827774048\n",
      "855 0.5723892450332642\n",
      "856 0.5721718072891235\n",
      "857 0.5719545483589172\n",
      "858 0.5717374086380005\n",
      "859 0.5715204477310181\n",
      "860 0.5713037252426147\n",
      "861 0.5710870027542114\n",
      "862 0.5708704590797424\n",
      "863 0.5706542134284973\n",
      "864 0.570438027381897\n",
      "865 0.5702219009399414\n",
      "866 0.5700060725212097\n",
      "867 0.5697903037071228\n",
      "868 0.5695747137069702\n",
      "869 0.5693593621253967\n",
      "870 0.5691439509391785\n",
      "871 0.5689288973808289\n",
      "872 0.5687138438224792\n",
      "873 0.5684990286827087\n",
      "874 0.5682843923568726\n",
      "875 0.5680698156356812\n",
      "876 0.5678554177284241\n",
      "877 0.5676411986351013\n",
      "878 0.5674271583557129\n",
      "879 0.5672131776809692\n",
      "880 0.5669994354248047\n",
      "881 0.5667857527732849\n",
      "882 0.5665722489356995\n",
      "883 0.5663589239120483\n",
      "884 0.566145658493042\n",
      "885 0.5659326314926147\n",
      "886 0.565719723701477\n",
      "887 0.5655069947242737\n",
      "888 0.5652943849563599\n",
      "889 0.5650819540023804\n",
      "890 0.5648697018623352\n",
      "891 0.5646575689315796\n",
      "892 0.5644454956054688\n",
      "893 0.564233660697937\n",
      "894 0.5640219449996948\n",
      "895 0.5638103485107422\n",
      "896 0.5635989308357239\n",
      "897 0.5633877515792847\n",
      "898 0.5631765723228455\n",
      "899 0.5629655718803406\n",
      "900 0.56275475025177\n",
      "901 0.562544047832489\n",
      "902 0.5623335242271423\n",
      "903 0.5621231198310852\n",
      "904 0.5619128942489624\n",
      "905 0.5617027878761292\n",
      "906 0.561492919921875\n",
      "907 0.5612830519676208\n",
      "908 0.561073362827301\n",
      "909 0.5608639121055603\n",
      "910 0.5606545209884644\n",
      "911 0.5604453086853027\n",
      "912 0.5602363348007202\n",
      "913 0.5600273013114929\n",
      "914 0.5598185658454895\n",
      "915 0.5596099495887756\n",
      "916 0.5594014525413513\n",
      "917 0.5591931343078613\n",
      "918 0.5589848756790161\n",
      "919 0.55877685546875\n",
      "920 0.5585689544677734\n",
      "921 0.5583612322807312\n",
      "922 0.558153510093689\n",
      "923 0.5579460859298706\n",
      "924 0.557738721370697\n",
      "925 0.5575315356254578\n",
      "926 0.5573245286941528\n",
      "927 0.5571175813674927\n",
      "928 0.5569108724594116\n",
      "929 0.5567042231559753\n",
      "930 0.5564978122711182\n",
      "931 0.5562915205955505\n",
      "932 0.5560852885246277\n",
      "933 0.5558792948722839\n",
      "934 0.5556734204292297\n",
      "935 0.5554676055908203\n",
      "936 0.55526202917099\n",
      "937 0.5550565719604492\n",
      "938 0.554851233959198\n",
      "939 0.5546460747718811\n",
      "940 0.5544410347938538\n",
      "941 0.554236114025116\n",
      "942 0.5540314316749573\n",
      "943 0.5538267493247986\n",
      "944 0.553622305393219\n",
      "945 0.5534180402755737\n",
      "946 0.5532138347625732\n",
      "947 0.5530097484588623\n",
      "948 0.5528059005737305\n",
      "949 0.5526021718978882\n",
      "950 0.5523985028266907\n",
      "951 0.5521950125694275\n",
      "952 0.5519917607307434\n",
      "953 0.5517885088920593\n",
      "954 0.5515854954719543\n",
      "955 0.5513826012611389\n",
      "956 0.551179826259613\n",
      "957 0.5509771704673767\n",
      "958 0.5507747530937195\n",
      "959 0.5505723357200623\n",
      "960 0.5503701567649841\n",
      "961 0.5501680970191956\n",
      "962 0.5499662160873413\n",
      "963 0.5497643947601318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964 0.5495628118515015\n",
      "965 0.5493612885475159\n",
      "966 0.5491598844528198\n",
      "967 0.5489586591720581\n",
      "968 0.5487576127052307\n",
      "969 0.5485566854476929\n",
      "970 0.5483558773994446\n",
      "971 0.5481552481651306\n",
      "972 0.5479546785354614\n",
      "973 0.5477543473243713\n",
      "974 0.5475540161132812\n",
      "975 0.5473539233207703\n",
      "976 0.5471539497375488\n",
      "977 0.5469542145729065\n",
      "978 0.5467544794082642\n",
      "979 0.5465549230575562\n",
      "980 0.5463554263114929\n",
      "981 0.5461562275886536\n",
      "982 0.545957088470459\n",
      "983 0.5457581281661987\n",
      "984 0.5455592274665833\n",
      "985 0.5453605651855469\n",
      "986 0.5451619029045105\n",
      "987 0.5449634194374084\n",
      "988 0.5447651743888855\n",
      "989 0.5445669889450073\n",
      "990 0.5443689823150635\n",
      "991 0.5441710352897644\n",
      "992 0.5439733266830444\n",
      "993 0.5437756776809692\n",
      "994 0.5435782074928284\n",
      "995 0.5433807969093323\n",
      "996 0.5431836247444153\n",
      "997 0.5429865717887878\n",
      "998 0.54278963804245\n",
      "999 0.5425928235054016\n",
      "predict 1 hour  1.0 False\n",
      "predict 7 hours 7.0 True\n"
     ]
    }
   ],
   "source": [
    "# 6. logistic regression\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.\n",
    "        \"\"\"\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[1.0]]))\n",
    "print(\"predict 1 hour \", 1.0, model(hour_var).data[0][0] > 0.5)\n",
    "hour_var = Variable(torch.Tensor([[7.0]]))\n",
    "print(\"predict 7 hours\", 7.0, model(hour_var).data[0][0] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n",
      "0 0.7145146131515503\n",
      "1 0.7145146131515503\n",
      "2 0.7145146131515503\n",
      "3 0.7145146131515503\n",
      "4 0.7145146131515503\n",
      "5 0.7145146131515503\n",
      "6 0.7145146131515503\n",
      "7 0.7145146131515503\n",
      "8 0.7145146131515503\n",
      "9 0.7145146131515503\n",
      "10 0.7145146131515503\n",
      "11 0.7145146131515503\n",
      "12 0.7145146131515503\n",
      "13 0.7145146131515503\n",
      "14 0.7145146131515503\n",
      "15 0.7145146131515503\n",
      "16 0.7145146131515503\n",
      "17 0.7145146131515503\n",
      "18 0.7145146131515503\n",
      "19 0.7145146131515503\n",
      "20 0.7145146131515503\n",
      "21 0.7145146131515503\n",
      "22 0.7145146131515503\n",
      "23 0.7145146131515503\n",
      "24 0.7145146131515503\n",
      "25 0.7145146131515503\n",
      "26 0.7145146131515503\n",
      "27 0.7145146131515503\n",
      "28 0.7145146131515503\n",
      "29 0.7145146131515503\n",
      "30 0.7145146131515503\n",
      "31 0.7145146131515503\n",
      "32 0.7145146131515503\n",
      "33 0.7145146131515503\n",
      "34 0.7145146131515503\n",
      "35 0.7145146131515503\n",
      "36 0.7145146131515503\n",
      "37 0.7145146131515503\n",
      "38 0.7145146131515503\n",
      "39 0.7145146131515503\n",
      "40 0.7145146131515503\n",
      "41 0.7145146131515503\n",
      "42 0.7145146131515503\n",
      "43 0.7145146131515503\n",
      "44 0.7145146131515503\n",
      "45 0.7145146131515503\n",
      "46 0.7145146131515503\n",
      "47 0.7145146131515503\n",
      "48 0.7145146131515503\n",
      "49 0.7145146131515503\n",
      "50 0.7145146131515503\n",
      "51 0.7145146131515503\n",
      "52 0.7145146131515503\n",
      "53 0.7145146131515503\n",
      "54 0.7145146131515503\n",
      "55 0.7145146131515503\n",
      "56 0.7145146131515503\n",
      "57 0.7145146131515503\n",
      "58 0.7145146131515503\n",
      "59 0.7145146131515503\n",
      "60 0.7145146131515503\n",
      "61 0.7145146131515503\n",
      "62 0.7145146131515503\n",
      "63 0.7145146131515503\n",
      "64 0.7145146131515503\n",
      "65 0.7145146131515503\n",
      "66 0.7145146131515503\n",
      "67 0.7145146131515503\n",
      "68 0.7145146131515503\n",
      "69 0.7145146131515503\n",
      "70 0.7145146131515503\n",
      "71 0.7145146131515503\n",
      "72 0.7145146131515503\n",
      "73 0.7145146131515503\n",
      "74 0.7145146131515503\n",
      "75 0.7145146131515503\n",
      "76 0.7145146131515503\n",
      "77 0.7145146131515503\n",
      "78 0.7145146131515503\n",
      "79 0.7145146131515503\n",
      "80 0.7145146131515503\n",
      "81 0.7145146131515503\n",
      "82 0.7145146131515503\n",
      "83 0.7145146131515503\n",
      "84 0.7145146131515503\n",
      "85 0.7145146131515503\n",
      "86 0.7145146131515503\n",
      "87 0.7145146131515503\n",
      "88 0.7145146131515503\n",
      "89 0.7145146131515503\n",
      "90 0.7145146131515503\n",
      "91 0.7145146131515503\n",
      "92 0.7145146131515503\n",
      "93 0.7145146131515503\n",
      "94 0.7145146131515503\n",
      "95 0.7145146131515503\n",
      "96 0.7145146131515503\n",
      "97 0.7145146131515503\n",
      "98 0.7145146131515503\n",
      "99 0.7145146131515503\n"
     ]
    }
   ],
   "source": [
    "# 7. diabets logistic\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "\n",
    "print(x_data.data.shape)\n",
    "print(y_data.data.shape)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 23 inputs \n",
      "-0.7647  0.0050  0.0492 -0.5354  0.0000 -0.1148 -0.7523  0.0000\n",
      "-0.4118  0.5578  0.3770 -0.1111  0.2884  0.1535 -0.5380 -0.5667\n",
      "-0.4118  0.3668  0.3770 -0.1717 -0.7920  0.0432 -0.8224 -0.5333\n",
      "-0.2941  0.6683  0.2131  0.0000  0.0000 -0.2072 -0.8070  0.5000\n",
      "-0.4118  0.3769  0.7705  0.0000  0.0000  0.4545 -0.8728 -0.4667\n",
      "-0.7647  0.2563 -0.0164 -0.5960 -0.6690  0.0075 -0.9915 -0.6667\n",
      "-0.7647  0.2161  0.1475 -0.3535 -0.7754  0.1654 -0.3100 -0.9333\n",
      "-0.8824  0.8191  0.0492 -0.3939 -0.5745  0.0164 -0.7865 -0.4333\n",
      "-0.8824 -0.0955  0.0164 -0.6364 -0.8605 -0.2519  0.0162 -0.8667\n",
      "-0.8824 -0.2663 -0.1803 -0.7980  0.0000 -0.3145 -0.8548  0.0000\n",
      "-0.0588  0.1859  0.1803 -0.6162  0.0000 -0.3115  0.1939 -0.1667\n",
      "-0.1765  0.4774  0.2459  0.0000  0.0000  0.1744 -0.8471 -0.2667\n",
      "-0.8824  0.0754  0.1803 -0.3939 -0.8061 -0.0820 -0.3655 -0.9000\n",
      "-0.5294 -0.0050  0.1148 -0.2323  0.0000 -0.0224 -0.9428 -0.6000\n",
      " 0.1765  0.7990  0.1475  0.0000  0.0000  0.0462 -0.8958 -0.4667\n",
      "-0.7647  0.1457  0.1148 -0.5556  0.0000 -0.1446 -0.9880 -0.8667\n",
      " 0.0000  0.3869  0.0000  0.0000  0.0000  0.0820 -0.2699 -0.8667\n",
      " 0.0000  0.1357  0.3115 -0.6768  0.0000 -0.0760 -0.3202  0.0000\n",
      "-0.4118 -0.0050 -0.1148 -0.4343 -0.8038  0.0134 -0.6405 -0.7000\n",
      "-0.1765  0.5276  0.4426 -0.1111  0.0000  0.4903 -0.7788 -0.5000\n",
      "-0.6471  0.1156  0.0164  0.0000  0.0000 -0.3264 -0.9453  0.0000\n",
      "-0.8824 -0.1256  0.2787 -0.4545 -0.9244  0.0313 -0.9804 -0.9667\n",
      "-0.0588  0.4372  0.0820  0.0000  0.0000  0.0402 -0.9564 -0.3333\n",
      "[torch.FloatTensor of size 23x8]\n",
      " labels \n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    1\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "[torch.FloatTensor of size 23x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8-1. dataset loader\n",
    "\n",
    "\n",
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Run your training process\n",
    "print(epoch, i, \"inputs\", inputs.data, \"labels\", labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.7089021801948547\n",
      "0 1 0.6981281638145447\n",
      "0 2 0.6980457901954651\n",
      "0 3 0.7024941444396973\n",
      "0 4 0.7026824355125427\n",
      "0 5 0.698099672794342\n",
      "0 6 0.6950750946998596\n",
      "0 7 0.7027164697647095\n",
      "0 8 0.698056161403656\n",
      "0 9 0.6979888081550598\n",
      "0 10 0.7071740627288818\n",
      "0 11 0.6979287266731262\n",
      "0 12 0.701103925704956\n",
      "0 13 0.6965833306312561\n",
      "0 14 0.6994715929031372\n",
      "0 15 0.7025038599967957\n",
      "0 16 0.7025662064552307\n",
      "0 17 0.7086721062660217\n",
      "0 18 0.6996685266494751\n",
      "0 19 0.696401834487915\n",
      "0 20 0.7026519775390625\n",
      "0 21 0.7011197805404663\n",
      "0 22 0.7027924060821533\n",
      "0 23 0.7031750679016113\n",
      "1 0 0.7025793194770813\n",
      "1 1 0.7010924220085144\n",
      "1 2 0.6965274214744568\n",
      "1 3 0.698012113571167\n",
      "1 4 0.6981269717216492\n",
      "1 5 0.6979406476020813\n",
      "1 6 0.699626624584198\n",
      "1 7 0.7025923132896423\n",
      "1 8 0.707346498966217\n",
      "1 9 0.7010943293571472\n",
      "1 10 0.6919493079185486\n",
      "1 11 0.7043165564537048\n",
      "1 12 0.6996725797653198\n",
      "1 13 0.6996256113052368\n",
      "1 14 0.7117915749549866\n",
      "1 15 0.7084751725196838\n",
      "1 16 0.6981825828552246\n",
      "1 17 0.7040506601333618\n",
      "1 18 0.7025684118270874\n",
      "1 19 0.6949701905250549\n",
      "1 20 0.6996546983718872\n",
      "1 21 0.7040787935256958\n",
      "1 22 0.7042217254638672\n",
      "1 23 0.6925058364868164\n"
     ]
    }
   ],
   "source": [
    "# 8-2. dataset load logistic\n",
    "\n",
    "# References\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
    "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Diabetes dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('./data/diabetes.csv.gz',\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.data[0])\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 =  0.356674943939\n",
      "loss2 =  2.30258509299\n",
      "PyTorch Loss1 =  \n",
      " 0.4170\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "PyTorch Loss2= \n",
      " 1.8406\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Y_pred1= \n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Y_pred2= \n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Batch Loss1 =  \n",
      " 0.4966\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Batch Loss2= \n",
      " 1.2389\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9-1 softmax loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Cross entropy example\n",
    "import numpy as np\n",
    "# One hot\n",
    "# 0: 1 0 0\n",
    "# 1: 0 1 0\n",
    "# 2: 0 0 1\n",
    "Y = np.array([1, 0, 0])\n",
    "\n",
    "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
    "print(\"loss1 = \", np.sum(-Y * np.log(Y_pred1)))\n",
    "print(\"loss2 = \", np.sum(-Y * np.log(Y_pred2)))\n",
    "\n",
    "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([0]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 1 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\n",
    "Y_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"PyTorch Loss1 = \", l1.data, \"\\nPyTorch Loss2=\", l2.data)\n",
    "\n",
    "print(\"Y_pred1=\", torch.max(Y_pred1.data, 1)[1])\n",
    "print(\"Y_pred2=\", torch.max(Y_pred2.data, 1)[1])\n",
    "\n",
    "# target is of size nBatch\n",
    "# each element in target has to have 0 <= value < nClasses (0-2)\n",
    "# Input is class, not one-hot\n",
    "Y = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n",
    "\n",
    "# input is of size nBatch x nClasses = 2 x 4\n",
    "# Y_pred are logits (not softmax)\n",
    "Y_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.9],\n",
    "                                 [1.1, 0.1, 0.2],\n",
    "                                 [0.2, 2.1, 0.1]]))\n",
    "\n",
    "\n",
    "Y_pred2 = Variable(torch.Tensor([[0.8, 0.2, 0.3],\n",
    "                                 [0.2, 0.3, 0.5],\n",
    "                                 [0.2, 0.2, 0.5]]))\n",
    "\n",
    "l1 = loss(Y_pred1, Y)\n",
    "l2 = loss(Y_pred2, Y)\n",
    "\n",
    "print(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316142\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.307732\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.298258\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.311986\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.301505\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.300578\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.302939\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.302920\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.307575\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.297686\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.301886\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.298780\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.299634\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.302643\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.300937\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.302305\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.301867\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.295091\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.297110\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.300047\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.305119\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.297011\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.293114\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.292451\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.285762\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.296643\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.295819\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.302936\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.294560\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.294358\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.290041\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.296646\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.291478\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.298093\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.289207\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.284445\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.285653\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.293104\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.297328\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.295657\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.289929\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.287635\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.281645\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.289831\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.278047\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.286247\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.284566\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.289520\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.272954\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.275632\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.279456\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.268826\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.276485\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.265510\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.281189\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.278357\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.272182\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.276339\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.268478\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.267752\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.258087\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.280648\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.255473\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.247458\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.230417\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.246101\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.238985\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.236419\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.231824\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.218901\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.216278\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.212264\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.180685\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.180639\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.205603\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.187361\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.185755\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.131454\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.120337\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.131707\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.110967\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 2.028720\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 2.086318\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 2.085803\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.997410\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.920379\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.974593\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.948734\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.911447\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.843583\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.781655\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.777079\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.838438\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.711501\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.621331\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.525861\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.523129\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.412825\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.354263\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.237411\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.394891\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.070708\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.221432\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.123386\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.090582\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.969627\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.025353\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 1.010439\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.943440\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.796464\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.859125\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.942755\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.684134\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.756517\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.839077\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.624089\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.943920\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.824944\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.740959\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.962691\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.766888\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.674859\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.634869\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.658264\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.558925\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.609899\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.739333\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.624554\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.593575\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.579570\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.604400\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.604251\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.526500\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.822112\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.627664\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.588514\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.559553\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.582233\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.607300\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.741127\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.720885\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.813703\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.826518\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.571470\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.605170\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.756391\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.387226\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.459914\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.539970\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.465338\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.508871\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.472050\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.337198\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.508019\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.843730\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.392305\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.314996\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.399243\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.492157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.478265\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.643858\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.417558\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.676498\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.585639\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.430896\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.551726\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.494937\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.490878\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.502571\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.496751\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.483037\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.639158\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.333208\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.522753\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.652004\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.341873\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.479150\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.546926\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.534993\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.311214\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.664460\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.598767\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.338100\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.466476\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.439512\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.707748\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.387878\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.537639\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.520743\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.484488\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.616833\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.305654\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.536818\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.463296\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.346767\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.408413\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.334834\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.453568\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.272777\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.412273\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.507549\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.391061\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.239512\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.315383\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.223117\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.267664\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.191508\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.371269\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.301934\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.314952\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.449835\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.318466\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.365915\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.419785\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.389602\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.454937\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.347077\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.325167\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.375329\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.311586\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.184237\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.516461\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.558873\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.380430\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.499295\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.239525\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.389203\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.202611\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.319025\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.352674\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.271076\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.397099\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.310507\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.209104\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.793062\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.415802\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.798803\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.372219\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.220593\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.245202\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.482982\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.349281\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.344206\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.532642\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.250936\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.304232\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.411693\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.303119\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.487424\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.214792\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.243172\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.460221\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.421049\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.389177\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.390998\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.405840\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.422796\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.238605\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.522558\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.361904\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.526037\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.201223\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.344283\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.200442\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.377492\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.302902\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.559987\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.473917\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.273133\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.307812\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.119904\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.252551\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.174850\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.301222\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.296425\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.193823\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.469662\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.335792\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.554770\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.345461\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.284373\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.410767\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.261669\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.376395\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.337326\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.346005\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.358365\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.325227\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.260762\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.160259\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.246322\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.218369\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.285914\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.155590\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.306894\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.198977\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.245295\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.279588\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.139471\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.247057\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.277136\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.447091\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.254373\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.353909\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.258641\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.333210\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.210896\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.415493\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.256806\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.271092\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.250637\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.346116\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.343432\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.331945\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.147769\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.146155\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.199204\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.478091\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.217937\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.212196\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.256443\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.222160\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.425229\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.093261\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.138107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.404980\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.191013\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.203193\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.323788\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.115550\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.201605\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.212655\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.291999\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.241101\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.265011\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.348172\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.178782\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.305844\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.423226\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.290925\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.135309\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.429134\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.200973\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.191796\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.422330\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.120875\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.201808\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.106022\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.332991\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.371289\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.280200\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.237741\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.107733\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.446808\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.257135\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.120068\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.252395\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.167300\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.224298\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.359217\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.247024\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.147329\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.215048\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.277910\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.346021\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.201436\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.324396\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.275393\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.470458\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.162911\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.087412\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.311667\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.132420\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.298458\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.549558\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.281391\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.277980\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.177924\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.148314\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.228963\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.413603\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.328545\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.240372\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.185625\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.435458\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.244994\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.357363\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.380925\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.163656\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.164693\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.285811\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.313537\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.281028\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.128899\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.283573\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.146843\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.071765\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.211174\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.111677\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.295297\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.197964\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.154584\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.193016\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.263196\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.230796\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.166942\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.350496\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.256554\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.217549\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.482980\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.175341\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.242940\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.229418\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.259116\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.269588\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.247957\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.106910\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.129512\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.144682\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.146664\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.169760\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.070372\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.135849\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.071967\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.163346\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.156125\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.107631\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.079517\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.135770\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.132499\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.205160\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.157132\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.171367\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.045349\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.243299\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.118751\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.191202\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.226138\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.132286\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.125554\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.177195\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.212727\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.344618\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.144094\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.265247\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.279139\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.301467\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.191803\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.137209\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.084105\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.241328\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.220448\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.145686\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.085069\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.126659\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.137556\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.135755\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.154600\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.393970\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.103014\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.242545\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.129882\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.156116\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.117115\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.140678\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.180869\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.119036\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.177206\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.227540\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.061187\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.144379\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.113150\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.060358\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.065475\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.211083\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.205740\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.321668\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.094564\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.235643\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.408097\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.096654\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.128077\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.209987\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.117742\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.073209\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.202314\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.138525\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.152671\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.145867\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.109274\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.092345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.343108\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.262056\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.033162\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.095810\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.137833\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.109428\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.350237\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.143271\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.125105\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.136069\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.253268\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.310050\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.117398\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.221723\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.299403\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.132404\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.079283\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.102473\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.129845\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.176802\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.093641\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.127828\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.106208\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.218775\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.137504\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.171810\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.228788\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.317208\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.253296\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.233975\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.114566\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.102658\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.275042\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.115555\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.166100\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.098334\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.163126\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.102848\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.398635\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.076203\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.141804\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.120041\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.193317\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.177224\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.154056\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.250199\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.093634\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.070925\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.057847\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.201755\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.074306\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.171930\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.093169\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.111841\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.177709\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.051780\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.173549\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.174196\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.109315\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.410460\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.264556\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.371142\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.212814\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.139434\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.334684\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.053296\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.214975\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.099036\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.124816\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.208347\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.061303\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.088941\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.205749\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.140506\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.107786\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.033897\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.050296\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.094047\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.167973\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.125441\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.177433\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.120893\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.074385\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.170453\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.074814\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.080548\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.137153\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.074525\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.109859\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.149375\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.177037\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.210686\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.105003\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.057007\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.041586\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.088968\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.098007\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.151479\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.097690\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.226644\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.145404\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.090899\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.085468\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.117247\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.285706\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.099621\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.272596\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.175226\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.102299\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.070135\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.063371\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.116122\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.299439\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.128933\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.069237\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.191723\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.084519\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.072749\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.069518\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.032887\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.035570\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.101770\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.133366\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.290285\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.132493\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.146236\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.083575\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.095334\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.130582\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.071181\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.124897\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.180207\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.096501\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.098191\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.207398\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.160078\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.150732\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.237460\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.230480\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.062206\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.126392\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.050721\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.094684\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.087825\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.137318\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.064516\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.202000\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.060015\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.123590\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.128352\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.071413\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.108700\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.034214\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.027545\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.186334\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.132531\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.133014\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.089404\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.123334\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.093243\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.071136\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.096009\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.071942\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.051135\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.078096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.049929\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.035640\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.100469\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.087465\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.120384\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.164407\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.125273\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.143170\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.065821\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.064350\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.057930\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.097406\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.073080\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.110410\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.108464\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.167878\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.279291\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.158064\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.135706\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.110842\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.072195\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.088550\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.064881\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.207580\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.177154\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.187562\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.115446\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.378058\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.068055\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.105480\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.107974\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.130248\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.133715\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.123804\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.146760\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.068933\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.052545\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.156269\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.047349\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.179575\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.043025\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.068041\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.236064\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.241897\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.049614\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.221221\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.172745\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.191222\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.036253\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.073618\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.072335\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.066147\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.054449\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.111559\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.201388\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.162384\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.244889\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.126393\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.086532\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.086746\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.065950\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.059940\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.095900\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.071710\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.177627\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.116071\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.109541\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.054610\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.027557\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.172690\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.096820\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.123812\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.161763\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.128608\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.073881\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.100462\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.089029\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.064902\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.167610\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.135756\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.209618\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.145049\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.227246\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.072360\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.024180\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.087428\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.032621\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.177737\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.055069\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.067204\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.186016\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.136414\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.161697\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.126796\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.024675\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.133277\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.082635\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.107608\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.185194\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.095989\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.053144\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.078230\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.114720\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.091381\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.035526\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.105478\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.255911\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.158961\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.015369\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.081712\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.063226\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.148308\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.040117\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.017375\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.061364\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.095035\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.070405\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.113306\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.033222\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.050836\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.087859\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.050441\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.123418\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.078324\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.027919\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.062020\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.021733\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.100521\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.016416\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.081364\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.029869\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.060076\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.216962\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.144576\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.102797\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.100360\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.100593\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.075767\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.064024\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.085102\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.133817\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.180438\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.023935\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.073906\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.220152\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.090648\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.025587\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.063321\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.119174\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.093779\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.175827\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.207205\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.061470\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.192713\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.044143\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.044731\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.022326\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.045996\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.056409\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.073123\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.092476\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.049090\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.039499\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.088463\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.113159\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.127101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.032413\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.145174\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.062723\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.090861\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.102598\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.110116\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.032654\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.057910\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.043416\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.013715\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.028102\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.063084\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.075150\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.078966\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.090524\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.047475\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.101732\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.092181\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.222208\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.024364\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.055000\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.060943\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.094637\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.011858\n",
      "\n",
      "Test set: Average loss: 0.0016, Accuracy: 9685/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9-2 softmax mnist\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/venv/py3/lib/python3.5/site-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.319500\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.292898\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.289500\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.285129\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.266376\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.243713\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.234079\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.207976\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.153191\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.154187\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.068593\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.973876\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.819871\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.472609\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.427214\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.271433\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.040491\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.963183\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.578554\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.716940\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.616458\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.680554\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.671474\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.549853\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.550331\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.568847\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.775745\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.713361\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.456040\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.468118\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.366575\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.411288\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.461424\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.319005\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.366806\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.472161\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.411977\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.335832\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.268582\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.194591\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.234607\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.243424\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.231890\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.213154\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.312843\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.345121\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.353682\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.174929\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.283369\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.250054\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.149553\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.499040\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.404402\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.368118\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.239730\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.220660\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.277580\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.243382\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.306894\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.232832\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.245371\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.189789\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.180816\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.285042\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.400420\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.235899\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.385471\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.338033\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.271979\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.264078\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.197347\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.083029\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.289959\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.148299\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.329583\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.297957\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.160340\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.259238\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.254287\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.245344\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.207933\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.412107\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.381221\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.220475\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.147174\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.340176\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.242217\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.163514\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.147610\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.221505\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.264034\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.315813\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.213931\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.282161\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.183387\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.163482\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.271329\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.151835\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.281422\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.175198\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.224516\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.251557\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.151162\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.243011\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.167748\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.149320\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.144181\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.286863\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.251616\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.248016\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.150746\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.236572\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.208494\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.197652\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.107299\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.153498\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.081115\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.180961\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.091930\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.045127\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.218829\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.197604\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.138613\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.108098\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.108994\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.167014\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.159385\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.157705\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.228376\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.094163\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.109521\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.101697\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.075986\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.187923\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.230340\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.272848\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.094638\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.188555\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.381812\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.126586\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.141601\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.053264\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.160830\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.157339\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.080134\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.126318\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.149316\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.234589\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.240630\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.133914\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.144887\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.124998\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.056365\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.075997\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.101578\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.080933\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.167279\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.071709\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.095112\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.162824\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.074361\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.173251\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.027242\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.033174\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.061316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.099933\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.075369\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.169033\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.182880\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.109051\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.044440\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.167501\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.114766\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.134146\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.074208\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.182562\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.063717\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.159961\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.078906\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.096927\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.153420\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.194011\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.093960\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.068611\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.066739\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.145373\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.352213\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.061210\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.160229\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.157200\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.079381\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.276837\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.118461\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.053543\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.099186\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.153220\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.030736\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.101097\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.192526\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.067998\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.155902\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.157049\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.088015\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.078568\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.248617\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.117223\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.131687\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.062394\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.074754\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.060174\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.049994\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.285175\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.137290\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.065255\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.191325\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.087608\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.084890\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.083806\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.062789\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.128109\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.264873\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.047817\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.094705\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.096504\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.050870\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.222472\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.036131\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.110932\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.042302\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.151190\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.135430\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.044451\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.124114\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.208191\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.056039\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.068089\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.058068\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.113426\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.443175\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.091129\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.196697\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.141363\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.202581\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.147798\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.077561\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.077704\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.050609\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.201562\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.017174\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.142661\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.081450\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.069460\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.163370\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.175726\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.118095\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.034196\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.168664\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.142337\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.123768\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.016827\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.089755\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.143577\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.096365\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.028097\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.134558\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.040511\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.065449\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.025457\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.057380\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.071880\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.090790\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.083272\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.127900\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.071639\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.173730\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.114263\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.109378\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.071955\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.052253\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.117883\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.030766\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.121572\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.060481\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.019942\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.046082\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.481436\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.067045\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.046540\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.201305\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.200685\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.074989\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.077334\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.098673\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.058917\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.114092\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.044848\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.070916\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.029073\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.093272\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.111100\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.127038\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.145050\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.087151\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.056982\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.029800\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.023250\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.105908\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.132918\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.071170\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.160229\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.112352\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.057968\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.097559\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.043264\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.039161\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.224339\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.072705\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.168395\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.091917\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.039492\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.182653\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.036523\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.190664\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.129730\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.103435\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.116435\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.053442\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.078466\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.145091\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.079698\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.043784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.301186\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.023737\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.108017\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.148332\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.018325\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.041269\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.033969\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.175850\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.068201\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.015735\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.066413\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.086220\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.036130\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.039361\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.045031\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.146137\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.054742\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.335858\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.049244\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.203828\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.040820\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.130529\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.042800\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.117958\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.114959\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.080418\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.141002\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.094705\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.088132\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.087633\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.022551\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.108147\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.046268\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.094620\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.145125\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.064634\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.042073\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.101659\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.092228\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.097469\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.078514\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.095867\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.186267\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.088264\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.070833\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.118796\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.032113\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.088123\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.039798\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.060427\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.105020\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.062975\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.050640\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.064254\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.033240\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.044064\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.053817\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.100870\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.035276\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.082342\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.045015\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.076815\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.063787\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.079857\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.031207\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.035611\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.097038\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.127656\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.023711\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.064525\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.140153\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.056664\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.055372\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.096721\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.153107\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.062206\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.064806\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.033050\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.083129\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.028939\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.041684\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.049232\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.052447\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.042567\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.108604\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.111051\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.093211\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.034293\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.017853\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.076945\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.076906\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.035362\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.129166\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.156236\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.041388\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.197338\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.061373\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.071534\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.116460\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.073609\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.096247\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.077630\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.101332\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.064890\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.019656\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.135147\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.136189\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.066140\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.071442\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.139751\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.022598\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.059979\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.158284\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.055290\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.156833\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.115468\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.043290\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.080649\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.016989\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.094018\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.110375\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.031110\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.020757\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.049054\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.072312\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.170764\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.035996\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.067730\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.108827\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.033493\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.050476\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.028826\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.030492\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.014689\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.085793\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.118025\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.082710\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.064172\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.141202\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.028511\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.067121\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.060131\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.116644\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.046088\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.043558\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.019468\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.081163\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.084232\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.011824\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.035339\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.092668\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.060145\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.015248\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.124036\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.052811\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.068079\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.141820\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.043681\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.042535\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.162155\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.031053\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.071442\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.090939\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.140628\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.137647\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.059493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.030309\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.080539\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.092254\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.092281\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.046978\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.055710\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.034637\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.024366\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.012413\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.063499\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.066098\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.026826\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.058143\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.021718\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.170595\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.065993\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.021270\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.055679\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.037750\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.034487\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.024442\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.008375\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.005892\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.203405\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.060876\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.009878\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.013046\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.038864\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.133661\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.070578\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.072621\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.098526\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.024049\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.060728\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.059617\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.033489\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.175788\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.122280\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.040964\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.009694\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.086564\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.107304\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.105027\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.097990\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.069312\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.016145\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.115559\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.142973\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.117500\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.120356\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.181979\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.042607\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.015542\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.028856\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.112778\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.035275\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.024436\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.118470\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.032398\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.073512\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.047127\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.128615\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.104090\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.034722\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.063515\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.124990\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.016381\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.076570\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.022028\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.054596\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.118649\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.123594\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.293739\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.083474\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.034505\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.218766\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.045046\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.037910\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.072627\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.044304\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.024884\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.037258\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.220392\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.199344\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.059446\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.168427\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.170328\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.022032\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.139913\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.046230\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.040734\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.012360\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.028054\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.069169\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.040073\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.027303\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.079048\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.200502\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.115812\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.031177\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.085078\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.041391\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.144360\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.120164\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.038906\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.060906\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.032339\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.041564\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.094066\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.143849\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.063088\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.249415\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.006349\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.118475\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.149144\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.008153\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.082293\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.020462\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.024679\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.152227\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.081140\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.019691\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.026271\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.087780\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.049732\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.033752\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.021520\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.034577\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.067866\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.062516\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.025548\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.018176\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.074669\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.055687\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.023387\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.005197\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.121379\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.042901\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.056875\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.031945\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.090942\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.056233\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.098662\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.040258\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.286747\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.027901\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.034036\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.009046\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.147152\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.020642\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.041730\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.023387\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.028066\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.023861\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.048502\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.026142\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.071551\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.149221\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.150711\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.044112\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.119000\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.021861\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.061582\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.082551\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.130991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.039185\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.030273\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.082167\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.092133\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.091059\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.022530\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.014088\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.053925\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.031718\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.028522\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.011386\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.021276\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.098737\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.028933\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.143953\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.019793\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.110389\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.004315\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.085256\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.030260\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.142096\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.142454\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.011547\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.111340\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.049012\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.084939\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.041761\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.019575\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.026935\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.013912\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.047479\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.063594\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.008460\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.022787\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.096486\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.068760\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.214638\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.065860\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.047827\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.122671\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.031101\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.028266\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.026155\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.091773\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.063156\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.019378\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.029735\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.063686\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.027203\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.016271\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.011368\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.036039\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.024338\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.048983\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.033344\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.068562\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.025826\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.026822\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.021874\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.025617\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.044654\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.027678\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.032088\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.148115\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.019336\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.020008\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.235599\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.030561\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.049527\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.070319\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.036905\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.005065\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.074027\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.041793\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.016051\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.066669\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.049932\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.049464\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.063075\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.002741\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.114674\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.044013\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.075896\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.079862\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.050623\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.051143\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.114933\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.088026\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.037043\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.047335\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.084180\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.049132\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.014203\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.224388\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.100551\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.052730\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.038912\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.084479\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.030861\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.125755\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.013471\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.026178\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.132855\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.194531\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.077173\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.090650\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.021115\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.012634\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.060285\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.099381\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.028307\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.074885\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.034596\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.056150\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.017533\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.049518\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.166162\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.118106\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.026697\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.079560\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.051149\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.004416\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.026102\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.055140\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.017434\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.069179\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.083857\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.052098\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.045157\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.075654\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.091774\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.057015\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.129297\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.075406\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.087319\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.031040\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.123638\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.022042\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.037169\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.086366\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.017090\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.019275\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.077720\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.006906\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.110044\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.045003\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.049341\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.041987\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.090718\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.109346\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.127390\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.081450\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.069100\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.057261\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.055514\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.152230\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.028581\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.155312\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.057369\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.045232\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.031805\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.090113\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.053424\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.025752\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.040208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.313964\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.032920\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.037503\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.143344\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.045236\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.022686\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.030187\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.040202\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.224778\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.052128\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.063841\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.058738\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.014848\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.037866\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.049599\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.037353\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.096749\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.082595\n",
      "\n",
      "Test set: Average loss: 0.0587, Accuracy: 9818/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10-1 cnn_mnist\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jw/venv/py3/lib/python3.5/site-packages/ipykernel_launcher.py:88: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303408\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.304997\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.299001\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.310453\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.302622\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.297562\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.282559\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.294089\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.277132\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.302828\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.296567\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.293573\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.286664\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.288528\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.258519\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.291134\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.260770\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.253098\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.209404\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.231338\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.176473\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.139322\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.121251\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.914947\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.649656\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.595176\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.104892\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.016250\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.743706\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.858299\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.563746\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.641106\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.663527\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.489950\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.410837\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.364197\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.622507\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.462599\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.340998\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.344778\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.432749\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.264079\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.307488\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.413994\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.282505\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.508591\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.275583\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.299872\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.361154\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.415609\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.375274\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.282586\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.186240\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.359267\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.263232\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.234624\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.212708\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.203557\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.247929\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.276319\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.228467\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.480376\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.131004\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.199273\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.165617\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.130990\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.141132\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.259049\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.064413\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.112964\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.163474\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.303275\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.190160\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.164424\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.196607\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.097131\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.399470\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.238070\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.404046\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.099786\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.145157\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.152106\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.270294\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.116930\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.043580\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.144174\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.233583\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.084816\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.303239\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.155565\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.173953\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.172515\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.098442\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.113688\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.141147\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.312347\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.155072\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.276549\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.139274\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.174426\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.084581\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.148586\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.079212\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.265269\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.239031\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.185137\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.109177\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.131916\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.167699\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.160955\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.043132\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.157280\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.138590\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.159327\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.380284\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.209279\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.335816\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.075903\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.087770\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.257321\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.105054\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.243244\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.078931\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.056343\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.146060\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.164039\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.078214\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.042271\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.430545\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.172193\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.108070\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.204419\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.089088\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.217214\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.098593\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.165338\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.076554\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.220624\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.130296\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.173000\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.325109\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.233189\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.059221\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.148815\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.093484\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.142229\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.106547\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.272250\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.330767\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.116737\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.085913\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.144250\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.059291\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.154076\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.062485\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.177352\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.172162\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.085567\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.211991\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.060789\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.035795\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.028583\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.112082\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.050463\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.066082\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.141566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.185026\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.112420\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.083204\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.113864\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.070270\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.085109\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.031691\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.067124\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.089200\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.217419\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.115223\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.069219\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.106650\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.184776\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.094703\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.204755\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.074952\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.070188\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.030482\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.246852\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.067042\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.070558\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.117453\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.065429\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.060485\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.067967\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.148412\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.096356\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.242163\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.086723\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.063336\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.045545\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.102694\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.055608\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.154174\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.276428\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.023036\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.087276\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.041501\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.048654\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.081451\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.074710\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.146384\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.043836\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.171497\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.126933\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.029211\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.060289\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.061708\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.080070\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.035344\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.181535\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.049351\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.087824\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.033865\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.087646\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.097542\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.143294\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.242420\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.414933\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.173144\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.029566\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.103965\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.080865\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.059189\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.071138\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.080939\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.036448\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.028776\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.011704\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.059162\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.137441\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.078720\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.083978\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.116390\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.060420\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.064170\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.048871\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.078895\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.228021\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.124399\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.080362\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.053377\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.030608\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.088189\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.192957\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.143046\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.088761\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.167309\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.016474\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.150931\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.046958\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.116971\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.018659\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.031208\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.069513\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.097668\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.144171\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.081305\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.078622\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.054118\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.174584\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.062247\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.050197\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.033924\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.129156\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.034167\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.039170\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.039306\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.059724\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.112035\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.188924\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.054694\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.078748\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.082972\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.078067\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.053890\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.060319\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.030266\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.064256\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.024485\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.097420\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.079189\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.026886\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.060748\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.053505\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.069073\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.109308\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.079013\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.132106\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.094307\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.108900\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.022494\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.088158\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.035062\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.051932\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.037997\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.106590\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.062873\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.048718\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.055873\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.059897\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.080603\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.211233\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.064844\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.049636\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.105998\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.014383\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.170590\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.068305\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.145379\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.103812\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.029051\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.036750\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.137821\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.058071\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.119964\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.030071\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.120784\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.144916\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.117714\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.027783\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.022367\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.058466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-63c90f9db904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-63c90f9db904>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 11-1  toy inception mnist\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)  # flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
